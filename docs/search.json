[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IML",
    "section": "",
    "text": "Esame di introduzione al machine learning"
  },
  {
    "objectID": "exams/esame_1_5.html",
    "href": "exams/esame_1_5.html",
    "title": "01-05: Minimizzazione con SciPy",
    "section": "",
    "text": "5. I dati qui a fianco rappresentano un campione di 1000 valori indipendenti ottenuti con la formula arand()+brand(), a partire da due numeri reali a e b incogniti.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 5\", header=None)\nbase.head(1)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n0\n5. I dati qui a fianco rappresentano un campio...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n25.1\n26.0\n10.6\n20.2\n7.7\n39.8\n31.9\n14.3\n36.2\n38.1\ndf = base.drop(columns=base.columns[:9])\nprint(df.head(2))\nprint(df.shape)\n\nsamples = df.values.flatten()\nprint(samples.shape)\nprint(samples[0])\nprint(samples[10])\nprint(samples.mean())\n\n     9     10    11    12    13    14    15    16    17    18\n0  25.1  26.0  10.6  20.2   7.7  39.8  31.9  14.3  36.2  38.1\n1  25.4  21.3  18.8  15.0  21.1  34.1  48.9  32.2  26.2  10.2\n(100, 10)\n(1000,)\n25.1\n25.4\n25.112599999999997\n5.1 Si trovino due numeri reali s e t tali che: - s rende minima la media di \\(|X_i-s|\\) e, - t rende minima la media di \\((X_i-t)^2\\)\n(Entrambe le medie si considerano fatte sul campione, al variare di i da 1 a 1000.) Cos’altro rappresentano questi valori?",
    "crumbs": [
      "Exams",
      "01-05: Minimizzazione con SciPy"
    ]
  },
  {
    "objectID": "exams/esame_1_5.html#minimizzazione",
    "href": "exams/esame_1_5.html#minimizzazione",
    "title": "01-05: Minimizzazione con SciPy",
    "section": "Minimizzazione",
    "text": "Minimizzazione\nscipy.optimize.minimize si usa per trovare il minimo di una funzione scalare rispetto a uno o più parametri.\nSi parte definendo una funzione obiettivo che prende in input un vettore di parametri e restituisce un numero reale. Anche nel caso di un solo parametro, l’argomento è sempre un array.\nfrom scipy.optimize import minimize\n\nf = lambda x: (x[0] - 2)**2\nPoi si fornisce una stima iniziale (x0) e, se necessario, si specifica il metodo di ottimizzazione:\nres = minimize(f, x0=[0.0], method=\"BFGS\")\nIl risultato è un oggetto OptimizeResult che contiene le informazioni sull’ottimizzazione. In particolare:\n\nres.x è il punto in cui la funzione raggiunge il minimo\nres.fun è il valore minimo della funzione\nres.success indica se l’algoritmo è convergito correttamente\n\nPer accedere al minimizzatore stimato:\nres.x[0]\nLa scelta del metodo è importante. Per funzioni derivabili (ad esempio quadratiche) sono adatti metodi come BFGS; per funzioni non derivabili (come il valore assoluto) è preferibile usare metodi come Powell o Nelder-Mead.\nIn sintesi, minimize permette di risolvere problemi di stima della forma:\n\\(\\hat{\\theta} = \\arg\\min_\\theta f(\\theta)\\)\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\nNota. Si potrebbe scrivere anche s e t semplicemente, ma è più corretto scrivere s[0] perchè internamente minimize usa un vettore e non uno scalare, ma non cambia nulla\n\ns_fun = lambda s: np.mean(np.abs(samples - s[0]))\nt_fun = lambda t: np.mean((samples - t[0])**2)\n\ns_min = minimize(s_fun, x0=samples[0])\nt_min = minimize(t_fun, x0=samples[0])\n\ns = s_min.x[0]\nt = t_min.x[0]\n\nprint(f\"s: {s:.4f}\")\nprint(f\"t: {t:.5f}\")\n\ns: 25.0000\nt: 25.11260\n\n\n\\(s\\) è il valore che rende minima la deviazione assoluta media dal campione: coincide con la mediana campionaria ed è una misura di posizione robusta, poco sensibile agli outlier.\n\\(t\\) è il valore che rende minimo l’errore quadratico medio: coincide con la media campionaria ed è la misura di posizione “classica”, ottimale in senso dei minimi quadrati ma sensibile ai valori estremi.",
    "crumbs": [
      "Exams",
      "01-05: Minimizzazione con SciPy"
    ]
  },
  {
    "objectID": "exams/esame_1_2.html",
    "href": "exams/esame_1_2.html",
    "title": "01-02: EDA e PCA",
    "section": "",
    "text": "2. I dati qui a fianco riportano misurazioni fisiologiche, scelte di stile di vita e marcatori biochimici, che si ipotizza siano fortemente correlati con l’età. Questo dataset viene usato in tutti gli esercizi restanti del compito.\nDescrizione delle variabili: group gruppo di età (senior/non-senior) age età gender sesso PA Se il soggetto svolge, nella settimana tipica, attività sportive, di fitness o ricreative di intensità moderata o vigorosa BMI Body Mass Index GLU Glucosio nel sangue da digiuno diabetic Diabetico (1 sì, 2 no, 3 incerto) GLT Glucosio orale insulin Livello di insulina",
    "crumbs": [
      "Exams",
      "01-02: EDA e PCA"
    ]
  },
  {
    "objectID": "exams/esame_1_2.html#explorative-data-analysis",
    "href": "exams/esame_1_2.html#explorative-data-analysis",
    "title": "01-02: EDA e PCA",
    "section": "Explorative Data Analysis",
    "text": "Explorative Data Analysis\n2.1 Studiare la distribuzione delle diverse variabili, sia univariata, sia bivariata, alla ricerca di outliers e altri difetti. Si scelgano opportune trasformazioni per facilitare questo compito e per ottenere migliori risultati negli esercizi seguenti.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\n\n\nprint(base.columns)\ndf = base.drop(columns=base.columns[0:9])\ndf = df.set_index([\"id\"])\nprint(df.columns)\n\nIndex(['2. I dati qui a fianco riportano misurazioni fisiologiche, scelte di stile di vita e marcatori biochimici, che si ipotizza siano fortemente correlati con l’età.\\nQuesto dataset viene usato in tutti gli esercizi restanti del compito.\\nDescrizione delle variabili:\\ngroup gruppo di età (senior/non-senior)\\nage età\\ngender sesso\\nPA Se il soggetto svolge, nella settimana tipica, attività sportive, di fitness o ricreative di intensità moderata o vigorosa\\nBMI Body Mass Index\\nGLU Glucosio nel sangue da digiuno\\ndiabetic Diabetico (1 sì, 2 no, 3 incerto)\\nGLT Glucosio orale\\ninsulin Livello di insulina',\n       'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5',\n       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'id', 'group', 'age',\n       'gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'],\n      dtype='object')\nIndex(['group', 'age', 'gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT',\n       'insulin'],\n      dtype='object')\n\n\n\nprint(df.info())\nprint(df[\"group\"].unique())\ndf = pd.get_dummies(df, \"group\", dtype=int)\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2278 entries, 1 to 2278\nData columns (total 9 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   group     2078 non-null   object \n 1   age       2078 non-null   float64\n 2   gender    2278 non-null   int64  \n 3   PA        2278 non-null   int64  \n 4   BMI       2278 non-null   float64\n 5   GLU       2278 non-null   int64  \n 6   diabetic  2278 non-null   int64  \n 7   GLT       2278 non-null   int64  \n 8   insulin   2278 non-null   float64\ndtypes: float64(3), int64(5), object(1)\nmemory usage: 178.0+ KB\nNone\n['Adult' 'Senior' nan]\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2278 entries, 1 to 2278\nData columns (total 10 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   age           2078 non-null   float64\n 1   gender        2278 non-null   int64  \n 2   PA            2278 non-null   int64  \n 3   BMI           2278 non-null   float64\n 4   GLU           2278 non-null   int64  \n 5   diabetic      2278 non-null   int64  \n 6   GLT           2278 non-null   int64  \n 7   insulin       2278 non-null   float64\n 8   group_Adult   2278 non-null   int64  \n 9   group_Senior  2278 non-null   int64  \ndtypes: float64(3), int64(7)\nmemory usage: 195.8 KB\nNone\n\n\n\nfor col in df.columns:\n    plt.hist(df[col])\n    plt.title(col)\n    plt.show()\n    break # debug only\n\n\n\n\n\n\n\n\n\nsns.pairplot(df)\nplt.show()",
    "crumbs": [
      "Exams",
      "01-02: EDA e PCA"
    ]
  },
  {
    "objectID": "exams/esame_1_2.html#ricerca-di-outlier",
    "href": "exams/esame_1_2.html#ricerca-di-outlier",
    "title": "01-02: EDA e PCA",
    "section": "Ricerca di Outlier",
    "text": "Ricerca di Outlier\n\nplt.plot(df[\"age\"], df[\"PA\"], \".\")\nplt.xlabel(\"age\")\nplt.ylabel(\"PA\")\nplt.show()\n\n\n\n\n\n\n\n\n\noutlier = df[df[\"PA\"] == 7]\ndf.drop(index=outlier.index, inplace=True)",
    "crumbs": [
      "Exams",
      "01-02: EDA e PCA"
    ]
  },
  {
    "objectID": "exams/esame_1_2.html#pca-e-factor-loading",
    "href": "exams/esame_1_2.html#pca-e-factor-loading",
    "title": "01-02: EDA e PCA",
    "section": "PCA e Factor Loading",
    "text": "PCA e Factor Loading\n2.2 Eseguire una PCA delle variabili da L a S. Quante componenti conviene tenere? Descrivere qualitativamente il significato delle prime componenti.\n\nfrom sklearn.decomposition import PCA\n\n\nprint(df.shape)\n\ndf_train = df[df[\"age\"].isna() != True]\ndf_test = df[df[\"age\"].isna()]\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n(2277, 10)\n(2077, 10)\n(200, 10)\n\n\n\ndf_train = df_train - df_train.mean()\ndf_test = df_test - df_test.mean()\n\n\npca = PCA()\npca.fit(df_train)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \nNone\n\n\n\ncopy \nTrue\n\n\n\nwhiten \nFalse\n\n\n\nsvd_solver \n'auto'\n\n\n\ntol \n0.0\n\n\n\niterated_power \n'auto'\n\n\n\nn_oversamples \n10\n\n\n\npower_iteration_normalizer \n'auto'\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\n\nx = np.linspace(0,1, df_train.shape[1])\ny = pca.explained_variance_ratio_\n\nplt.plot(x,y)\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nthr = 0.9\nsumm = 0\nk = 0\n\nfor v_ratio in pca.explained_variance_ratio_:\n    summ += v_ratio\n    k += 1\n\n    if summ &gt;= thr: break\n\nprint(summ)\nprint(k)\n\n0.9050591901248333\n2\n\n\n\nloadings = pca.components_.T\nloadings_df = pd.DataFrame(loadings, index=df_train.columns)\nloadings_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nage\n0.149467\n0.982362\n0.030367\n0.071332\n-0.079321\n0.000377\n-0.012669\n0.012654\n-0.000773\n-8.189253e-20\n\n\ngender\n0.000087\n0.000353\n-0.007604\n0.002312\n0.010518\n0.959367\n-0.206288\n-0.191365\n0.016317\n-2.973482e-19\n\n\nPA\n0.000531\n-0.000115\n-0.002723\n0.000352\n-0.004322\n0.281361\n0.691982\n0.664798\n0.003776\n4.255316e-17\n\n\nBMI\n0.030699\n0.022993\n0.113314\n0.516372\n0.847839\n-0.007767\n0.012016\n-0.003536\n-0.000418\n-2.738812e-18\n\n\nGLU\n0.272446\n-0.057854\n0.940743\n-0.192546\n-0.016719\n0.008135\n0.002002\n-0.001913\n0.000806\n9.080420e-18\n\n\ndiabetic\n0.000029\n0.000531\n-0.000385\n0.001406\n-0.000392\n-0.016760\n-0.004472\n0.006064\n0.999830\n1.237927e-16\n\n\nGLT\n0.949014\n-0.133745\n-0.284926\n-0.010398\n0.013671\n-0.002579\n-0.001096\n0.000378\n-0.000097\n-2.785639e-18\n\n\ninsulin\n0.043079\n-0.113648\n0.141360\n0.831313\n-0.523560\n0.003886\n-0.005553\n0.000824\n-0.001226\n-1.056133e-18\n\n\ngroup_Adult\n-0.001952\n-0.011924\n0.002121\n0.000072\n0.009093\n-0.003307\n-0.489002\n0.510495\n-0.005328\n7.071068e-01\n\n\ngroup_Senior\n0.001952\n0.011924\n-0.002121\n-0.000072\n-0.009093\n0.003307\n0.489002\n-0.510495\n0.005328\n7.071068e-01\n\n\n\n\n\n\n\n\nloadings_df = loadings_df &gt;= loadings_df.mean()\nloadings_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nage\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ngender\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\nPA\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\nBMI\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\nGLU\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ndiabetic\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\nGLT\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ninsulin\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ngroup_Adult\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\ngroup_Senior\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue",
    "crumbs": [
      "Exams",
      "01-02: EDA e PCA"
    ]
  },
  {
    "objectID": "exams/esame_2_2.html",
    "href": "exams/esame_2_2.html",
    "title": "02-02: Analisi dataset Gallstone",
    "section": "",
    "text": "2. Il dataset qui a fianco proviene dall’UCI ML laboratory https://archive.ics.uci.edu/dataset/1150/gallstone-1\nIl dataset clinico è stato raccolto presso l’Ambulatorio di Medicina Interna dell’Ospedale Ankara VM Medical Park e include dati di 319 individui (giugno 2022 – giugno 2023), di cui 161 con diagnosi di calcolosi biliare. Esso contiene 38 variabili, tra cui dati demografici, di bioimpedenza e di laboratorio, ed è stato approvato eticamente dal Comitato Etico dell’Ospedale della Città di Ankara (E2-23-4632). Le variabili demografiche comprendono età, sesso, altezza, peso e BMI. I dati di bioimpedenza includono acqua totale, extracellulare e intracellulare, massa muscolare e grassa, proteine, area del grasso viscerale e grasso epatico. Le variabili di laboratorio comprendono glucosio, colesterolo totale, HDL, LDL, trigliceridi, AST, ALT, ALP, creatinina, GFR, PCR, emoglobina e vitamina D. Il dataset è completo, senza valori mancanti, ed è bilanciato rispetto alla presenza della malattia, eliminando la necessità di ulteriori fasi di pre-processing. Fornisce una solida base per la previsione della calcolosi biliare tramite modelli di machine learning basati su caratteristiche non derivanti da imaging.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nbase = pd.read_excel(\"inml25tst02.xlsx\", sheet_name=\"Es 2\")\ndf = base.drop(columns=base.columns[:9]).set_index(\"id\").sort_index()\ndf.head(1)\n\n\n\n\n\n\n\n\nGallstone Status\nAge\nGender\nComorbidity\nCoronary Artery Disease (CAD)\nHypothyroidism\nHyperlipidemia\nDiabetes Mellitus (DM)\nHeight\nWeight\n...\nHigh Density Lipoprotein (HDL)\nTriglyceride\nAspartat Aminotransferaz (AST)\nAlanin Aminotransferaz (ALT)\nAlkaline Phosphatase (ALP)\nCreatinine\nGlomerular Filtration Rate (GFR)\nC-Reactive Protein (CRP)\nHemoglobin (HGB)\nVitamin D\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n50\n0\n0\n0\n0\n0\n0\n185\n92.8\n...\n40.0\n134.0\n20.0\n22.0\n87.0\n0.82\n112.47\n0.0\n16.0\n33.0\n\n\n\n\n1 rows × 39 columns\nX = df.drop(columns=[\"Gallstone Status\"])\ny = df[\"Gallstone Status\"]",
    "crumbs": [
      "Exams",
      "02-02: Analisi dataset Gallstone"
    ]
  },
  {
    "objectID": "exams/esame_2_2.html#correlazione-tra-input-e-output",
    "href": "exams/esame_2_2.html#correlazione-tra-input-e-output",
    "title": "02-02: Analisi dataset Gallstone",
    "section": "Correlazione tra input e output",
    "text": "Correlazione tra input e output\n2.1 Siccome le variabili sono tante, vorremmo concentrarci su quelle che sono più correlate a quella di output, Gallstone Status. Calcolare la correlazione di ciascuna variabile con quella di output, ed elencare le 15 variabili più correlate.\nSiccome non abbiamo ancora verificato la distribuzione delle variabili, è meglio usare la correlazione di Spearman, invece della solita correlazione lineare (di Pearson). Questa si ottiene calcolando la correlazione di lineare dopo aver sostituito i valori della variabile con i ranghi associati a quei valori, quindi 1 al posto del valore più piccolo, 2 al posto del secondo più piccolo, e così via.\n\nspearman_corr = df.corr(method=\"spearman\")\nsns.heatmap(spearman_corr, cbar=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nk = 15\nspearman_corr[\"Gallstone Status\"].sort_values()[:k]\n\nVitamin D                                        -0.379213\nAspartat Aminotransferaz (AST)                   -0.289784\nLean Mass (LM) (%)                               -0.227886\nBone Mass (BM)                                   -0.219057\nHemoglobin (HGB)                                 -0.191811\nExtracellular Water (ECW)                        -0.180222\nCreatinine                                       -0.134031\nExtracellular Fluid/Total Body Water (ECF/TBW)   -0.132715\nHeight                                           -0.121004\nTotal Body Water (TBW)                           -0.105059\nBody Protein Content (Protein) (%)               -0.104445\nCoronary Artery Disease (CAD)                    -0.096998\nMuscle Mass (MM)                                 -0.086199\nLow Density Lipoprotein (LDL)                    -0.080484\nAlanin Aminotransferaz (ALT)                     -0.071140\nName: Gallstone Status, dtype: float64\n\n\n2.2 Studiare la distribuzione delle 15 variabili trovate sopra, sia univariata, sia bivariata, alla ricerca di outliers e altri difetti. Si scelgano opportune trasformazioni per facilitare questo compito e per ottenere migliori risultati negli esercizi seguenti.\n2.3 Eseguire una PCA delle variabili da L in poi, usando le versioni trasformate più regolari per quelle che si sono modificate al punto precedente. Quante componenti conviene tenere?\n2.4 Si usino le componenti ridotte, ottenute con la PCA, per prevedere la variabile di output, Gallstone Status. È richiesto di usare qualche tecnica di validazione per stimare l’accuratezza del metodo.",
    "crumbs": [
      "Exams",
      "02-02: Analisi dataset Gallstone"
    ]
  },
  {
    "objectID": "lab/13_14_Outlier_Regressione.html",
    "href": "lab/13_14_Outlier_Regressione.html",
    "title": "13-14: Outlier e Regressione",
    "section": "",
    "text": "from ucimlrepo import fetch_ucirepo \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# fetch dataset \nabalone = fetch_ucirepo(id=1) \n  \n# data (as pandas dataframes) \nX: pd.DataFrame = abalone.data.features \ny: pd.DataFrame = abalone.data.targets.values\n\n\nX.columns\n\nIndex(['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight',\n       'Viscera_weight', 'Shell_weight'],\n      dtype='object')\n\n\n\nsns.violinplot(X, x=\"Length\", y=\"Sex\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.hist(X[\"Length\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(X)\nplt.show()\n\n\n\n\n\n\n\n\n\nX = pd.get_dummies(X, columns=[\"Sex\"])\n\n\nimport numpy as np\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# X: shape (n_samples, n_features)\n# example: X = np.array([[...], [...], ...])\n\nlof = LocalOutlierFactor(\n    n_neighbors=20,      # k\n    contamination=0.05   # expected fraction of outliers\n)\n\nlabels = lof.fit_predict(X)\nscores = lof.negative_outlier_factor_\n\n\nlabels\n\narray([1, 1, 1, ..., 1, 1, 1], shape=(4177,))\n\n\n\nX.shape\n\n(4177, 10)\n\n\n\nX_inliers = X[labels == 1]\ny_inliers = y[labels == 1]\n\n\nprint(X_inliers.shape)\nprint(y_inliers.shape)\n\n(3968, 10)\n(3968, 1)",
    "crumbs": [
      "Laboratory",
      "13-14: Outlier e Regressione"
    ]
  },
  {
    "objectID": "lab/13_14_Outlier_Regressione.html#analisi-degli-outlier",
    "href": "lab/13_14_Outlier_Regressione.html#analisi-degli-outlier",
    "title": "13-14: Outlier e Regressione",
    "section": "",
    "text": "from ucimlrepo import fetch_ucirepo \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# fetch dataset \nabalone = fetch_ucirepo(id=1) \n  \n# data (as pandas dataframes) \nX: pd.DataFrame = abalone.data.features \ny: pd.DataFrame = abalone.data.targets.values\n\n\nX.columns\n\nIndex(['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight',\n       'Viscera_weight', 'Shell_weight'],\n      dtype='object')\n\n\n\nsns.violinplot(X, x=\"Length\", y=\"Sex\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.hist(X[\"Length\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(X)\nplt.show()\n\n\n\n\n\n\n\n\n\nX = pd.get_dummies(X, columns=[\"Sex\"])\n\n\nimport numpy as np\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# X: shape (n_samples, n_features)\n# example: X = np.array([[...], [...], ...])\n\nlof = LocalOutlierFactor(\n    n_neighbors=20,      # k\n    contamination=0.05   # expected fraction of outliers\n)\n\nlabels = lof.fit_predict(X)\nscores = lof.negative_outlier_factor_\n\n\nlabels\n\narray([1, 1, 1, ..., 1, 1, 1], shape=(4177,))\n\n\n\nX.shape\n\n(4177, 10)\n\n\n\nX_inliers = X[labels == 1]\ny_inliers = y[labels == 1]\n\n\nprint(X_inliers.shape)\nprint(y_inliers.shape)\n\n(3968, 10)\n(3968, 1)",
    "crumbs": [
      "Laboratory",
      "13-14: Outlier e Regressione"
    ]
  },
  {
    "objectID": "lab/13_14_Outlier_Regressione.html#regressione-su-dati-filtrati",
    "href": "lab/13_14_Outlier_Regressione.html#regressione-su-dati-filtrati",
    "title": "13-14: Outlier e Regressione",
    "section": "Regressione su dati filtrati",
    "text": "Regressione su dati filtrati\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_inliers, y_inliers, test_size=0.2, random_state=42)\n\n\nmodel_all = LinearRegression()\nmodel_filtered = LinearRegression()\n\n\nmodel_all.fit(X_train, y_train)\nmodel_filtered.fit(X_train_filtered, y_train_filtered)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\n\npred_all = model_all.predict(X_test)\npred_filtered = model_filtered.predict(X_test_filtered)\n\n\nprint(f\"All: {r2_score(y_test, pred_all)}\")\nprint(f\"FIltered: {r2_score(y_test_filtered, pred_filtered)}\")\n\nAll: 0.5481628137889278\nFIltered: 0.5319942459710485",
    "crumbs": [
      "Laboratory",
      "13-14: Outlier e Regressione"
    ]
  },
  {
    "objectID": "lab/11_12_MLE_Regressione_Abalone.html",
    "href": "lab/11_12_MLE_Regressione_Abalone.html",
    "title": "11-12: MLE e Regressione Abalone",
    "section": "",
    "text": "from scipy.stats import poisson\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nmu = 10\nsamples = poisson.rvs(mu=mu, size=100)\nsamples\n\narray([ 8, 11, 11,  6, 12, 16, 11, 12,  8, 11, 12,  9,  9, 11,  7,  2, 11,\n        7,  7,  9, 11, 10,  7, 12, 14, 12, 12,  8,  9, 14,  9, 11, 10, 10,\n       12,  8,  6,  7,  9,  7, 18, 10, 14,  6, 12,  9, 13,  4, 14, 15, 12,\n       11,  7, 10,  9, 16, 11,  6,  6, 12,  6, 12,  9,  8, 11, 11, 13, 13,\n       13,  9, 13,  8,  9, 11, 11,  8,  9, 12, 12,  9, 10,  9, 11, 13, 17,\n        7,  8, 11, 11, 12,  9,  5, 17,  5,  8, 16,  7,  9,  6, 12])\n\n\n\nsns.kdeplot(samples)\nplt.hist(samples, density=True)\nplt.show()\n\n\n\n\n\n\n\n\nAttenzione! logpmf è la log-likelihood, ma per una variabile discreta, mentre non per quelle continue si usa logpdf.\n\nlkls = []\nfor i in range(0, 100):\n    lkls.append(np.round(poisson.logpmf(samples, mu=i).mean(),3))\n\n\nplt.plot(range(0,100), lkls)\nplt.plot(lkls.index(max(lkls)), max(lkls),\"o\" ,label=\"max\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nlkls.index(max(lkls))\n\n10\n\n\n\nfrom scipy.optimize import minimize_scalar\n\n\nget_lkl = lambda mu: poisson.logpmf(samples, mu=mu).mean() * -1\nminimize_scalar(get_lkl, bracket=(0, 15))\n\n message: \n          Optimization terminated successfully;\n          The returned value satisfies the termination criteria\n          (using xtol = 1.48e-08 )\n success: True\n     fun: 2.5126746355148497\n       x: 10.130000118440515\n     nit: 12\n    nfev: 15\n\n\n\nsamples.mean()\n\nnp.float64(10.13)",
    "crumbs": [
      "Laboratory",
      "11-12: MLE e Regressione Abalone"
    ]
  },
  {
    "objectID": "lab/11_12_MLE_Regressione_Abalone.html#analisi-di-una-distribuzione-tramite-mle",
    "href": "lab/11_12_MLE_Regressione_Abalone.html#analisi-di-una-distribuzione-tramite-mle",
    "title": "11-12: MLE e Regressione Abalone",
    "section": "",
    "text": "from scipy.stats import poisson\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nmu = 10\nsamples = poisson.rvs(mu=mu, size=100)\nsamples\n\narray([ 8, 11, 11,  6, 12, 16, 11, 12,  8, 11, 12,  9,  9, 11,  7,  2, 11,\n        7,  7,  9, 11, 10,  7, 12, 14, 12, 12,  8,  9, 14,  9, 11, 10, 10,\n       12,  8,  6,  7,  9,  7, 18, 10, 14,  6, 12,  9, 13,  4, 14, 15, 12,\n       11,  7, 10,  9, 16, 11,  6,  6, 12,  6, 12,  9,  8, 11, 11, 13, 13,\n       13,  9, 13,  8,  9, 11, 11,  8,  9, 12, 12,  9, 10,  9, 11, 13, 17,\n        7,  8, 11, 11, 12,  9,  5, 17,  5,  8, 16,  7,  9,  6, 12])\n\n\n\nsns.kdeplot(samples)\nplt.hist(samples, density=True)\nplt.show()\n\n\n\n\n\n\n\n\nAttenzione! logpmf è la log-likelihood, ma per una variabile discreta, mentre non per quelle continue si usa logpdf.\n\nlkls = []\nfor i in range(0, 100):\n    lkls.append(np.round(poisson.logpmf(samples, mu=i).mean(),3))\n\n\nplt.plot(range(0,100), lkls)\nplt.plot(lkls.index(max(lkls)), max(lkls),\"o\" ,label=\"max\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nlkls.index(max(lkls))\n\n10\n\n\n\nfrom scipy.optimize import minimize_scalar\n\n\nget_lkl = lambda mu: poisson.logpmf(samples, mu=mu).mean() * -1\nminimize_scalar(get_lkl, bracket=(0, 15))\n\n message: \n          Optimization terminated successfully;\n          The returned value satisfies the termination criteria\n          (using xtol = 1.48e-08 )\n success: True\n     fun: 2.5126746355148497\n       x: 10.130000118440515\n     nit: 12\n    nfev: 15\n\n\n\nsamples.mean()\n\nnp.float64(10.13)",
    "crumbs": [
      "Laboratory",
      "11-12: MLE e Regressione Abalone"
    ]
  },
  {
    "objectID": "lab/11_12_MLE_Regressione_Abalone.html#regressione-lineare-su-abalone",
    "href": "lab/11_12_MLE_Regressione_Abalone.html#regressione-lineare-su-abalone",
    "title": "11-12: MLE e Regressione Abalone",
    "section": "Regressione Lineare su Abalone",
    "text": "Regressione Lineare su Abalone\n\nfrom ucimlrepo import fetch_ucirepo \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport seaborn as sns\n\n\n# fetch dataset \nabalone = fetch_ucirepo(id=1) \n  \n# data (as pandas dataframes) \nX = abalone.data.features \ny = abalone.data.targets.values\n\n\nX.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4177 entries, 0 to 4176\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Sex             4177 non-null   object \n 1   Length          4177 non-null   float64\n 2   Diameter        4177 non-null   float64\n 3   Height          4177 non-null   float64\n 4   Whole_weight    4177 non-null   float64\n 5   Shucked_weight  4177 non-null   float64\n 6   Viscera_weight  4177 non-null   float64\n 7   Shell_weight    4177 non-null   float64\ndtypes: float64(7), object(1)\nmemory usage: 261.2+ KB\n\n\n\nnp.unique(X[\"Sex\"])\n\narray(['F', 'I', 'M'], dtype=object)\n\n\n\nX = pd.get_dummies(X, columns=['Sex'])\n\n\nprint(X.head(1))\n\n   Length  Diameter  Height  Whole_weight  Shucked_weight  Viscera_weight  \\\n0   0.455     0.365   0.095         0.514          0.2245           0.101   \n\n   Shell_weight  Sex_F  Sex_I  Sex_M  \n0          0.15  False  False   True  \n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\npca = PCA(n_components=1)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(model.coef_)\nprint(model.intercept_)\n\n[[2.46651499]]\n[9.90841066]\n\n\n\nf = lambda x: model.coef_ * x + model.intercept_\n\n\nx = np.linspace(0.1, 100, 100)\ny = f(x)\n\n\nx\n\narray([  0.1       ,   1.10909091,   2.11818182,   3.12727273,\n         4.13636364,   5.14545455,   6.15454545,   7.16363636,\n         8.17272727,   9.18181818,  10.19090909,  11.2       ,\n        12.20909091,  13.21818182,  14.22727273,  15.23636364,\n        16.24545455,  17.25454545,  18.26363636,  19.27272727,\n        20.28181818,  21.29090909,  22.3       ,  23.30909091,\n        24.31818182,  25.32727273,  26.33636364,  27.34545455,\n        28.35454545,  29.36363636,  30.37272727,  31.38181818,\n        32.39090909,  33.4       ,  34.40909091,  35.41818182,\n        36.42727273,  37.43636364,  38.44545455,  39.45454545,\n        40.46363636,  41.47272727,  42.48181818,  43.49090909,\n        44.5       ,  45.50909091,  46.51818182,  47.52727273,\n        48.53636364,  49.54545455,  50.55454545,  51.56363636,\n        52.57272727,  53.58181818,  54.59090909,  55.6       ,\n        56.60909091,  57.61818182,  58.62727273,  59.63636364,\n        60.64545455,  61.65454545,  62.66363636,  63.67272727,\n        64.68181818,  65.69090909,  66.7       ,  67.70909091,\n        68.71818182,  69.72727273,  70.73636364,  71.74545455,\n        72.75454545,  73.76363636,  74.77272727,  75.78181818,\n        76.79090909,  77.8       ,  78.80909091,  79.81818182,\n        80.82727273,  81.83636364,  82.84545455,  83.85454545,\n        84.86363636,  85.87272727,  86.88181818,  87.89090909,\n        88.9       ,  89.90909091,  90.91818182,  91.92727273,\n        92.93636364,  93.94545455,  94.95454545,  95.96363636,\n        96.97272727,  97.98181818,  98.99090909, 100.        ])\n\n\n\ny\n\narray([[ 10.15506215,  12.644     ,  15.13293785,  17.6218757 ,\n         20.11081355,  22.5997514 ,  25.08868925,  27.5776271 ,\n         30.06656495,  32.5555028 ,  35.04444065,  37.5333785 ,\n         40.02231635,  42.5112542 ,  45.00019205,  47.4891299 ,\n         49.97806775,  52.4670056 ,  54.95594345,  57.4448813 ,\n         59.93381915,  62.422757  ,  64.91169485,  67.4006327 ,\n         69.88957055,  72.3785084 ,  74.86744625,  77.3563841 ,\n         79.84532195,  82.3342598 ,  84.82319765,  87.3121355 ,\n         89.80107335,  92.2900112 ,  94.77894905,  97.2678869 ,\n         99.75682475, 102.2457626 , 104.73470045, 107.2236383 ,\n        109.71257615, 112.201514  , 114.69045185, 117.1793897 ,\n        119.66832755, 122.1572654 , 124.64620325, 127.1351411 ,\n        129.62407895, 132.1130168 , 134.60195465, 137.0908925 ,\n        139.57983035, 142.0687682 , 144.55770605, 147.0466439 ,\n        149.53558175, 152.0245196 , 154.51345745, 157.0023953 ,\n        159.49133315, 161.980271  , 164.46920885, 166.9581467 ,\n        169.44708455, 171.9360224 , 174.42496025, 176.9138981 ,\n        179.40283595, 181.8917738 , 184.38071165, 186.8696495 ,\n        189.35858735, 191.8475252 , 194.33646305, 196.8254009 ,\n        199.31433875, 201.8032766 , 204.29221445, 206.7811523 ,\n        209.27009015, 211.759028  , 214.24796585, 216.7369037 ,\n        219.22584155, 221.7147794 , 224.20371725, 226.6926551 ,\n        229.18159295, 231.6705308 , 234.15946865, 236.6484065 ,\n        239.13734435, 241.6262822 , 244.11522005, 246.6041579 ,\n        249.09309575, 251.5820336 , 254.07097145, 256.5599093 ]])\n\n\n\nplt.plot(x,y[0])\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nase = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\n\n\nprint(f\"MSE: {round(mse,2)}\")\nprint(f\"ASE: {round(ase,2)}\")\nprint(f\"R^2: {round(r2,2)}\")\n\nMSE: 7.06\nASE: 1.91\nR^2: 0.32",
    "crumbs": [
      "Laboratory",
      "11-12: MLE e Regressione Abalone"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html",
    "href": "lab/07_08_EDA_PCA.html",
    "title": "07-08: EDA e PCA",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#pandas-df-exploration",
    "href": "lab/07_08_EDA_PCA.html#pandas-df-exploration",
    "title": "07-08: EDA e PCA",
    "section": "Pandas df exploration",
    "text": "Pandas df exploration\n\ndf = pd.read_excel(\"body.xlsx\", index_col=0)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 507 entries, 1 to 507\nData columns (total 25 columns):\n #   Column                                                      Non-Null Count  Dtype  \n---  ------                                                      --------------  -----  \n 0   Biacromial diameter (see Fig. 2)                            507 non-null    float64\n 1   Biiliac diameter, or \"pelvic breadth\" (see Fig. 2)          507 non-null    float64\n 2   Bitrochanteric diameter (see Fig. 2)                        507 non-null    float64\n 3   Chest depth between spine and sternum at nipple level,      507 non-null    float64\n 4   Chest diameter at nipple level, mid-expiration              507 non-null    float64\n 5   Elbow diameter, sum of two elbows                           507 non-null    float64\n 6   Wrist diameter, sum of two wrists                           507 non-null    float64\n 7   Knee diameter, sum of two knees                             507 non-null    float64\n 8   Ankle diameter, sum of two ankles                           507 non-null    float64\n 9   Shoulder girth over deltoid muscles                         507 non-null    float64\n 10  Chest girth, nipple line in males and just above breast     507 non-null    float64\n 11  Waist girth, narrowest part of torso below the rib cage,    507 non-null    float64\n 12  Navel (or \"Abdominal\") girth at umbilicus and iliac crest,  507 non-null    float64\n 13  Hip girth at level of bitrochanteric diameter               507 non-null    float64\n 14  Thigh girth below gluteal fold, average of right and left   507 non-null    float64\n 15  Bicep girth, flexed, average of right and left girths       507 non-null    float64\n 16  Forearm girth, extended, palm up, average of right and      507 non-null    float64\n 17  Knee girth over patella, slightly flexed position, average  507 non-null    float64\n 18  Calf maximum girth, average of right and left girths        507 non-null    float64\n 19  Ankle minimum girth, average of right and left girths       507 non-null    float64\n 20  Wrist minimum girth, average of right and left girths       507 non-null    float64\n 21  Age (years)                                                 507 non-null    int64  \n 22  Weight (kg)                                                 507 non-null    float64\n 23  Height (cm)                                                 507 non-null    float64\n 24  Gender (1 - male, 0 - female)                               507 non-null    int64  \ndtypes: float64(23), int64(2)\nmemory usage: 103.0 KB\n\n\n\nheight = df[\"Height (cm)\"].values\ngender = df[\"Gender (1 - male, 0 - female)\"].values\nweight = df[\"Weight (kg)\"].values\n\n\n#plt.plot(height, weight, \"o\")\nplt.scatter(height, weight, marker=\".\")\nplt.xlabel(\"height\")\nplt.ylabel(\"weight\")\nplt.show()",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#statistiche-principali",
    "href": "lab/07_08_EDA_PCA.html#statistiche-principali",
    "title": "07-08: EDA e PCA",
    "section": "Statistiche principali",
    "text": "Statistiche principali\n\nmean = df.mean()\ncorr = df.corr()\ncov = df.cov()\nstd = df.std()\n\n\nsns.heatmap(corr)\nplt.show()\n\n\n\n\n\n\n\n\n\nfemale = df[df[\"Gender (1 - male, 0 - female)\"] == 0]\nmale = df[df[\"Gender (1 - male, 0 - female)\"] == 1]\n\nf_weight = female[\"Weight (kg)\"]\nf_height = female[\"Height (cm)\"]\n\nm_weight = male[\"Weight (kg)\"]\nm_height = male[\"Height (cm)\"]",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#kde-plot",
    "href": "lab/07_08_EDA_PCA.html#kde-plot",
    "title": "07-08: EDA e PCA",
    "section": "KDE Plot",
    "text": "KDE Plot\n\nsns.kdeplot(df[\"Height (cm)\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(f_height, label=\"female\")\nsns.kdeplot(m_height, label=\"male\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#scatter-plot",
    "href": "lab/07_08_EDA_PCA.html#scatter-plot",
    "title": "07-08: EDA e PCA",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nplt.scatter(f_weight, f_height, label=\"female\")\nplt.scatter(male[\"Weight (kg)\"], male[\"Height (cm)\"], label=\"male\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAlternativa\n\n# concateni i dati\nweights = np.concatenate([female[\"Weight (kg)\"], male[\"Weight (kg)\"]])\nheights = np.concatenate([female[\"Height (cm)\"], male[\"Height (cm)\"]])\n\n# etichette di classe\ny = np.concatenate([\n    np.zeros(len(female)),  # female = 0\n    np.ones(len(male))      # male = 1\n])\n\nplt.scatter(weights, heights, c=y, cmap=\"coolwarm\", alpha=0.7)\nplt.xlabel(\"Weight (kg)\")\nplt.ylabel(\"Height (cm)\")\nplt.show()",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#covarianza-e-correlazione-lineare",
    "href": "lab/07_08_EDA_PCA.html#covarianza-e-correlazione-lineare",
    "title": "07-08: EDA e PCA",
    "section": "Covarianza e Correlazione Lineare",
    "text": "Covarianza e Correlazione Lineare\n\nnp.round(np.cov(height, weight), 2) \n\narray([[ 88.5 ,  90.05],\n       [ 90.05, 178.11]])\n\n\n\nnp.round(np.corrcoef(height, weight), 2)\n\narray([[1.  , 0.72],\n       [0.72, 1.  ]])",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#standard-scaling",
    "href": "lab/07_08_EDA_PCA.html#standard-scaling",
    "title": "07-08: EDA e PCA",
    "section": "Standard Scaling",
    "text": "Standard Scaling\n\ncentered = df - df.mean()\nprint(centered.shape)\ncentered = centered.drop([\"Gender (1 - male, 0 - female)\"], axis=1)\nprint(centered.shape)\n\n(507, 25)\n(507, 24)\n\n\n\nscaled = centered / df.std(ddof=1)\n\n\nround(scaled.mean(),2)\n\nAge (years)                                                   0.0\nAnkle diameter, sum of two ankles                            -0.0\nAnkle minimum girth, average of right and left girths         0.0\nBiacromial diameter (see Fig. 2)                             -0.0\nBicep girth, flexed, average of right and left girths        -0.0\nBiiliac diameter, or \"pelvic breadth\" (see Fig. 2)           -0.0\nBitrochanteric diameter (see Fig. 2)                          0.0\nCalf maximum girth, average of right and left girths          0.0\nChest depth between spine and sternum at nipple level,        0.0\nChest diameter at nipple level, mid-expiration                0.0\nChest girth, nipple line in males and just above breast      -0.0\nElbow diameter, sum of two elbows                             0.0\nForearm girth, extended, palm up, average of right and       -0.0\nGender (1 - male, 0 - female)                                 NaN\nHeight (cm)                                                  -0.0\nHip girth at level of bitrochanteric diameter                 0.0\nKnee diameter, sum of two knees                               0.0\nKnee girth over patella, slightly flexed position, average   -0.0\nNavel (or \"Abdominal\") girth at umbilicus and iliac crest,   -0.0\nShoulder girth over deltoid muscles                           0.0\nThigh girth below gluteal fold, average of right and left    -0.0\nWaist girth, narrowest part of torso below the rib cage,     -0.0\nWeight (kg)                                                  -0.0\nWrist diameter, sum of two wrists                            -0.0\nWrist minimum girth, average of right and left girths         0.0\ndtype: float64\n\n\n\nround(scaled.std(),2)\n\nAge (years)                                                   1.0\nAnkle diameter, sum of two ankles                             1.0\nAnkle minimum girth, average of right and left girths         1.0\nBiacromial diameter (see Fig. 2)                              1.0\nBicep girth, flexed, average of right and left girths         1.0\nBiiliac diameter, or \"pelvic breadth\" (see Fig. 2)            1.0\nBitrochanteric diameter (see Fig. 2)                          1.0\nCalf maximum girth, average of right and left girths          1.0\nChest depth between spine and sternum at nipple level,        1.0\nChest diameter at nipple level, mid-expiration                1.0\nChest girth, nipple line in males and just above breast       1.0\nElbow diameter, sum of two elbows                             1.0\nForearm girth, extended, palm up, average of right and        1.0\nGender (1 - male, 0 - female)                                 NaN\nHeight (cm)                                                   1.0\nHip girth at level of bitrochanteric diameter                 1.0\nKnee diameter, sum of two knees                               1.0\nKnee girth over patella, slightly flexed position, average    1.0\nNavel (or \"Abdominal\") girth at umbilicus and iliac crest,    1.0\nShoulder girth over deltoid muscles                           1.0\nThigh girth below gluteal fold, average of right and left     1.0\nWaist girth, narrowest part of torso below the rib cage,      1.0\nWeight (kg)                                                   1.0\nWrist diameter, sum of two wrists                             1.0\nWrist minimum girth, average of right and left girths         1.0\ndtype: float64",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#pca",
    "href": "lab/07_08_EDA_PCA.html#pca",
    "title": "07-08: EDA e PCA",
    "section": "PCA",
    "text": "PCA\n\npca = PCA()\n#centered = pca.fit_transform(centered)\npca.fit(centered)\n\nplt.plot(pca.explained_variance_ratio_, '-')\nplt.plot(pca.explained_variance_ratio_, 'o')\nplt.show()\n\n\n\n\n\n\n\n\n\nsumm = 0\nthr = 0.9\nk = 0\n\nfor var in pca.explained_variance_ratio_:\n    if summ &lt;= thr:\n        k += 1\n        summ += var\n    else:\n        break\n\nprint(k)\n\n4\n\n\n\npca = PCA(n_components=k)\np_array = pca.fit_transform(centered)\np_df = pd.DataFrame(p_array)\np_df.columns = [\"c1\", \"c2\", \"c3\", \"c4\"]\np_df[\"target\"] = gender\n\n\nsns.heatmap(p_df.corr())\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LinearRegression\n\nx_train, x_test, y_train, y_test = train_test_split(p_array, gender, test_size=0.2)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape) \n\nregr = LinearRegression()\nregr.fit(x_train, y_train)\n\npredictions = regr.predict(x_test)\npredictions = (predictions &gt;= 0.5).astype(int)\nacc = accuracy_score(predictions, y_test)\nprint(round(acc,2))\n\n(405, 4)\n(102, 4)\n(405,)\n(102,)\n0.97",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/07_08_EDA_PCA.html#factor-loadings",
    "href": "lab/07_08_EDA_PCA.html#factor-loadings",
    "title": "07-08: EDA e PCA",
    "section": "Factor Loadings",
    "text": "Factor Loadings\n\nloadings = pca.components_.T\nl_df = pd.DataFrame(loadings, index=centered.columns, columns= [\"c1\", \"c2\", \"c3\", \"c4\"])\n\n\nl_df\n\n\n\n\n\n\n\n\nc1\nc2\nc3\nc4\n\n\n\n\nBiacromial diameter (see Fig. 2)\n0.088647\n-0.106290\n0.096804\n0.026067\n\n\nBiiliac diameter, or \"pelvic breadth\" (see Fig. 2)\n0.042160\n0.047046\n-0.051891\n0.141353\n\n\nBitrochanteric diameter (see Fig. 2)\n0.052266\n0.030594\n-0.047169\n0.109800\n\n\nChest depth between spine and sternum at nipple level,\n0.081933\n0.010757\n0.022322\n-0.023044\n\n\nChest diameter at nipple level, mid-expiration\n0.092392\n-0.044954\n0.042627\n-0.060098\n\n\nElbow diameter, sum of two elbows\n0.043226\n-0.030326\n0.037368\n0.009872\n\n\nWrist diameter, sum of two wrists\n0.028752\n-0.018737\n0.025919\n0.001314\n\n\nKnee diameter, sum of two knees\n0.039211\n-0.014619\n-0.002909\n0.027345\n\n\nAnkle diameter, sum of two ankles\n0.036049\n-0.017290\n0.032360\n0.026081\n\n\nShoulder girth over deltoid muscles\n0.372689\n-0.226972\n0.190561\n-0.341242\n\n\nChest girth, nipple line in males and just above breast\n0.371584\n-0.093149\n0.128207\n-0.362374\n\n\nWaist girth, narrowest part of torso below the rib cage,\n0.409625\n0.138307\n0.014333\n-0.211150\n\n\nNavel (or \"Abdominal\") girth at umbilicus and iliac crest,\n0.281018\n0.431562\n-0.364365\n0.159809\n\n\nHip girth at level of bitrochanteric diameter\n0.197000\n0.178900\n-0.400822\n0.165068\n\n\nThigh girth below gluteal fold, average of right and left\n0.088108\n0.069092\n-0.371231\n0.055705\n\n\nBicep girth, flexed, average of right and left girths\n0.148497\n-0.062530\n0.027850\n-0.141787\n\n\nForearm girth, extended, palm up, average of right and\n0.097703\n-0.064307\n0.037546\n-0.064731\n\n\nKnee girth over patella, slightly flexed position, average\n0.077776\n-0.004790\n-0.084661\n0.076924\n\n\nCalf maximum girth, average of right and left girths\n0.081002\n-0.013709\n-0.083551\n0.029954\n\n\nAnkle minimum girth, average of right and left girths\n0.054795\n-0.018874\n-0.014001\n0.026119\n\n\nWrist minimum girth, average of right and left girths\n0.044978\n-0.030127\n0.033409\n-0.009197\n\n\nAge (years)\n0.117430\n0.719571\n0.600513\n0.120955\n\n\nWeight (kg)\n0.516284\n-0.108589\n-0.136999\n0.133517\n\n\nHeight (cm)\n0.258789\n-0.374423\n0.314729\n0.740977\n\n\n\n\n\n\n\n\nc1 = l_df[\"c1\"]\nb_c1 = c1 &gt;= c1.mean()\nb_c1\n\nBiacromial diameter (see Fig. 2)                              False\nBiiliac diameter, or \"pelvic breadth\" (see Fig. 2)            False\nBitrochanteric diameter (see Fig. 2)                          False\nChest depth between spine and sternum at nipple level,        False\nChest diameter at nipple level, mid-expiration                False\nElbow diameter, sum of two elbows                             False\nWrist diameter, sum of two wrists                             False\nKnee diameter, sum of two knees                               False\nAnkle diameter, sum of two ankles                             False\nShoulder girth over deltoid muscles                            True\nChest girth, nipple line in males and just above breast        True\nWaist girth, narrowest part of torso below the rib cage,       True\nNavel (or \"Abdominal\") girth at umbilicus and iliac crest,     True\nHip girth at level of bitrochanteric diameter                  True\nThigh girth below gluteal fold, average of right and left     False\nBicep girth, flexed, average of right and left girths         False\nForearm girth, extended, palm up, average of right and        False\nKnee girth over patella, slightly flexed position, average    False\nCalf maximum girth, average of right and left girths          False\nAnkle minimum girth, average of right and left girths         False\nWrist minimum girth, average of right and left girths         False\nAge (years)                                                   False\nWeight (kg)                                                    True\nHeight (cm)                                                    True\nName: c1, dtype: bool",
    "crumbs": [
      "Laboratory",
      "07-08: EDA e PCA"
    ]
  },
  {
    "objectID": "lab/05_06_Limite_Centrale.html",
    "href": "lab/05_06_Limite_Centrale.html",
    "title": "05-06: Limite Centrale",
    "section": "",
    "text": "import numpy as np\nfrom scipy.stats import norm, t\nimport matplotlib.pyplot as plt\n\n\nepochs = 1_000\nthreshold = 10\n\nsamples = []\n\nfor _ in range(epochs):\n\n    count = 0\n    summ = 0\n    while summ &lt;= threshold:\n        \n        summ += np.random.uniform(0,1)\n        count += 1\n\n        if summ &gt;= threshold:\n            samples.append(count)\n\n\nsamples = np.array(samples)\n\n\nmean = np.mean(samples)\nstd = np.std(samples, ddof=1)\nn = samples.size\n\n\nstd\n\nnp.float64(2.6760299206354428)\n\n\n\nplt.hist(samples)\nplt.show()\n\n\n\n\n\n\n\n\n\nx_gauss = np.linspace(mean-3*std,mean+3*std, 100)\ny_gauss = norm.pdf(x_gauss, loc=mean, scale=std)\n\n\nx_t = np.linspace(mean-3*std,mean+3*std, 100)\ny_t = t.pdf(x_t, n-1, loc=mean, scale=std)\n\n\nplt.hist(samples, density=True)\nplt.plot(x_t, y_t, \"y--\")\nplt.plot(x_gauss, y_gauss, \"r--\")\nplt.show()",
    "crumbs": [
      "Laboratory",
      "05-06: Limite Centrale"
    ]
  },
  {
    "objectID": "th/15. ANOVA a due vie (con o senza repliche).html",
    "href": "th/15. ANOVA a due vie (con o senza repliche).html",
    "title": "ANOVA a due vie (con o senza repliche)",
    "section": "",
    "text": "ANOVA a due vie (con o senza repliche)\nL’analisi della varianza a due vie estende l’ANOVA a una via per studiare l’effetto congiunto di due variabili categoriche sulla variabile risposta \\(Y\\), che deve essere numerica e distribuita approssimativamente secondo una normale, con varianza costante.\n\nStruttura del problema\nSi considerano due variabili categoriche: - \\(x_1\\) con \\(m\\) livelli distinti - \\(x_2\\) con \\(n\\) livelli distinti\nPer ogni combinazione dei livelli di \\(x_1\\) e \\(x_2\\), si raccolgono \\(l\\) osservazioni (dette repliche). Il numero totale di osservazioni sarà:\n\\[\nN = m \\cdot n \\cdot l\n\\]\nSe \\(l = 1\\) si parla di ANOVA senza repliche. Se invece \\(l \\ge 2\\), si parla di ANOVA con repliche, e diventa possibile valutare anche l’interazione tra i due fattori.\nI dati possono essere presentati in due formati: - Formato largo (“unstacked”): una tabella \\(m \\times n\\) in cui ogni cella contiene \\(l\\) osservazioni. - Formato lungo (“stacked”): una tabella con \\(N\\) righe, dove per ogni osservazione si annotano il livello di \\(x_1\\), il livello di \\(x_2\\) e il valore di \\(Y\\).\nNel caso in cui il numero di repliche non sia costante per ogni combinazione, è spesso necessario riequilibrare i dati, scartando le osservazioni in eccesso per ottenere una struttura regolare.\n\n\nModello generale\nNel caso più completo, ogni combinazione di \\((x_1 = i, x_2 = j)\\) ha una propria media \\(\\mu_{ij}\\), e le osservazioni sono indipendenti e normalmente distribuite:\n\\[\nY_{ijk} \\sim \\mathcal{N}(\\mu_{ij}, \\sigma^2), \\quad i = 1,...,m;\\ j = 1,...,n;\\ k = 1,...,l\n\\]\nQuesto modello, detto completo con interazioni, ha \\(m \\cdot n\\) medie da stimare (più la varianza \\(\\sigma^2\\)), e quindi richiede \\(l \\ge 2\\) per poter stimare anche la variabilità residua.\n\n\nModello additivo (senza interazione)\nSe si ipotizza che non ci sia interazione tra le due variabili, si può semplificare il modello scrivendo:\n\\[\n\\mu_{ij} = \\mu + \\alpha_i + \\beta_j\n\\]\nQui: - \\(\\mu\\) è la media globale - \\(\\alpha_i\\) rappresenta l’effetto del livello \\(i\\) della prima variabile (\\(x_1\\)) - \\(\\beta_j\\) rappresenta l’effetto del livello \\(j\\) della seconda variabile (\\(x_2\\))\nPer rendere il modello identificabile si impongono le condizioni:\n\\[\n\\sum_{i=1}^m \\alpha_i = 0, \\quad \\sum_{j=1}^n \\beta_j = 0\n\\]\nQuesta formulazione è molto utile perché permette di interpretare separatamente gli effetti delle due variabili, ed è utilizzabile anche quando non si hanno repliche (\\(l = 1\\)), a patto che non ci siano interazioni significative.\n\n\nModello con interazione\nQuando l’effetto di una variabile dipende dal livello dell’altra, si introduce un termine di interazione \\(\\gamma_{ij}\\):\n\\[\n\\mu_{ij} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij}\n\\]\nIn questo caso: - ogni \\(\\gamma_{ij}\\) misura la deviazione rispetto al modello additivo per la cella \\((i, j)\\) - si impongono i vincoli:\n\\[\n\\sum_i \\gamma_{ij} = 0, \\quad \\sum_j \\gamma_{ij} = 0\n\\]\nL’aggiunta di \\(\\gamma_{ij}\\) consente al modello di adattarsi meglio ai dati, ma rende più complessa l’interpretazione.\n\n\nTest per l’interazione\nPrima di scegliere tra il modello additivo e quello con interazione, si effettua un test con le seguenti ipotesi:\n\n\\(H_0\\): tutte le \\(\\gamma_{ij} = 0\\) (nessuna interazione)\n\\(H_1\\): almeno una \\(\\gamma_{ij} \\ne 0\\)\n\nSe \\(H_0\\) viene accettata, si può utilizzare il modello additivo, riducendo anche il numero di parametri. In questo caso, se \\(l \\ge 2\\), si può persino calcolare la media per ogni combinazione \\((i, j)\\) e ridurre l’analisi a una ANOVA senza repliche (\\(l = 1\\)) usando le medie.\nSe invece l’interazione risulta significativa, il modello additivo non è più valido e si deve mantenere il modello completo.\nL’analisi della varianza a due vie prevede il confronto tra le devianze associate agli effetti riga, colonna, interazione ed errore. La struttura della devianza dipende dalla presenza o meno di repliche.\n\n\nCaso senza repliche (\\(l = 1\\))\nIn assenza di repliche, non è possibile stimare l’interazione tra i fattori. Il modello è quindi il seguente:\n\\[\nY_{ij} \\sim \\mathcal{N}(\\mu + \\alpha_i + \\beta_j,\\ \\sigma^2)\n\\]\nLe devianze si calcolano come:\n\nDevianza totale: \\[\nSS_T = \\sum_{i,j} (Y_{ij} - \\bar{Y}_{**})^2\n\\]\nDevianza riga: \\[\nSS_R = n \\sum_i (\\bar{Y}_{i*} - \\bar{Y}_{**})^2\n\\]\nDevianza colonna: \\[\nSS_C = m \\sum_j (\\bar{Y}_{*j} - \\bar{Y}_{**})^2\n\\]\nDevianza errore: \\[\nSS_E = \\sum_{i,j} (Y_{ij} - \\bar{Y}_{i*} - \\bar{Y}_{*j} + \\bar{Y}_{**})^2\n\\]\n\nGradi di libertà associati:\nFonte di variabilità | Gradi di libertà |\n|-|| | Riga | \\(m - 1\\) | | Colonna | \\(n - 1\\) | | Errore | \\((m - 1)(n - 1)\\) | | Totale | \\(mn - 1\\) |\nStatistica del test (effetto riga):\n\\[\nF_R = \\frac{SS_R / (m - 1)}{SS_E / [(m - 1)(n - 1)]} \\sim F(m-1, (m-1)(n-1))\n\\]\nAnalogo il test per l’effetto colonna.\n\n\nCaso con repliche (\\(l \\ge 2\\))\nCon repliche si può stimare anche la componente di interazione. Le osservazioni sono:\n\\[\nY_{ijk} \\sim \\mathcal{N}(\\mu + \\alpha_i + \\beta_j + \\gamma_{ij},\\ \\sigma^2)\n\\]\nLe devianze si calcolano come:\n\nDevianza riga: \\[\nSS_R = n l \\sum_i (\\bar{Y}_{i**} - \\bar{Y}_{***})^2\n\\]\nDevianza colonna: \\[\nSS_C = m l \\sum_j (\\bar{Y}_{*j*} - \\bar{Y}_{***})^2\n\\]\nDevianza interazione: \\[\nSS_{Int} = l \\sum_{i,j} (\\bar{Y}_{ij*} - \\bar{Y}_{i**} - \\bar{Y}_{*j*} + \\bar{Y}_{***})^2\n\\]\nDevianza errore: \\[\nSS_E = \\sum_{i,j,k} (Y_{ijk} - \\bar{Y}_{ij*})^2\n\\]\n\nGradi di libertà associati:\n\n\n\nFonte di variabilità\nGradi di libertà\n\n\n\n\nRiga\n\\(m - 1\\)\n\n\nColonna\n\\(n - 1\\)\n\n\nInterazione\n\\((m - 1)(n - 1)\\)\n\n\nErrore\n\\(mn(l - 1)\\)\n\n\nTotale\n\\(mnl - 1\\)\n\n\n\nLe statistiche dei test \\(F\\):\n\nPer l’effetto riga:\n\n\\[\nF_R = \\frac{SS_R / (m - 1)}{SS_E / [mn(l - 1)]} \\sim F(m-1, mn(l-1))\n\\]\n\nPer l’effetto colonna:\n\n\\[\nF_C = \\frac{SS_C / (n - 1)}{SS_E / [mn(l - 1)]} \\sim F(n-1, mn(l-1))\n\\]\n\nPer l’interazione:\n\n\\[\nF_{Int} = \\frac{SS_{Int} / [(m - 1)(n - 1)]}{SS_E / [mn(l - 1)]} \\sim F((m-1)(n-1), mn(l-1))\n\\]\n\n\nStimatori\nPer la varianza dell’errore (valido in entrambi i casi):\n\\[\n\\hat{\\sigma}^2 = \\frac{SS_E}{gdl_E}\n\\]\ndove \\(gdl_E\\) sono i gradi di libertà associati all’errore: - \\((m - 1)(n - 1)\\) nel caso senza repliche - \\(mn(l - 1)\\) nel caso con repliche\nLe medie campionarie, invece, sono: - \\(\\bar{Y}_{i**}\\) = media dei dati nella riga \\(i\\) - \\(\\bar{Y}_{*j*}\\) = media dei dati nella colonna \\(j\\) - \\(\\bar{Y}_{ij*}\\) = media dei dati nella cella \\((i,j)\\) - \\(\\bar{Y}_{***}\\) = media globale di tutti i dati\n\n\nConsiderazioni\nSe il test sull’interazione non risulta significativo, è possibile utilizzare il modello additivo, più semplice da interpretare. In quel caso si possono anche ridurre i dati sostituendo le osservazioni con le medie per cella e applicare una ANOVA senza repliche.\nSe invece l’interazione è significativa, bisogna mantenere il modello completo. In questo caso, l’interpretazione dei singoli effetti diventa più complessa, perché l’effetto di una variabile dipende dal livello dell’altra.\nLa correttezza dell’intera procedura dipende dalle ipotesi classiche dell’ANOVA: normalità dei residui, indipendenza e omoschedasticità.",
    "crumbs": [
      "Theory",
      "ANOVA a due vie (con o senza repliche)"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "I metodi stepwise sono procedure automatiche per la selezione delle variabili in un modello di regressione. L’obiettivo è costruire un modello che sia sufficientemente parsimonioso, evitando variabili inutili, ma che mantenga una buona capacità di adattamento ai dati.\nNel backward stepwise si parte dal modello più complesso possibile, che include tutte le \\(p\\) variabili esplicative. A ogni iterazione viene rimossa la variabile meno significativa, tipicamente quella con il contributo statistico più debole, finché tutte le variabili rimanenti soddisfano un criterio di significatività prefissato.\nNel forward stepwise, invece, si parte dal modello nullo, privo di regressori. Le variabili vengono aggiunte una alla volta, scegliendo a ogni passo quella che produce il maggiore miglioramento del modello secondo un criterio statistico. Il procedimento si arresta quando nessuna variabile candidata apporta un miglioramento ritenuto significativo.\nIl metodo misto (o stepwise propriamente detto) combina i due approcci. L’inclusione delle variabili avviene in modo analogo al forward, ma dopo ogni nuova aggiunta si verifica se alcune delle variabili già presenti siano diventate non significative. In tal caso, queste possono essere rimosse, rendendo il procedimento più flessibile.\nNella pratica, i software statistici non si basano sempre direttamente sui p-value, ma utilizzano spesso soglie sul test F, note come F to enter e F to remove. Tali soglie sono equivalenti all’introduzione di livelli di significatività \\(\\alpha_{\\text{in}}\\) per l’ingresso e \\(\\alpha_{\\text{out}}\\) per l’uscita delle variabili. Poiché le decisioni dipendono dai valori scelti per queste\n\n\nIl metodo backward per la selezione delle variabili si fonda sui p-value associati ai test d’ipotesi sui singoli coefficienti di regressione. Per ciascun regressore si considera il test:\n\\[\nH_0:\\ \\beta_j = 0 \\quad \\text{contro} \\quad H_1:\\ \\beta_j \\neq 0,\n\\]\nche verifica se la variabile \\(X_j\\) fornisce un contributo statisticamente significativo al modello, una volta tenute fisse le altre.\nDal punto di vista operativo, si parte dal modello completo, contenente tutte le \\(p\\) variabili esplicative. A ogni iterazione si esaminano i p-value stimati \\(\\alpha_j^*\\) e si decide se mantenere o rimuovere una variabile in base a soglie prefissate. Una prassi comune prevede che, se \\(\\alpha_j^* &lt; 0.001\\), la variabile venga chiaramente mantenuta nel modello; in contesti più conservativi si può adottare una correzione di Bonferroni, con una soglia approssimativa \\(\\alpha \\approx \\tfrac{5\\%}{p}\\), per tenere conto della molteplicità dei test. Se il p-value rientra in una fascia intermedia, ad esempio \\(0.1\\% \\le \\alpha_j^* &lt; 30\\%\\), la decisione non è netta e spesso la variabile viene comunque mantenuta. Valori elevati, tipicamente \\(\\alpha_j^* \\ge 30\\%\\), suggeriscono invece che il contributo della variabile sia trascurabile e che essa possa essere eliminata.\nL’algoritmo procede rimuovendo, a ogni passo, la variabile con il p-value più alto, ricalcolando poi il modello ridotto e i nuovi p-value. Il procedimento si arresta quando tutte le variabili rimanenti risultano significative secondo la soglia scelta.\nUn aspetto critico è la multicollinearità. In presenza di forti correlazioni tra i regressori, gli stimatori dei coefficienti diventano instabili e i p-value possono risultare artificialmente elevati. Di conseguenza, il metodo backward può eliminare variabili che sarebbero rilevanti dal punto di vista sostantivo, rendendo la selezione fortemente dipendente dalla struttura di correlazione dei dati.\n\n\n\nIl metodo forward costruisce il modello in modo incrementale, aggiungendo una variabile alla volta. Si parte dal modello nullo, che contiene solo l’intercetta, e si valuta quale variabile, tra quelle non ancora incluse, produce il maggior miglioramento del modello.\nIl miglioramento può essere misurato tramite indicatori globali di adattamento, come la riduzione dell’errore standard della regressione \\(S_e\\) oppure l’aumento del coefficiente di determinazione \\(R^2\\). Parallelamente, si richiede che l’ingresso della variabile sia supportato da una significatività statistica, valutata tramite il p-value del coefficiente o tramite un test F che confronta il modello corrente con quello arricchito dalla nuova variabile.\nLa prima variabile inserita è quindi quella che, da sola, spiega meglio la risposta. Successivamente, a ogni iterazione si considera l’aggiunta di una nuova variabile al modello già costruito e si seleziona quella che fornisce il miglior contributo addizionale. Il procedimento si arresta quando nessuna delle variabili rimanenti è in grado di produrre un miglioramento statisticamente significativo secondo la soglia scelta.\nÈ importante notare che il metodo forward non riconsidera mai l’esclusione delle variabili già inserite. Per questo motivo, in presenza di interazioni o collinearità tra regressori, il percorso di selezione può differire sensibilmente da quello del metodo backward. Di conseguenza, forward e backward possono condurre a modelli finali diversi, anche partendo dallo stesso insieme iniziale di variabili.\n\n\n\n\nI metodi globali affrontano il problema della selezione delle variabili valutando tutti i possibili sottoinsiemi di regressori. Dato un insieme di \\(p\\) variabili esplicative, ciò equivale a considerare \\(2^p\\) modelli distinti, dal modello nullo a quello completo. Questo approccio è concettualmente più completo rispetto ai metodi stepwise, ma diventa rapidamente computazionalmente oneroso all’aumentare di \\(p\\).\nLa scelta del modello finale avviene confrontando i diversi sottoinsiemi tramite criteri globali di bontà del modello. Un primo criterio è il minimo errore standard della regressione \\(S_e\\), che privilegia modelli con residui mediamente più piccoli. Un’alternativa è il massimo \\(R_a^2\\), il coefficiente di determinazione corretto, che penalizza l’aggiunta di variabili superflue e riduce l’ottimismo del semplice \\(R^2\\).\nSono molto utilizzati anche criteri informativi come AIC e BIC, che bilanciano qualità dell’adattamento e complessità del modello introducendo una penalizzazione esplicita per il numero di parametri. In generale, l’AIC tende a favorire modelli più ricchi, mentre il BIC applica una penalizzazione più severa e porta a modelli più parsimoniosi.\nUn approccio sempre più rilevante è la valutazione delle prestazioni in validazione, ad esempio tramite il valore di \\(R^2\\) su un validation set, l’errore di previsione o misure come l’MSE stimato con cross-validation. Questo sposta l’attenzione dall’adattamento ai dati osservati alla capacità predittiva del modello.\nUn aspetto cruciale è che tutti questi indicatori sono affetti da variabilità campionaria. In assenza di una validazione esterna o di tecniche di ri-campionamento, la scelta del modello può risultare instabile, soprattutto quando le differenze tra modelli sono piccole o quando i regressori sono fortemente correlati.\n\n\n\n\n\nIl coefficiente di determinazione \\(R^2\\) misura la quota di variabilità totale della risposta spiegata dal modello. È definito come\n\\[\nR^2 = 1 - \\frac{SSR}{SSY}, \\qquad\nSSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2, \\qquad\nSSY = \\sum_{i=1}^n (Y_i - \\bar{Y})^2.\n\\]\nIn questa notazione, \\(SSR\\) rappresenta la somma dei residui al quadrato (sum of squared residuals), mentre \\(SSY\\) è la somma dei quadrati totali rispetto alla media campionaria. Il valore di \\(R^2\\) appartiene all’intervallo \\([0,1]\\): valori prossimi a \\(1\\) indicano che il modello spiega una grande parte della variabilità osservata, mentre valori prossimi a \\(0\\) indicano una capacità esplicativa ridotta.\nUna proprietà importante è che \\(R^2\\) non penalizza l’aggiunta di nuove variabili. Inserendo ulteriori regressori, \\(SSR\\) non può aumentare e quindi \\(R^2\\) può solo crescere o rimanere invariato, anche quando le nuove variabili non apportano un contributo informativo reale.\n\n\n\nIl coefficiente di determinazione corretto \\(R_a^2\\) introduce una correzione che tiene conto del numero di parametri stimati nel modello, con l’obiettivo di ridurre il rischio di overfitting. È definito come\n\\[\nR_a^2 = 1 - \\frac{S_e^2}{S_Y^2}, \\qquad\nS_e^2 = \\frac{SSR}{n - p - 1}, \\qquad\nS_Y^2 = \\frac{SSY}{n - 1},\n\\]\ndove \\(p\\) è il numero di regressori esclusa l’intercetta. Qui \\(S_e^2\\) è una stima della varianza dell’errore, mentre \\(S_Y^2\\) rappresenta la varianza campionaria della risposta.\nA differenza di \\(R^2\\), il valore di \\(R_a^2\\) aumenta solo se la variabile aggiunta migliora effettivamente il modello, compensando la perdita di gradi di libertà dovuta all’introduzione di un nuovo parametro. Per questo motivo, \\(R_a^2\\) è spesso preferito a \\(R^2\\) nei confronti tra modelli con diverso numero di regressori.\n\n\n\n\nL’overfitting si verifica quando un modello si adatta in modo eccessivo ai dati di training, catturando anche il rumore campionario, ma mostra una scarsa capacità di generalizzazione su nuovi dati. In questi casi il modello presenta prestazioni apparentemente molto buone sul campione utilizzato per la stima, ma errori elevati in previsione.\nUn primo fattore chiave è il rapporto tra numerosità campionaria e complessità del modello. Mantenere un buon rapporto dati/variabili, espresso come \\(N/P \\gg 1\\), riduce il rischio che il modello disponga di troppi gradi di libertà rispetto alle informazioni effettivamente disponibili nei dati.\nIn presenza di dati molto rumorosi è spesso preferibile adottare modelli più parsimoniosi, evitando l’inclusione indiscriminata di regressori. In questo contesto, il metodo forward può talvolta risultare meno aggressivo del backward, poiché inserisce solo variabili con un contributo incrementale evidente; tuttavia, questa non è una regola generale e il comportamento dipende fortemente dalla struttura dei dati e dalle correlazioni tra regressori.\nStrumenti fondamentali per il controllo dell’overfitting sono la validazione, ad esempio tramite validation set o cross-validation, e la regolarizzazione, che introduce penalizzazioni sui coefficienti per limitare la complessità del modello e stabilizzare le stime.\n\n\n\nLa validazione serve a valutare la capacità di generalizzazione di un modello, cioè quanto bene esso si comporti su dati non utilizzati nella stima. L’idea di fondo è separare, in modo esplicito o implicito, la fase di apprendimento da quella di valutazione.\nUn primo approccio è l’uso di un validation set, in cui una frazione dei dati, tipicamente intorno al 10–20%, viene tenuta da parte e non utilizzata per stimare il modello. Il modello è addestrato sul training set e valutato sui dati lasciati fuori, ottenendo una stima diretta dell’errore di previsione.\nUn metodo più stabile è la cross-validation \\(k\\)-fold. Il campione viene suddiviso in \\(k\\) blocchi di dimensione simile; a ogni iterazione uno dei blocchi funge da validation set mentre i restanti \\(k-1\\) sono usati per il training. L’errore di generalizzazione è stimato mediando le prestazioni ottenute sui diversi blocchi. Questo approccio riduce la dipendenza della valutazione da una singola partizione dei dati.\nIl caso limite della cross-validation è la leave-one-out cross-validation (LOOCV), in cui si pone \\(k=n\\). A ogni iterazione si lascia fuori un solo punto e si stima il modello sugli altri \\(n-1\\) dati. La LOOCV utilizza quasi tutta l’informazione disponibile per il training, ma può risultare computazionalmente costosa e con varianza elevata della stima dell’errore.\nÈ importante distinguere la validazione predittiva da altre tecniche di ri-campionamento. Il termine Jackknife è concettualmente correlato, ma ha uno scopo diverso: è utilizzato principalmente per stimare bias e varianza di statistiche, non per valutare le prestazioni predittive di un modello. In ambito di validazione, il riferimento corretto è quindi la LOOCV, non il Jackknife.\n\n\n\nLa regolarizzazione introduce una penalizzazione sui coefficienti del modello con l’obiettivo di contenere la complessità, stabilizzare le stime e prevenire coefficienti di ampiezza eccessiva, fenomeno tipico in presenza di multicollinearità o di un numero elevato di regressori rispetto ai dati disponibili.\n\n\nLa Ridge Regression aggiunge alla funzione di perdita una penalizzazione proporzionale alla somma dei quadrati dei coefficienti. In genere l’intercetta non viene penalizzata. Il problema di ottimizzazione è\n\\[\n\\min_{\\mathbf B}\\ \\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\sum_{j=1}^p B_j X_{ij}\\right)^2\n\\;+\\;\n\\lambda\\sum_{j=1}^p B_j^2.\n\\]\nLa penalizzazione L2 tende a ridurre l’ampiezza dei coefficienti, ma non li annulla esattamente. Il risultato è un modello più stabile, in cui le variabili fortemente correlate condividono il peso esplicativo invece di competere in modo instabile.\n\n\n\nLa Lasso Regression utilizza una penalizzazione basata sulla somma dei valori assoluti dei coefficienti:\n\\[\n\\min_{\\mathbf B}\\ \\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\sum_{j=1}^p B_j X_{ij}\\right)^2\n\\;+\\;\n\\lambda\\sum_{j=1}^p |B_j|.\n\\]\nLa penalizzazione L1 ha una proprietà distintiva: induce sparsità nel vettore dei coefficienti. Alcuni coefficienti vengono spinti esattamente a zero, realizzando di fatto una selezione automatica delle variabili, oltre alla regolarizzazione.\n\n\n\nL’Elastic Net combina le penalizzazioni L1 e L2, unendo i vantaggi di Ridge e Lasso:\n\\[\n\\min_{\\mathbf B}\\ \\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\sum_{j=1}^p B_j X_{ij}\\right)^2\n\\;+\\;\n\\lambda_1\\sum_{j=1}^p |B_j|\n\\;+\\;\n\\lambda_2\\sum_{j=1}^p B_j^2.\n\\]\nQuesta formulazione è particolarmente utile quando i regressori sono numerosi e fortemente correlati: la componente L2 stabilizza le stime, mentre la componente L1 consente la selezione delle variabili.\nDal punto di vista pratico, i regressori vengono standardizzati per rendere confrontabili le penalizzazioni tra coefficienti, l’intercetta non viene penalizzata e i parametri di regolarizzazione \\(\\lambda\\) (e il bilanciamento tra L1 e L2 nell’Elastic Net) vengono scelti tramite cross-validation, ottimizzando le prestazioni predittive del modello.\n\n\n\n\nIl double descent è un fenomeno osservato in modelli ad altissima capacità, in cui l’errore di generalizzazione non segue l’andamento classico a U, ma mostra due fasi distinte. Al crescere della complessità del modello, l’errore inizialmente diminuisce, poi aumenta in prossimità della soglia di interpolazione (dove il modello riesce a fittare esattamente i dati di training) e infine ridiscende quando la capacità continua ad aumentare.\nLa prima fase corrisponde al regime classico bias–variance: aumentando la complessità si riduce il bias ma cresce la varianza, portando a overfitting. Il picco di errore si verifica tipicamente quando il numero di parametri è comparabile o supera di poco il numero di osservazioni, e il modello interpola i dati di training.\nNel regime successivo, detto overparameterized, l’errore di generalizzazione può sorprendentemente diminuire. Questo comportamento è stato osservato soprattutto in presenza di regolarizzazione implicita, come quella indotta dagli algoritmi di ottimizzazione (ad esempio gradient descent), dall’early stopping o dalla struttura stessa dei modelli. In questo regime, pur avendo capacità sufficiente per interpolare, il modello tende a convergere verso soluzioni con buone proprietà di generalizzazione.\nIl fenomeno del double descent è particolarmente rilevante nei modelli moderni ad alta dimensionalità, come le reti neurali profonde e i modelli di grandi dimensioni, e mostra che la relazione tra complessità del modello e generalizzazione è più articolata di quanto suggerito dalla visione classica dell’overfitting.\n\n\n\nLa regressione polinomiale estende la regressione lineare introducendo termini polinomiali delle variabili esplicative, consentendo di modellare relazioni non lineari pur rimanendo in un quadro lineare nei parametri. Nel caso univariato, il modello di grado \\(d\\) è\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_d x^d.\n\\]\nNonostante la non linearità in \\(x\\), il modello resta lineare rispetto ai coefficienti \\(\\beta_j\\), e può quindi essere stimato con i metodi usuali dei minimi quadrati.\nUna regola importante è la regola gerarchica: se si include un termine di grado \\(d\\), è buona pratica includere anche tutti i termini di grado inferiore. Questo garantisce interpretabilità e stabilità del modello, evitando specificazioni incoerenti come la presenza di \\(x^2\\) senza il termine lineare \\(x\\).\nNel caso multivariato, la regressione polinomiale può includere anche termini di interazione tra variabili, ad esempio \\(x_1 x_2\\), che permettono di modellare effetti combinati in cui l’influenza di una variabile dipende dal livello di un’altra. Anche per le interazioni vale una forma di gerarchia: includere un termine di interazione implica includere i termini principali corrispondenti.\nL’aumento del grado polinomiale o del numero di interazioni incrementa rapidamente la complessità del modello e il numero di parametri. Per questo motivo la regressione polinomiale è particolarmente soggetta a overfitting quando il grado è troppo elevato rispetto alla numerosità campionaria. In pratica, la scelta del grado e dei termini da includere dovrebbe essere guidata da validazione e, quando necessario, da tecniche di regolarizzazione per controllare la variabilità delle stime.\n\n\n\nQuando l’ipotesi di omoschedasticità non è soddisfatta, cioè quando la varianza degli errori non è costante tra le osservazioni, la stima OLS non è più efficiente. In questi casi si utilizza la regressione pesata (Weighted Least Squares, WLS), che assegna pesi diversi alle osservazioni in funzione della loro variabilità.\nL’idea è minimizzare una somma dei quadrati dei residui pesata:\n\\[\nSSR_W = \\sum_{i=1}^n W_i\\left(Y_i - \\sum_{j=1}^p B_j X_{ij}\\right)^2,\n\\qquad\nW_i = \\frac{1}{\\sigma_i^2},\n\\]\ndove \\(\\sigma_i^2\\) è la varianza dell’errore associato all’osservazione \\(i\\)-esima. In questo modo, le osservazioni con varianza maggiore ricevono un peso minore e influenzano meno la stima dei coefficienti.\nLa WLS è equivalente a una trasformazione dei dati. Definendo\n\\[\n\\tilde{X}_{ij} = \\frac{X_{ij}}{\\sigma_i},\n\\qquad\n\\tilde{Y}_i = \\frac{Y_i}{\\sigma_i},\n\\]\nil problema di minimizzazione pesata si riconduce a una regressione OLS applicata ai dati trasformati. Questa equivalenza chiarisce che la regressione pesata ristabilisce, sui dati trasformati, l’ipotesi di varianza costante degli errori.\nIn pratica, le varianze \\(\\sigma_i^2\\) non sono quasi mai note. In tali situazioni possono essere stimate, ad esempio specificando un modello per la varianza in funzione delle covariate o dei valori predetti, e utilizzate per definire i pesi. Il procedimento può essere iterato: si stima il modello, si aggiornano le varianze e i pesi, e si ricalcola la regressione fino a convergenza.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#metodi-stepwise",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#metodi-stepwise",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "I metodi stepwise sono procedure automatiche per la selezione delle variabili in un modello di regressione. L’obiettivo è costruire un modello che sia sufficientemente parsimonioso, evitando variabili inutili, ma che mantenga una buona capacità di adattamento ai dati.\nNel backward stepwise si parte dal modello più complesso possibile, che include tutte le \\(p\\) variabili esplicative. A ogni iterazione viene rimossa la variabile meno significativa, tipicamente quella con il contributo statistico più debole, finché tutte le variabili rimanenti soddisfano un criterio di significatività prefissato.\nNel forward stepwise, invece, si parte dal modello nullo, privo di regressori. Le variabili vengono aggiunte una alla volta, scegliendo a ogni passo quella che produce il maggiore miglioramento del modello secondo un criterio statistico. Il procedimento si arresta quando nessuna variabile candidata apporta un miglioramento ritenuto significativo.\nIl metodo misto (o stepwise propriamente detto) combina i due approcci. L’inclusione delle variabili avviene in modo analogo al forward, ma dopo ogni nuova aggiunta si verifica se alcune delle variabili già presenti siano diventate non significative. In tal caso, queste possono essere rimosse, rendendo il procedimento più flessibile.\nNella pratica, i software statistici non si basano sempre direttamente sui p-value, ma utilizzano spesso soglie sul test F, note come F to enter e F to remove. Tali soglie sono equivalenti all’introduzione di livelli di significatività \\(\\alpha_{\\text{in}}\\) per l’ingresso e \\(\\alpha_{\\text{out}}\\) per l’uscita delle variabili. Poiché le decisioni dipendono dai valori scelti per queste\n\n\nIl metodo backward per la selezione delle variabili si fonda sui p-value associati ai test d’ipotesi sui singoli coefficienti di regressione. Per ciascun regressore si considera il test:\n\\[\nH_0:\\ \\beta_j = 0 \\quad \\text{contro} \\quad H_1:\\ \\beta_j \\neq 0,\n\\]\nche verifica se la variabile \\(X_j\\) fornisce un contributo statisticamente significativo al modello, una volta tenute fisse le altre.\nDal punto di vista operativo, si parte dal modello completo, contenente tutte le \\(p\\) variabili esplicative. A ogni iterazione si esaminano i p-value stimati \\(\\alpha_j^*\\) e si decide se mantenere o rimuovere una variabile in base a soglie prefissate. Una prassi comune prevede che, se \\(\\alpha_j^* &lt; 0.001\\), la variabile venga chiaramente mantenuta nel modello; in contesti più conservativi si può adottare una correzione di Bonferroni, con una soglia approssimativa \\(\\alpha \\approx \\tfrac{5\\%}{p}\\), per tenere conto della molteplicità dei test. Se il p-value rientra in una fascia intermedia, ad esempio \\(0.1\\% \\le \\alpha_j^* &lt; 30\\%\\), la decisione non è netta e spesso la variabile viene comunque mantenuta. Valori elevati, tipicamente \\(\\alpha_j^* \\ge 30\\%\\), suggeriscono invece che il contributo della variabile sia trascurabile e che essa possa essere eliminata.\nL’algoritmo procede rimuovendo, a ogni passo, la variabile con il p-value più alto, ricalcolando poi il modello ridotto e i nuovi p-value. Il procedimento si arresta quando tutte le variabili rimanenti risultano significative secondo la soglia scelta.\nUn aspetto critico è la multicollinearità. In presenza di forti correlazioni tra i regressori, gli stimatori dei coefficienti diventano instabili e i p-value possono risultare artificialmente elevati. Di conseguenza, il metodo backward può eliminare variabili che sarebbero rilevanti dal punto di vista sostantivo, rendendo la selezione fortemente dipendente dalla struttura di correlazione dei dati.\n\n\n\nIl metodo forward costruisce il modello in modo incrementale, aggiungendo una variabile alla volta. Si parte dal modello nullo, che contiene solo l’intercetta, e si valuta quale variabile, tra quelle non ancora incluse, produce il maggior miglioramento del modello.\nIl miglioramento può essere misurato tramite indicatori globali di adattamento, come la riduzione dell’errore standard della regressione \\(S_e\\) oppure l’aumento del coefficiente di determinazione \\(R^2\\). Parallelamente, si richiede che l’ingresso della variabile sia supportato da una significatività statistica, valutata tramite il p-value del coefficiente o tramite un test F che confronta il modello corrente con quello arricchito dalla nuova variabile.\nLa prima variabile inserita è quindi quella che, da sola, spiega meglio la risposta. Successivamente, a ogni iterazione si considera l’aggiunta di una nuova variabile al modello già costruito e si seleziona quella che fornisce il miglior contributo addizionale. Il procedimento si arresta quando nessuna delle variabili rimanenti è in grado di produrre un miglioramento statisticamente significativo secondo la soglia scelta.\nÈ importante notare che il metodo forward non riconsidera mai l’esclusione delle variabili già inserite. Per questo motivo, in presenza di interazioni o collinearità tra regressori, il percorso di selezione può differire sensibilmente da quello del metodo backward. Di conseguenza, forward e backward possono condurre a modelli finali diversi, anche partendo dallo stesso insieme iniziale di variabili.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#metodi-globali",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#metodi-globali",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "I metodi globali affrontano il problema della selezione delle variabili valutando tutti i possibili sottoinsiemi di regressori. Dato un insieme di \\(p\\) variabili esplicative, ciò equivale a considerare \\(2^p\\) modelli distinti, dal modello nullo a quello completo. Questo approccio è concettualmente più completo rispetto ai metodi stepwise, ma diventa rapidamente computazionalmente oneroso all’aumentare di \\(p\\).\nLa scelta del modello finale avviene confrontando i diversi sottoinsiemi tramite criteri globali di bontà del modello. Un primo criterio è il minimo errore standard della regressione \\(S_e\\), che privilegia modelli con residui mediamente più piccoli. Un’alternativa è il massimo \\(R_a^2\\), il coefficiente di determinazione corretto, che penalizza l’aggiunta di variabili superflue e riduce l’ottimismo del semplice \\(R^2\\).\nSono molto utilizzati anche criteri informativi come AIC e BIC, che bilanciano qualità dell’adattamento e complessità del modello introducendo una penalizzazione esplicita per il numero di parametri. In generale, l’AIC tende a favorire modelli più ricchi, mentre il BIC applica una penalizzazione più severa e porta a modelli più parsimoniosi.\nUn approccio sempre più rilevante è la valutazione delle prestazioni in validazione, ad esempio tramite il valore di \\(R^2\\) su un validation set, l’errore di previsione o misure come l’MSE stimato con cross-validation. Questo sposta l’attenzione dall’adattamento ai dati osservati alla capacità predittiva del modello.\nUn aspetto cruciale è che tutti questi indicatori sono affetti da variabilità campionaria. In assenza di una validazione esterna o di tecniche di ri-campionamento, la scelta del modello può risultare instabile, soprattutto quando le differenze tra modelli sono piccole o quando i regressori sono fortemente correlati.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#coefficienti-di-determinazione",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#coefficienti-di-determinazione",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "Il coefficiente di determinazione \\(R^2\\) misura la quota di variabilità totale della risposta spiegata dal modello. È definito come\n\\[\nR^2 = 1 - \\frac{SSR}{SSY}, \\qquad\nSSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2, \\qquad\nSSY = \\sum_{i=1}^n (Y_i - \\bar{Y})^2.\n\\]\nIn questa notazione, \\(SSR\\) rappresenta la somma dei residui al quadrato (sum of squared residuals), mentre \\(SSY\\) è la somma dei quadrati totali rispetto alla media campionaria. Il valore di \\(R^2\\) appartiene all’intervallo \\([0,1]\\): valori prossimi a \\(1\\) indicano che il modello spiega una grande parte della variabilità osservata, mentre valori prossimi a \\(0\\) indicano una capacità esplicativa ridotta.\nUna proprietà importante è che \\(R^2\\) non penalizza l’aggiunta di nuove variabili. Inserendo ulteriori regressori, \\(SSR\\) non può aumentare e quindi \\(R^2\\) può solo crescere o rimanere invariato, anche quando le nuove variabili non apportano un contributo informativo reale.\n\n\n\nIl coefficiente di determinazione corretto \\(R_a^2\\) introduce una correzione che tiene conto del numero di parametri stimati nel modello, con l’obiettivo di ridurre il rischio di overfitting. È definito come\n\\[\nR_a^2 = 1 - \\frac{S_e^2}{S_Y^2}, \\qquad\nS_e^2 = \\frac{SSR}{n - p - 1}, \\qquad\nS_Y^2 = \\frac{SSY}{n - 1},\n\\]\ndove \\(p\\) è il numero di regressori esclusa l’intercetta. Qui \\(S_e^2\\) è una stima della varianza dell’errore, mentre \\(S_Y^2\\) rappresenta la varianza campionaria della risposta.\nA differenza di \\(R^2\\), il valore di \\(R_a^2\\) aumenta solo se la variabile aggiunta migliora effettivamente il modello, compensando la perdita di gradi di libertà dovuta all’introduzione di un nuovo parametro. Per questo motivo, \\(R_a^2\\) è spesso preferito a \\(R^2\\) nei confronti tra modelli con diverso numero di regressori.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#overfitting",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#overfitting",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "L’overfitting si verifica quando un modello si adatta in modo eccessivo ai dati di training, catturando anche il rumore campionario, ma mostra una scarsa capacità di generalizzazione su nuovi dati. In questi casi il modello presenta prestazioni apparentemente molto buone sul campione utilizzato per la stima, ma errori elevati in previsione.\nUn primo fattore chiave è il rapporto tra numerosità campionaria e complessità del modello. Mantenere un buon rapporto dati/variabili, espresso come \\(N/P \\gg 1\\), riduce il rischio che il modello disponga di troppi gradi di libertà rispetto alle informazioni effettivamente disponibili nei dati.\nIn presenza di dati molto rumorosi è spesso preferibile adottare modelli più parsimoniosi, evitando l’inclusione indiscriminata di regressori. In questo contesto, il metodo forward può talvolta risultare meno aggressivo del backward, poiché inserisce solo variabili con un contributo incrementale evidente; tuttavia, questa non è una regola generale e il comportamento dipende fortemente dalla struttura dei dati e dalle correlazioni tra regressori.\nStrumenti fondamentali per il controllo dell’overfitting sono la validazione, ad esempio tramite validation set o cross-validation, e la regolarizzazione, che introduce penalizzazioni sui coefficienti per limitare la complessità del modello e stabilizzare le stime.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#validazione",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#validazione",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "La validazione serve a valutare la capacità di generalizzazione di un modello, cioè quanto bene esso si comporti su dati non utilizzati nella stima. L’idea di fondo è separare, in modo esplicito o implicito, la fase di apprendimento da quella di valutazione.\nUn primo approccio è l’uso di un validation set, in cui una frazione dei dati, tipicamente intorno al 10–20%, viene tenuta da parte e non utilizzata per stimare il modello. Il modello è addestrato sul training set e valutato sui dati lasciati fuori, ottenendo una stima diretta dell’errore di previsione.\nUn metodo più stabile è la cross-validation \\(k\\)-fold. Il campione viene suddiviso in \\(k\\) blocchi di dimensione simile; a ogni iterazione uno dei blocchi funge da validation set mentre i restanti \\(k-1\\) sono usati per il training. L’errore di generalizzazione è stimato mediando le prestazioni ottenute sui diversi blocchi. Questo approccio riduce la dipendenza della valutazione da una singola partizione dei dati.\nIl caso limite della cross-validation è la leave-one-out cross-validation (LOOCV), in cui si pone \\(k=n\\). A ogni iterazione si lascia fuori un solo punto e si stima il modello sugli altri \\(n-1\\) dati. La LOOCV utilizza quasi tutta l’informazione disponibile per il training, ma può risultare computazionalmente costosa e con varianza elevata della stima dell’errore.\nÈ importante distinguere la validazione predittiva da altre tecniche di ri-campionamento. Il termine Jackknife è concettualmente correlato, ma ha uno scopo diverso: è utilizzato principalmente per stimare bias e varianza di statistiche, non per valutare le prestazioni predittive di un modello. In ambito di validazione, il riferimento corretto è quindi la LOOCV, non il Jackknife.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#regolarizzazione",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#regolarizzazione",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "La regolarizzazione introduce una penalizzazione sui coefficienti del modello con l’obiettivo di contenere la complessità, stabilizzare le stime e prevenire coefficienti di ampiezza eccessiva, fenomeno tipico in presenza di multicollinearità o di un numero elevato di regressori rispetto ai dati disponibili.\n\n\nLa Ridge Regression aggiunge alla funzione di perdita una penalizzazione proporzionale alla somma dei quadrati dei coefficienti. In genere l’intercetta non viene penalizzata. Il problema di ottimizzazione è\n\\[\n\\min_{\\mathbf B}\\ \\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\sum_{j=1}^p B_j X_{ij}\\right)^2\n\\;+\\;\n\\lambda\\sum_{j=1}^p B_j^2.\n\\]\nLa penalizzazione L2 tende a ridurre l’ampiezza dei coefficienti, ma non li annulla esattamente. Il risultato è un modello più stabile, in cui le variabili fortemente correlate condividono il peso esplicativo invece di competere in modo instabile.\n\n\n\nLa Lasso Regression utilizza una penalizzazione basata sulla somma dei valori assoluti dei coefficienti:\n\\[\n\\min_{\\mathbf B}\\ \\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\sum_{j=1}^p B_j X_{ij}\\right)^2\n\\;+\\;\n\\lambda\\sum_{j=1}^p |B_j|.\n\\]\nLa penalizzazione L1 ha una proprietà distintiva: induce sparsità nel vettore dei coefficienti. Alcuni coefficienti vengono spinti esattamente a zero, realizzando di fatto una selezione automatica delle variabili, oltre alla regolarizzazione.\n\n\n\nL’Elastic Net combina le penalizzazioni L1 e L2, unendo i vantaggi di Ridge e Lasso:\n\\[\n\\min_{\\mathbf B}\\ \\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\sum_{j=1}^p B_j X_{ij}\\right)^2\n\\;+\\;\n\\lambda_1\\sum_{j=1}^p |B_j|\n\\;+\\;\n\\lambda_2\\sum_{j=1}^p B_j^2.\n\\]\nQuesta formulazione è particolarmente utile quando i regressori sono numerosi e fortemente correlati: la componente L2 stabilizza le stime, mentre la componente L1 consente la selezione delle variabili.\nDal punto di vista pratico, i regressori vengono standardizzati per rendere confrontabili le penalizzazioni tra coefficienti, l’intercetta non viene penalizzata e i parametri di regolarizzazione \\(\\lambda\\) (e il bilanciamento tra L1 e L2 nell’Elastic Net) vengono scelti tramite cross-validation, ottimizzando le prestazioni predittive del modello.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#double-descent",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#double-descent",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "Il double descent è un fenomeno osservato in modelli ad altissima capacità, in cui l’errore di generalizzazione non segue l’andamento classico a U, ma mostra due fasi distinte. Al crescere della complessità del modello, l’errore inizialmente diminuisce, poi aumenta in prossimità della soglia di interpolazione (dove il modello riesce a fittare esattamente i dati di training) e infine ridiscende quando la capacità continua ad aumentare.\nLa prima fase corrisponde al regime classico bias–variance: aumentando la complessità si riduce il bias ma cresce la varianza, portando a overfitting. Il picco di errore si verifica tipicamente quando il numero di parametri è comparabile o supera di poco il numero di osservazioni, e il modello interpola i dati di training.\nNel regime successivo, detto overparameterized, l’errore di generalizzazione può sorprendentemente diminuire. Questo comportamento è stato osservato soprattutto in presenza di regolarizzazione implicita, come quella indotta dagli algoritmi di ottimizzazione (ad esempio gradient descent), dall’early stopping o dalla struttura stessa dei modelli. In questo regime, pur avendo capacità sufficiente per interpolare, il modello tende a convergere verso soluzioni con buone proprietà di generalizzazione.\nIl fenomeno del double descent è particolarmente rilevante nei modelli moderni ad alta dimensionalità, come le reti neurali profonde e i modelli di grandi dimensioni, e mostra che la relazione tra complessità del modello e generalizzazione è più articolata di quanto suggerito dalla visione classica dell’overfitting.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#regressione-polinomiale",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#regressione-polinomiale",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "La regressione polinomiale estende la regressione lineare introducendo termini polinomiali delle variabili esplicative, consentendo di modellare relazioni non lineari pur rimanendo in un quadro lineare nei parametri. Nel caso univariato, il modello di grado \\(d\\) è\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_d x^d.\n\\]\nNonostante la non linearità in \\(x\\), il modello resta lineare rispetto ai coefficienti \\(\\beta_j\\), e può quindi essere stimato con i metodi usuali dei minimi quadrati.\nUna regola importante è la regola gerarchica: se si include un termine di grado \\(d\\), è buona pratica includere anche tutti i termini di grado inferiore. Questo garantisce interpretabilità e stabilità del modello, evitando specificazioni incoerenti come la presenza di \\(x^2\\) senza il termine lineare \\(x\\).\nNel caso multivariato, la regressione polinomiale può includere anche termini di interazione tra variabili, ad esempio \\(x_1 x_2\\), che permettono di modellare effetti combinati in cui l’influenza di una variabile dipende dal livello di un’altra. Anche per le interazioni vale una forma di gerarchia: includere un termine di interazione implica includere i termini principali corrispondenti.\nL’aumento del grado polinomiale o del numero di interazioni incrementa rapidamente la complessità del modello e il numero di parametri. Per questo motivo la regressione polinomiale è particolarmente soggetta a overfitting quando il grado è troppo elevato rispetto alla numerosità campionaria. In pratica, la scelta del grado e dei termini da includere dovrebbe essere guidata da validazione e, quando necessario, da tecniche di regolarizzazione per controllare la variabilità delle stime.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#regressione-pesata",
    "href": "th/11. Selezione di Variabili e Regolarizzazione nella Regressione.html#regressione-pesata",
    "title": "Selezione di Variabili e Regolarizzazione",
    "section": "",
    "text": "Quando l’ipotesi di omoschedasticità non è soddisfatta, cioè quando la varianza degli errori non è costante tra le osservazioni, la stima OLS non è più efficiente. In questi casi si utilizza la regressione pesata (Weighted Least Squares, WLS), che assegna pesi diversi alle osservazioni in funzione della loro variabilità.\nL’idea è minimizzare una somma dei quadrati dei residui pesata:\n\\[\nSSR_W = \\sum_{i=1}^n W_i\\left(Y_i - \\sum_{j=1}^p B_j X_{ij}\\right)^2,\n\\qquad\nW_i = \\frac{1}{\\sigma_i^2},\n\\]\ndove \\(\\sigma_i^2\\) è la varianza dell’errore associato all’osservazione \\(i\\)-esima. In questo modo, le osservazioni con varianza maggiore ricevono un peso minore e influenzano meno la stima dei coefficienti.\nLa WLS è equivalente a una trasformazione dei dati. Definendo\n\\[\n\\tilde{X}_{ij} = \\frac{X_{ij}}{\\sigma_i},\n\\qquad\n\\tilde{Y}_i = \\frac{Y_i}{\\sigma_i},\n\\]\nil problema di minimizzazione pesata si riconduce a una regressione OLS applicata ai dati trasformati. Questa equivalenza chiarisce che la regressione pesata ristabilisce, sui dati trasformati, l’ipotesi di varianza costante degli errori.\nIn pratica, le varianze \\(\\sigma_i^2\\) non sono quasi mai note. In tali situazioni possono essere stimate, ad esempio specificando un modello per la varianza in funzione delle covariate o dei valori predetti, e utilizzate per definire i pesi. Il procedimento può essere iterato: si stima il modello, si aggiornano le varianze e i pesi, e si ricalcola la regressione fino a convergenza.",
    "crumbs": [
      "Theory",
      "Selezione di Variabili e Regolarizzazione"
    ]
  },
  {
    "objectID": "th/14_Analisi_della_varianza.html",
    "href": "th/14_Analisi_della_varianza.html",
    "title": "Analisi della varianza",
    "section": "",
    "text": "L’analisi della varianza, o ANOVA, è una tecnica statistica utilizzata per studiare la dipendenza di una variabile numerica \\(Y\\) da una o più variabili esplicative di tipo categorico. L’idea di fondo è confrontare le medie di \\(Y\\) tra diversi gruppi e stabilire se le differenze osservate possano essere attribuite al caso oppure siano statisticamente significative.\nA differenza della regressione lineare classica, in cui le covariate sono numeriche, nell’ANOVA i regressori assumono un numero finito di modalità, come gruppi, livelli o trattamenti. Il modello statistico sottostante assume che la variabile risposta sia affetta da rumore additivo gaussiano omoschedastico, cioè errori indipendenti, normalmente distribuiti e con varianza costante.\nA seconda del numero di fattori considerati, si distinguono diverse configurazioni. Nell’ANOVA a una via è presente un solo fattore categorico; nell’ANOVA a due vie entrano in gioco due fattori; infine, nell’ANOVA a due vie con repliche si hanno più osservazioni per ciascuna combinazione dei livelli dei fattori.\n\n\nNel caso più semplice si considera un unico fattore categoriale con \\(m\\) livelli. Ogni osservazione \\(Y_{ij}\\) appartiene al gruppo \\(i\\) ed è la \\(j\\)-esima replica all’interno di quel gruppo. Il modello è\n\\[\nY_{ij} = \\mu_i + \\varepsilon_{ij}, \\qquad \\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2),\n\\]\ndove \\(\\mu_i\\) rappresenta la media del gruppo \\(i\\) e \\(\\sigma^2\\) è la varianza comune a tutti i gruppi. L’indice \\(i = 1,\\dots,m\\) identifica i gruppi, mentre \\(j = 1,\\dots,n_i\\) indica le osservazioni disponibili nel gruppo \\(i\\).\nL’ipotesi nulla del test ANOVA a una via afferma che tutte le medie di gruppo coincidano,\n\\[\nH_0:\\ \\mu_1 = \\mu_2 = \\dots = \\mu_m,\n\\]\nmentre l’alternativa sostiene che almeno una media sia diversa. In termini interpretativi, il test verifica se la variabilità osservata tra le medie dei gruppi sia sufficientemente grande rispetto alla variabilità interna ai gruppi. Se così non fosse, le differenze tra gruppi possono essere spiegate come semplici fluttuazioni casuali; in caso contrario, si conclude che la variabile risposta \\(Y\\) dipende in modo significativo dal gruppo di appartenenza.\n\n\n\nNel modello ANOVA a una via, per ciascun gruppo \\(i\\) si stimano i parametri tramite le statistiche campionarie. In particolare, la media campionaria \\(\\bar Y_i\\) fornisce una stima naturale della media di gruppo \\(\\mu_i\\), mentre la varianza campionaria \\(S_i^2\\) stima la varianza comune \\(\\sigma^2\\).\nSotto le ipotesi del modello, la media campionaria di ciascun gruppo ha distribuzione normale,\n\\[\n\\bar Y_i \\sim \\mathcal{N}\\!\\left( \\mu_i, \\frac{\\sigma^2}{n_i} \\right),\n\\]\npoiché è la media di \\(n_i\\) osservazioni indipendenti gaussiane con varianza \\(\\sigma^2\\). Le varianze campionarie \\(S_i^2\\) sono invece stimatori non distorti della stessa varianza \\(\\sigma^2\\), ma basati solo sulle osservazioni del singolo gruppo.\nPer sfruttare l’ipotesi di varianza comune, si introduce una stima globale ottenuta aggregando l’informazione di tutti i gruppi. A questo scopo si definisce la devianza within (o devianza interna ai gruppi),\n\\[\n\\mathrm{SS}_W = \\sum_{i=1}^m \\sum_{j=1}^{n_i} \\bigl(Y_{ij} - \\bar Y_i\\bigr)^2,\n\\]\nche misura la variabilità delle osservazioni attorno alle rispettive medie di gruppo. Dividendo questa quantità per i gradi di libertà residui \\(N - m\\), con \\(N = \\sum_{i=1}^m n_i\\), si ottiene la stima pooled della varianza,\n\\[\nS_p^2 = \\frac{\\mathrm{SS}_W}{N - m}.\n\\]\nSotto le ipotesi del modello, questa statistica ha una distribuzione chi-quadrato scalata,\n\\[\n\\frac{(N - m) S_p^2}{\\sigma^2} \\sim \\chi^2_{N - m},\n\\]\nrisultato che è fondamentale per la costruzione del test ANOVA.\nAccanto alla devianza interna, si considera la devianza totale,\n\\[\n\\mathrm{SS}_Y = \\sum_{i=1}^m \\sum_{j=1}^{n_i} \\bigl(Y_{ij} - \\bar Y\\bigr)^2,\n\\]\ndove \\(\\bar Y\\) è la media complessiva di tutte le osservazioni. Questa quantità misura la variabilità totale di \\(Y\\) senza tenere conto della struttura a gruppi.\nLa parte di variabilità attribuibile alle differenze tra i gruppi è descritta dalla devianza between,\n\\[\n\\mathrm{SS}_B = \\sum_{i=1}^m n_i \\bigl(\\bar Y_i - \\bar Y\\bigr)^2,\n\\]\nche confronta le medie di gruppo con la media globale, pesandole per le rispettive numerosità.\nQueste tre devianze sono legate dalla classica identità delle devianze,\n\\[\n\\mathrm{SS}_Y = \\mathrm{SS}_B + \\mathrm{SS}_W,\n\\]\nche formalizza l’idea centrale dell’ANOVA: la variabilità totale della risposta può essere scomposta in una componente dovuta alle differenze tra gruppi e una componente dovuta alla variabilità interna ai gruppi.\n\n\n\nPer stabilire se le medie di gruppo siano tutte uguali si formalizza il problema come un test di ipotesi. L’ipotesi nulla afferma che non vi siano differenze sistematiche tra i gruppi,\n\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_m,\n\\]\nmentre l’ipotesi alternativa sostiene che almeno una delle medie differisca dalle altre.\nL’idea centrale del test ANOVA è confrontare due stime della varianza \\(\\sigma^2\\): una basata sulla variabilità tra i gruppi e una basata sulla variabilità interna ai gruppi. La prima è ottenuta dividendo la devianza between per i suoi gradi di libertà,\n\\[\nS_B^2 = \\frac{\\mathrm{SS}_B}{m - 1},\n\\]\nmentre la seconda coincide con la stima pooled già introdotta,\n\\[\nS_p^2 = \\frac{\\mathrm{SS}_W}{N - m}.\n\\]\nLa statistica test è quindi il loro rapporto,\n\\[\nF = \\frac{S_B^2}{S_p^2}\n  = \\frac{\\mathrm{SS}_B / (m - 1)}{\\mathrm{SS}_W / (N - m)}.\n\\]\nSotto l’ipotesi nulla, entrambe le quantità al numeratore e al denominatore stimano la stessa varianza \\(\\sigma^2\\), e il loro rapporto segue una distribuzione di Fisher–Snedecor con \\(m-1\\) e \\(N-m\\) gradi di libertà,\n\\[\nF \\sim F(m - 1,\\ N - m).\n\\]\nIl criterio decisionale consiste nel confrontare il valore osservato della statistica con il quantile superiore della distribuzione \\(F\\). Se il valore di \\(F\\) risulta sufficientemente grande, ossia maggiore del quantile critico \\(F_{\\alpha}(m-1, N-m)\\) per un livello di significatività \\(\\alpha\\), si rifiuta l’ipotesi nulla. In tal caso si conclude che la variabilità tra le medie di gruppo è troppo elevata per essere spiegata dal solo rumore interno, e quindi che almeno un gruppo presenta una media diversa dalle altre.\n\n\n\nDal punto di vista pratico, i dati per un’analisi ANOVA possono essere organizzati secondo due strutture principali. Nel formato wide ogni gruppo è rappresentato da una colonna distinta, ciascuna contenente le osservazioni relative a quel livello del fattore. Nel formato long (o stacked), invece, tutte le osservazioni sono raccolte in un’unica colonna numerica, affiancata da una colonna categoriale che indica il gruppo di appartenenza. Quest’ultimo formato è in genere preferito dai software statistici e risulta più flessibile quando si passa a modelli con più fattori.\nUna volta stimato il modello e svolto il test ANOVA, è fondamentale verificare a posteriori la validità delle ipotesi su cui il modello si basa. Questo controllo viene effettuato analizzando i residui, ossia le differenze tra le osservazioni e le corrispondenti medie di gruppo. In particolare, ci si aspetta che i residui siano approssimativamente normali, che mostrino una variabilità simile nei diversi gruppi (omoschedasticità) e che non presentino valori anomali particolarmente influenti.\nQualora queste condizioni risultino violate, una strategia comune consiste nell’applicare una trasformazione alla variabile risposta \\(Y\\), come il logaritmo o la radice quadrata. Tali trasformazioni possono ridurre l’asimmetria, stabilizzare la varianza e rendere più plausibili le assunzioni del modello.\nInfine, se il test ANOVA non risulta significativo, non vi è evidenza statistica di differenze tra le medie di gruppo. In questo caso è ragionevole trattare l’intero insieme di dati come un unico campione proveniente da una stessa distribuzione gaussiana, ignorando il fattore categoriale senza perdita informativa rilevante.",
    "crumbs": [
      "Theory",
      "Analisi della varianza"
    ]
  },
  {
    "objectID": "th/14_Analisi_della_varianza.html#anova-a-una-via",
    "href": "th/14_Analisi_della_varianza.html#anova-a-una-via",
    "title": "Analisi della varianza",
    "section": "",
    "text": "Nel caso più semplice si considera un unico fattore categoriale con \\(m\\) livelli. Ogni osservazione \\(Y_{ij}\\) appartiene al gruppo \\(i\\) ed è la \\(j\\)-esima replica all’interno di quel gruppo. Il modello è\n\\[\nY_{ij} = \\mu_i + \\varepsilon_{ij}, \\qquad \\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2),\n\\]\ndove \\(\\mu_i\\) rappresenta la media del gruppo \\(i\\) e \\(\\sigma^2\\) è la varianza comune a tutti i gruppi. L’indice \\(i = 1,\\dots,m\\) identifica i gruppi, mentre \\(j = 1,\\dots,n_i\\) indica le osservazioni disponibili nel gruppo \\(i\\).\nL’ipotesi nulla del test ANOVA a una via afferma che tutte le medie di gruppo coincidano,\n\\[\nH_0:\\ \\mu_1 = \\mu_2 = \\dots = \\mu_m,\n\\]\nmentre l’alternativa sostiene che almeno una media sia diversa. In termini interpretativi, il test verifica se la variabilità osservata tra le medie dei gruppi sia sufficientemente grande rispetto alla variabilità interna ai gruppi. Se così non fosse, le differenze tra gruppi possono essere spiegate come semplici fluttuazioni casuali; in caso contrario, si conclude che la variabile risposta \\(Y\\) dipende in modo significativo dal gruppo di appartenenza.",
    "crumbs": [
      "Theory",
      "Analisi della varianza"
    ]
  },
  {
    "objectID": "th/14_Analisi_della_varianza.html#stima-dei-parametri",
    "href": "th/14_Analisi_della_varianza.html#stima-dei-parametri",
    "title": "Analisi della varianza",
    "section": "",
    "text": "Nel modello ANOVA a una via, per ciascun gruppo \\(i\\) si stimano i parametri tramite le statistiche campionarie. In particolare, la media campionaria \\(\\bar Y_i\\) fornisce una stima naturale della media di gruppo \\(\\mu_i\\), mentre la varianza campionaria \\(S_i^2\\) stima la varianza comune \\(\\sigma^2\\).\nSotto le ipotesi del modello, la media campionaria di ciascun gruppo ha distribuzione normale,\n\\[\n\\bar Y_i \\sim \\mathcal{N}\\!\\left( \\mu_i, \\frac{\\sigma^2}{n_i} \\right),\n\\]\npoiché è la media di \\(n_i\\) osservazioni indipendenti gaussiane con varianza \\(\\sigma^2\\). Le varianze campionarie \\(S_i^2\\) sono invece stimatori non distorti della stessa varianza \\(\\sigma^2\\), ma basati solo sulle osservazioni del singolo gruppo.\nPer sfruttare l’ipotesi di varianza comune, si introduce una stima globale ottenuta aggregando l’informazione di tutti i gruppi. A questo scopo si definisce la devianza within (o devianza interna ai gruppi),\n\\[\n\\mathrm{SS}_W = \\sum_{i=1}^m \\sum_{j=1}^{n_i} \\bigl(Y_{ij} - \\bar Y_i\\bigr)^2,\n\\]\nche misura la variabilità delle osservazioni attorno alle rispettive medie di gruppo. Dividendo questa quantità per i gradi di libertà residui \\(N - m\\), con \\(N = \\sum_{i=1}^m n_i\\), si ottiene la stima pooled della varianza,\n\\[\nS_p^2 = \\frac{\\mathrm{SS}_W}{N - m}.\n\\]\nSotto le ipotesi del modello, questa statistica ha una distribuzione chi-quadrato scalata,\n\\[\n\\frac{(N - m) S_p^2}{\\sigma^2} \\sim \\chi^2_{N - m},\n\\]\nrisultato che è fondamentale per la costruzione del test ANOVA.\nAccanto alla devianza interna, si considera la devianza totale,\n\\[\n\\mathrm{SS}_Y = \\sum_{i=1}^m \\sum_{j=1}^{n_i} \\bigl(Y_{ij} - \\bar Y\\bigr)^2,\n\\]\ndove \\(\\bar Y\\) è la media complessiva di tutte le osservazioni. Questa quantità misura la variabilità totale di \\(Y\\) senza tenere conto della struttura a gruppi.\nLa parte di variabilità attribuibile alle differenze tra i gruppi è descritta dalla devianza between,\n\\[\n\\mathrm{SS}_B = \\sum_{i=1}^m n_i \\bigl(\\bar Y_i - \\bar Y\\bigr)^2,\n\\]\nche confronta le medie di gruppo con la media globale, pesandole per le rispettive numerosità.\nQueste tre devianze sono legate dalla classica identità delle devianze,\n\\[\n\\mathrm{SS}_Y = \\mathrm{SS}_B + \\mathrm{SS}_W,\n\\]\nche formalizza l’idea centrale dell’ANOVA: la variabilità totale della risposta può essere scomposta in una componente dovuta alle differenze tra gruppi e una componente dovuta alla variabilità interna ai gruppi.",
    "crumbs": [
      "Theory",
      "Analisi della varianza"
    ]
  },
  {
    "objectID": "th/14_Analisi_della_varianza.html#il-test-anova",
    "href": "th/14_Analisi_della_varianza.html#il-test-anova",
    "title": "Analisi della varianza",
    "section": "",
    "text": "Per stabilire se le medie di gruppo siano tutte uguali si formalizza il problema come un test di ipotesi. L’ipotesi nulla afferma che non vi siano differenze sistematiche tra i gruppi,\n\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_m,\n\\]\nmentre l’ipotesi alternativa sostiene che almeno una delle medie differisca dalle altre.\nL’idea centrale del test ANOVA è confrontare due stime della varianza \\(\\sigma^2\\): una basata sulla variabilità tra i gruppi e una basata sulla variabilità interna ai gruppi. La prima è ottenuta dividendo la devianza between per i suoi gradi di libertà,\n\\[\nS_B^2 = \\frac{\\mathrm{SS}_B}{m - 1},\n\\]\nmentre la seconda coincide con la stima pooled già introdotta,\n\\[\nS_p^2 = \\frac{\\mathrm{SS}_W}{N - m}.\n\\]\nLa statistica test è quindi il loro rapporto,\n\\[\nF = \\frac{S_B^2}{S_p^2}\n  = \\frac{\\mathrm{SS}_B / (m - 1)}{\\mathrm{SS}_W / (N - m)}.\n\\]\nSotto l’ipotesi nulla, entrambe le quantità al numeratore e al denominatore stimano la stessa varianza \\(\\sigma^2\\), e il loro rapporto segue una distribuzione di Fisher–Snedecor con \\(m-1\\) e \\(N-m\\) gradi di libertà,\n\\[\nF \\sim F(m - 1,\\ N - m).\n\\]\nIl criterio decisionale consiste nel confrontare il valore osservato della statistica con il quantile superiore della distribuzione \\(F\\). Se il valore di \\(F\\) risulta sufficientemente grande, ossia maggiore del quantile critico \\(F_{\\alpha}(m-1, N-m)\\) per un livello di significatività \\(\\alpha\\), si rifiuta l’ipotesi nulla. In tal caso si conclude che la variabilità tra le medie di gruppo è troppo elevata per essere spiegata dal solo rumore interno, e quindi che almeno un gruppo presenta una media diversa dalle altre.",
    "crumbs": [
      "Theory",
      "Analisi della varianza"
    ]
  },
  {
    "objectID": "th/14_Analisi_della_varianza.html#aspetti-operativi",
    "href": "th/14_Analisi_della_varianza.html#aspetti-operativi",
    "title": "Analisi della varianza",
    "section": "",
    "text": "Dal punto di vista pratico, i dati per un’analisi ANOVA possono essere organizzati secondo due strutture principali. Nel formato wide ogni gruppo è rappresentato da una colonna distinta, ciascuna contenente le osservazioni relative a quel livello del fattore. Nel formato long (o stacked), invece, tutte le osservazioni sono raccolte in un’unica colonna numerica, affiancata da una colonna categoriale che indica il gruppo di appartenenza. Quest’ultimo formato è in genere preferito dai software statistici e risulta più flessibile quando si passa a modelli con più fattori.\nUna volta stimato il modello e svolto il test ANOVA, è fondamentale verificare a posteriori la validità delle ipotesi su cui il modello si basa. Questo controllo viene effettuato analizzando i residui, ossia le differenze tra le osservazioni e le corrispondenti medie di gruppo. In particolare, ci si aspetta che i residui siano approssimativamente normali, che mostrino una variabilità simile nei diversi gruppi (omoschedasticità) e che non presentino valori anomali particolarmente influenti.\nQualora queste condizioni risultino violate, una strategia comune consiste nell’applicare una trasformazione alla variabile risposta \\(Y\\), come il logaritmo o la radice quadrata. Tali trasformazioni possono ridurre l’asimmetria, stabilizzare la varianza e rendere più plausibili le assunzioni del modello.\nInfine, se il test ANOVA non risulta significativo, non vi è evidenza statistica di differenze tra le medie di gruppo. In questo caso è ragionevole trattare l’intero insieme di dati come un unico campione proveniente da una stessa distribuzione gaussiana, ignorando il fattore categoriale senza perdita informativa rilevante.",
    "crumbs": [
      "Theory",
      "Analisi della varianza"
    ]
  },
  {
    "objectID": "th/09_Funzioni_di_Perdita_e_Inferenza.html",
    "href": "th/09_Funzioni_di_Perdita_e_Inferenza.html",
    "title": "Funzioni di Perdita e Inferenza",
    "section": "",
    "text": "La Cross-Entropy Loss (entropia incrociata) è una funzione di perdita utilizzata quando la variabile target è categorica, in particolare nei problemi di classificazione multiclasse. Il suo impiego è strettamente connesso a modelli probabilistici che, per ogni osservazione, stimano una distribuzione di probabilità sulle classi possibili.\nSi consideri un dataset formato da \\(n\\) osservazioni e \\(m\\) categorie. Ogni osservazione appartiene a una sola categoria ed è rappresentata tramite un vettore one-hot \\(b(i) \\in \\{0,1\\}^m\\), tale che \\(b_j(i)=1\\) se l’osservazione \\(i\\) appartiene alla classe \\(j\\) e \\(b_j(i)=0\\) altrimenti.\nIndichiamo con \\(p_j(i)\\) la probabilità che l’osservazione \\(i\\) appartenga alla classe \\(j\\). Per ogni \\(i\\), queste quantità definiscono una distribuzione discreta, quindi soddisfano i vincoli \\[\np_j(i) \\ge 0,\n\\qquad\n\\sum_{j=1}^m p_j(i) = 1.\n\\]\nGrazie alla codifica one-hot, la probabilità dell’osservazione \\(i\\) può essere scritta in forma compatta come \\[\nP(Y(i)=j) = p_j(i) = \\prod_{k=1}^m p_k(i)^{b_k(i)}.\n\\] Il prodotto seleziona automaticamente il termine corrispondente alla classe osservata, poiché tutti gli altri esponenti sono nulli.\nLa log-verosimiglianza dell’intero campione si ottiene sommando i contributi delle singole osservazioni e assume la forma \\[\n\\ell = \\sum_{i=1}^n \\sum_{j=1}^m b_j(i)\\,\\log p_j(i).\n\\] Massimizzare questa quantità equivale a rendere massima la probabilità assegnata dal modello alle classi effettivamente osservate.\n\n\nSe le probabilità non dipendono dall’input, cioè \\(p_j(i)=P_j\\) è costante per tutte le osservazioni, il problema si riduce alla stima dei parametri di una distribuzione categorica. In questo caso la massimizzazione della log-verosimiglianza porta alla stima \\[\n\\hat P_j = \\frac{O_j}{n},\n\\] dove \\(O_j\\) indica il numero di osservazioni appartenenti alla classe \\(j\\). La stima coincide quindi con la frequenza empirica, risultato coerente con l’interpretazione probabilistica del modello.\n\n\n\nQuando la probabilità della classe dipende dall’input \\(x(i)\\), si introduce un modello parametrico del tipo \\[\np_j(i) = \\pi_j(x(i); \\alpha,\\beta,\\gamma),\n\\] che associa a ciascuna osservazione una distribuzione di probabilità sulle classi. Un esempio fondamentale è fornito dalla softmax, definita come \\[\n\\pi_j(x) = \\frac{\\exp(\\eta_j(x))}{\\sum_{k=1}^m \\exp(\\eta_k(x))},\n\\qquad\n\\eta_j(x) = w_j^\\top x + c_j.\n\\] La softmax garantisce automaticamente che le probabilità siano non negative e sommino a uno. In questo contesto, la Cross-Entropy Loss coincide con il negativo della log-verosimiglianza e la stima dei parametri avviene numericamente, tipicamente tramite algoritmi di ottimizzazione iterativa come la discesa del gradiente.\n\n\n\nPoiché l’addestramento dei modelli di classificazione avviene tramite minimizzazione di una funzione obiettivo, la log-verosimiglianza viene riscritta cambiando segno e normalizzando rispetto al numero di osservazioni. Si ottiene così la cross-entropy loss, definita come \\[\n\\text{loss}_{\\text{CE}}(\\alpha,\\beta,\\gamma)\n= -\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^m b_j(i)\\,\\log \\pi_j(x(i);\\alpha,\\beta,\\gamma).\n\\] Questa quantità rappresenta il negativo della log-verosimiglianza media e misura quanto la distribuzione di probabilità predetta dal modello si discosta dalla distribuzione vera, che nel caso di classificazione supervisionata è codificata tramite vettori one-hot.\nLa stessa espressione può essere interpretata in termini di entropia incrociata tra la distribuzione target \\(b(i)\\) e la distribuzione predetta \\(\\pi(x(i))\\). Per una singola osservazione vale \\[\nH(b,\\pi) = -\\sum_{j=1}^m b_j \\log \\pi_j,\n\\] e la funzione di perdita complessiva non è altro che la media di questa quantità sul campione: \\[\n\\text{loss}_{\\text{CE}} = \\frac{1}{n}\\sum_{i=1}^n H\\bigl(b(i),\\pi(x(i))\\bigr).\n\\]\nDal punto di vista interpretativo, minimizzare la cross-entropy equivale a rendere massima la probabilità assegnata dal modello alla classe corretta. L’uso del logaritmo fa sì che predizioni errate ma molto “sicure”, cioè con probabilità alta assegnata alla classe sbagliata, vengano penalizzate in modo particolarmente severo.\nNella pratica computazionale è necessario prestare attenzione alla stabilità numerica. In genere si evita il problema di \\(\\log 0\\) introducendo un piccolo termine \\(\\varepsilon\\) nelle probabilità oppure lavorando direttamente con formulazioni numericamente stabili della softmax. Inoltre, in presenza di dataset sbilanciati o di rumore nelle etichette, si possono introdurre tecniche come il label smoothing o pesi di classe, che modificano la loss per rendere l’addestramento più robusto.\n\n\n\n\nLa Mean Squared Error (MSE) è una funzione di perdita tipicamente utilizzata quando la variabile risposta è quantitativa e si assume un modello gaussiano per gli errori. In questo contesto si ipotizza che ciascuna osservazione segua una distribuzione normale \\[\nY(i) \\sim \\mathcal{N}(\\mu(i), \\sigma^2(i)),\n\\] dove \\(\\mu(i)\\) rappresenta la media condizionata, eventualmente dipendente dagli input, mentre \\(\\sigma^2(i)\\) indica la varianza.\nSotto questa ipotesi, la log-verosimiglianza del campione può essere scritta come \\[\n\\ell\n= \\sum_{i=1}^n \\left[\n-\\log \\sigma(i)\n- \\log \\sqrt{2\\pi}\n- \\frac{(Y(i)-\\mu(i))^2}{2\\sigma^2(i)}\n\\right].\n\\] Questa espressione mette in evidenza come lo scarto quadratico tra osservazioni e media giochi un ruolo centrale nel modello gaussiano.\n\n\nNel caso più semplice si assume che \\(\\mu(i)=\\mu\\) e \\(\\sigma(i)=\\sigma\\) per ogni osservazione. La massimizzazione della log-verosimiglianza conduce alle stime classiche: la media campionaria come stimatore di \\(\\mu\\) e la varianza campionaria con denominatore \\(n\\) come stimatore di \\(\\sigma^2\\) nel senso della massima verosimiglianza. In questo scenario la MSE coincide con la varianza empirica attorno alla media stimata.\n\n\n\nUn caso fondamentale in regressione è quello in cui la media dipende dagli input \\(x(i)\\), mentre la varianza è costante, cioè \\(\\sigma^2(i)=\\sigma^2\\). Indicando con \\(\\mu(x(i);\\alpha,\\beta,\\gamma)\\) un modello parametrico per la media, la parte della log-verosimiglianza che dipende da tali parametri è, a meno di costanti additive e moltiplicative, proporzionale alla somma dei quadrati dei residui. Ne segue che massimizzare la log-verosimiglianza equivale a minimizzare la MSE: \\[\n\\text{loss}_{\\text{MSE}}(\\alpha,\\beta,\\gamma)\n= \\frac{1}{n}\\sum_{i=1}^n \\bigl(Y(i)-\\mu(x(i);\\alpha,\\beta,\\gamma)\\bigr)^2.\n\\]\nUna volta stimata la media, la stima di massima verosimiglianza della deviazione standard è data da \\[\n\\hat{\\sigma}\n= \\sqrt{\\text{loss}_{\\text{MSE}}(\\alpha,\\beta,\\gamma)},\n\\] mentre la corrispondente stima della varianza risulta \\[\n\\hat{\\sigma}^2_{\\text{MLE}}\n= \\frac{1}{n}\\sum_{i=1}^n \\bigl(Y(i)-\\hat{\\mu}(i)\\bigr)^2.\n\\] Dal punto di vista statistico questo stimatore è distorto; lo stimatore non distorto della varianza utilizza invece il denominatore \\(n-p\\), dove \\(p\\) è il numero di parametri stimati nel modello per la media.\n\n\n\nNel caso più generale anche la varianza dipende dagli input, cioè \\(\\sigma^2(i)=\\sigma^2(x(i))\\). In questa situazione la funzione obiettivo coerente con il modello gaussiano è il negativo della log-verosimiglianza: \\[\n-\\ell(\\theta)\n= \\frac{1}{2}\\sum_{i=1}^n \\left[\n\\log\\bigl(2\\pi \\sigma^2(i)\\bigr)\n+ \\frac{(Y(i)-\\mu(i))^2}{\\sigma^2(i)}\n\\right].\n\\] Questo criterio corrisponde a una forma di weighted least squares, in cui ciascun residuo è pesato con l’inverso della sua varianza. La stima congiunta dei parametri della media e della varianza non ammette in generale una soluzione in forma chiusa e richiede l’uso di metodi di ottimizzazione iterativi.\n\n\n\n\nNel modello di regressione lineare semplice si assume che le variabili indipendenti \\(x_i\\) siano deterministiche, mentre le variabili dipendenti \\(Y_i\\) siano casuali. L’ipotesi di base è che ciascuna osservazione segua un modello gaussiano con media lineare e varianza costante: \\[\nY_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2),\n\\qquad\n\\mu_i = \\beta_0 + \\beta_1 x_i.\n\\] In questa formulazione \\(\\beta_0\\) rappresenta l’intercetta del modello, mentre \\(\\beta_1\\) quantifica l’effetto medio di una variazione unitaria di \\(x\\) sulla risposta \\(Y\\).\nAssumendo indipendenza delle osservazioni e omoschedasticità, la massimizzazione della log-verosimiglianza gaussiana è equivalente alla minimizzazione della Mean Squared Error, definita come \\[\n\\text{loss}_{\\text{MSE}}(\\beta_0, \\beta_1)\n= \\frac{1}{n} \\sum_{i=1}^n \\bigl(Y_i - \\beta_0 - \\beta_1 x_i\\bigr)^2.\n\\] La stima dei parametri si riduce quindi a un problema di minimi quadrati.\nLa soluzione analitica di questo problema fornisce le stime di massima verosimiglianza dei coefficienti, che coincidono con le stime dei minimi quadrati ordinari: \\[\n\\hat{\\beta}_1\n= \\frac{\\sum_{i=1}^n x_i Y_i - n \\bar{x}\\,\\bar{Y}}\n{\\sum_{i=1}^n x_i^2 - n \\bar{x}^2},\n\\qquad\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x},\n\\] dove \\(\\bar{x}\\) e \\(\\bar{Y}\\) indicano le medie campionarie delle variabili indipendente e dipendente. La stima di \\(\\beta_1\\) può essere interpretata come il rapporto tra la covarianza campionaria tra \\(x\\) e \\(Y\\) e la varianza campionaria di \\(x\\).\nUna volta stimati i coefficienti, si procede alla stima della varianza degli errori. Lo stimatore di massima verosimiglianza utilizza il denominatore \\(n\\) ed è quindi distorto. Lo stimatore non distorto della varianza degli errori è invece \\[\nS_e^2\n= \\frac{1}{n - 2} \\sum_{i=1}^n \\bigl(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\bigr)^2,\n\\] dove il termine \\(n-2\\) rappresenta i gradi di libertà residui dopo la stima dei due parametri \\(\\beta_0\\) e \\(\\beta_1\\).\n\n\n\nIl teorema di Cochran fornisce la base teorica per comprendere come la variabilità totale dei dati possa essere scomposta in modo rigoroso nei modelli lineari sotto ipotesi gaussiane. L’idea centrale è che, quando le osservazioni seguono una distribuzione normale con varianza costante e i parametri vengono stimati all’interno di un modello lineare, la parte dei dati spiegata dal modello e la parte residua possono essere trattate come componenti indipendenti, ciascuna con una struttura probabilistica ben definita. Questo risultato giustifica formalmente la scomposizione della varianza utilizzata in regressione lineare, ANOVA e test F.\nSi consideri un vettore aleatorio \\(X=(X_1,\\dots,X_n)\\in\\mathbb{R}^n\\) con componenti indipendenti tali che \\[\nX_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2),\n\\] e si assuma che il vettore delle medie \\(\\mu=(\\mu_1,\\dots,\\mu_n)\\) appartenga a un sottospazio vettoriale \\(V\\subset\\mathbb{R}^n\\) di dimensione \\(k\\). In questo contesto, la stima di massima verosimiglianza di \\(\\mu\\) si ottiene come proiezione ortogonale del vettore osservato \\(X\\) sul sottospazio \\(V\\). Indicando con \\(\\pi_V(X)\\) tale proiezione, vale \\[\n\\hat{\\mu}=\\pi_V(X).\n\\] Dal punto di vista geometrico, questa stima è quella che minimizza la distanza euclidea tra \\(X\\) e l’insieme dei vettori ammissibili per \\(\\mu\\), coerentemente con il criterio dei minimi quadrati.\nLa componente di \\(X\\) non spiegata dal modello è il residuo \\(X-\\pi_V(X)\\). La sua norma al quadrato \\[\nW=\\|X-\\pi_V(X)\\|^2\n\\] rappresenta la somma dei quadrati residui. Il teorema di Cochran afferma che la quantità \\(W\\) è indipendente dalla componente stimata \\(\\pi_V(X)\\) e che, normalizzata per la varianza comune \\(\\sigma^2\\), segue una distribuzione chi-quadro con \\(n-k\\) gradi di libertà: \\[\n\\frac{W}{\\sigma^2}\\sim\\chi^2(n-k).\n\\]\nL’indipendenza tra la parte spiegata dal modello e la parte residua, insieme alla distribuzione chi-quadro della somma dei quadrati residui, costituisce il fondamento teorico della scomposizione della varianza. È proprio questo risultato che rende possibile confrontare in modo corretto la variabilità spiegata dal modello con quella attribuibile al rumore, utilizzando distribuzioni note e strumenti di inferenza statistica affidabili nei modelli di regressione lineare.",
    "crumbs": [
      "Theory",
      "Funzioni di Perdita e Inferenza"
    ]
  },
  {
    "objectID": "th/09_Funzioni_di_Perdita_e_Inferenza.html#cross-entropy-loss",
    "href": "th/09_Funzioni_di_Perdita_e_Inferenza.html#cross-entropy-loss",
    "title": "Funzioni di Perdita e Inferenza",
    "section": "",
    "text": "La Cross-Entropy Loss (entropia incrociata) è una funzione di perdita utilizzata quando la variabile target è categorica, in particolare nei problemi di classificazione multiclasse. Il suo impiego è strettamente connesso a modelli probabilistici che, per ogni osservazione, stimano una distribuzione di probabilità sulle classi possibili.\nSi consideri un dataset formato da \\(n\\) osservazioni e \\(m\\) categorie. Ogni osservazione appartiene a una sola categoria ed è rappresentata tramite un vettore one-hot \\(b(i) \\in \\{0,1\\}^m\\), tale che \\(b_j(i)=1\\) se l’osservazione \\(i\\) appartiene alla classe \\(j\\) e \\(b_j(i)=0\\) altrimenti.\nIndichiamo con \\(p_j(i)\\) la probabilità che l’osservazione \\(i\\) appartenga alla classe \\(j\\). Per ogni \\(i\\), queste quantità definiscono una distribuzione discreta, quindi soddisfano i vincoli \\[\np_j(i) \\ge 0,\n\\qquad\n\\sum_{j=1}^m p_j(i) = 1.\n\\]\nGrazie alla codifica one-hot, la probabilità dell’osservazione \\(i\\) può essere scritta in forma compatta come \\[\nP(Y(i)=j) = p_j(i) = \\prod_{k=1}^m p_k(i)^{b_k(i)}.\n\\] Il prodotto seleziona automaticamente il termine corrispondente alla classe osservata, poiché tutti gli altri esponenti sono nulli.\nLa log-verosimiglianza dell’intero campione si ottiene sommando i contributi delle singole osservazioni e assume la forma \\[\n\\ell = \\sum_{i=1}^n \\sum_{j=1}^m b_j(i)\\,\\log p_j(i).\n\\] Massimizzare questa quantità equivale a rendere massima la probabilità assegnata dal modello alle classi effettivamente osservate.\n\n\nSe le probabilità non dipendono dall’input, cioè \\(p_j(i)=P_j\\) è costante per tutte le osservazioni, il problema si riduce alla stima dei parametri di una distribuzione categorica. In questo caso la massimizzazione della log-verosimiglianza porta alla stima \\[\n\\hat P_j = \\frac{O_j}{n},\n\\] dove \\(O_j\\) indica il numero di osservazioni appartenenti alla classe \\(j\\). La stima coincide quindi con la frequenza empirica, risultato coerente con l’interpretazione probabilistica del modello.\n\n\n\nQuando la probabilità della classe dipende dall’input \\(x(i)\\), si introduce un modello parametrico del tipo \\[\np_j(i) = \\pi_j(x(i); \\alpha,\\beta,\\gamma),\n\\] che associa a ciascuna osservazione una distribuzione di probabilità sulle classi. Un esempio fondamentale è fornito dalla softmax, definita come \\[\n\\pi_j(x) = \\frac{\\exp(\\eta_j(x))}{\\sum_{k=1}^m \\exp(\\eta_k(x))},\n\\qquad\n\\eta_j(x) = w_j^\\top x + c_j.\n\\] La softmax garantisce automaticamente che le probabilità siano non negative e sommino a uno. In questo contesto, la Cross-Entropy Loss coincide con il negativo della log-verosimiglianza e la stima dei parametri avviene numericamente, tipicamente tramite algoritmi di ottimizzazione iterativa come la discesa del gradiente.\n\n\n\nPoiché l’addestramento dei modelli di classificazione avviene tramite minimizzazione di una funzione obiettivo, la log-verosimiglianza viene riscritta cambiando segno e normalizzando rispetto al numero di osservazioni. Si ottiene così la cross-entropy loss, definita come \\[\n\\text{loss}_{\\text{CE}}(\\alpha,\\beta,\\gamma)\n= -\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^m b_j(i)\\,\\log \\pi_j(x(i);\\alpha,\\beta,\\gamma).\n\\] Questa quantità rappresenta il negativo della log-verosimiglianza media e misura quanto la distribuzione di probabilità predetta dal modello si discosta dalla distribuzione vera, che nel caso di classificazione supervisionata è codificata tramite vettori one-hot.\nLa stessa espressione può essere interpretata in termini di entropia incrociata tra la distribuzione target \\(b(i)\\) e la distribuzione predetta \\(\\pi(x(i))\\). Per una singola osservazione vale \\[\nH(b,\\pi) = -\\sum_{j=1}^m b_j \\log \\pi_j,\n\\] e la funzione di perdita complessiva non è altro che la media di questa quantità sul campione: \\[\n\\text{loss}_{\\text{CE}} = \\frac{1}{n}\\sum_{i=1}^n H\\bigl(b(i),\\pi(x(i))\\bigr).\n\\]\nDal punto di vista interpretativo, minimizzare la cross-entropy equivale a rendere massima la probabilità assegnata dal modello alla classe corretta. L’uso del logaritmo fa sì che predizioni errate ma molto “sicure”, cioè con probabilità alta assegnata alla classe sbagliata, vengano penalizzate in modo particolarmente severo.\nNella pratica computazionale è necessario prestare attenzione alla stabilità numerica. In genere si evita il problema di \\(\\log 0\\) introducendo un piccolo termine \\(\\varepsilon\\) nelle probabilità oppure lavorando direttamente con formulazioni numericamente stabili della softmax. Inoltre, in presenza di dataset sbilanciati o di rumore nelle etichette, si possono introdurre tecniche come il label smoothing o pesi di classe, che modificano la loss per rendere l’addestramento più robusto.",
    "crumbs": [
      "Theory",
      "Funzioni di Perdita e Inferenza"
    ]
  },
  {
    "objectID": "th/09_Funzioni_di_Perdita_e_Inferenza.html#mean-squared-error-loss-mse",
    "href": "th/09_Funzioni_di_Perdita_e_Inferenza.html#mean-squared-error-loss-mse",
    "title": "Funzioni di Perdita e Inferenza",
    "section": "",
    "text": "La Mean Squared Error (MSE) è una funzione di perdita tipicamente utilizzata quando la variabile risposta è quantitativa e si assume un modello gaussiano per gli errori. In questo contesto si ipotizza che ciascuna osservazione segua una distribuzione normale \\[\nY(i) \\sim \\mathcal{N}(\\mu(i), \\sigma^2(i)),\n\\] dove \\(\\mu(i)\\) rappresenta la media condizionata, eventualmente dipendente dagli input, mentre \\(\\sigma^2(i)\\) indica la varianza.\nSotto questa ipotesi, la log-verosimiglianza del campione può essere scritta come \\[\n\\ell\n= \\sum_{i=1}^n \\left[\n-\\log \\sigma(i)\n- \\log \\sqrt{2\\pi}\n- \\frac{(Y(i)-\\mu(i))^2}{2\\sigma^2(i)}\n\\right].\n\\] Questa espressione mette in evidenza come lo scarto quadratico tra osservazioni e media giochi un ruolo centrale nel modello gaussiano.\n\n\nNel caso più semplice si assume che \\(\\mu(i)=\\mu\\) e \\(\\sigma(i)=\\sigma\\) per ogni osservazione. La massimizzazione della log-verosimiglianza conduce alle stime classiche: la media campionaria come stimatore di \\(\\mu\\) e la varianza campionaria con denominatore \\(n\\) come stimatore di \\(\\sigma^2\\) nel senso della massima verosimiglianza. In questo scenario la MSE coincide con la varianza empirica attorno alla media stimata.\n\n\n\nUn caso fondamentale in regressione è quello in cui la media dipende dagli input \\(x(i)\\), mentre la varianza è costante, cioè \\(\\sigma^2(i)=\\sigma^2\\). Indicando con \\(\\mu(x(i);\\alpha,\\beta,\\gamma)\\) un modello parametrico per la media, la parte della log-verosimiglianza che dipende da tali parametri è, a meno di costanti additive e moltiplicative, proporzionale alla somma dei quadrati dei residui. Ne segue che massimizzare la log-verosimiglianza equivale a minimizzare la MSE: \\[\n\\text{loss}_{\\text{MSE}}(\\alpha,\\beta,\\gamma)\n= \\frac{1}{n}\\sum_{i=1}^n \\bigl(Y(i)-\\mu(x(i);\\alpha,\\beta,\\gamma)\\bigr)^2.\n\\]\nUna volta stimata la media, la stima di massima verosimiglianza della deviazione standard è data da \\[\n\\hat{\\sigma}\n= \\sqrt{\\text{loss}_{\\text{MSE}}(\\alpha,\\beta,\\gamma)},\n\\] mentre la corrispondente stima della varianza risulta \\[\n\\hat{\\sigma}^2_{\\text{MLE}}\n= \\frac{1}{n}\\sum_{i=1}^n \\bigl(Y(i)-\\hat{\\mu}(i)\\bigr)^2.\n\\] Dal punto di vista statistico questo stimatore è distorto; lo stimatore non distorto della varianza utilizza invece il denominatore \\(n-p\\), dove \\(p\\) è il numero di parametri stimati nel modello per la media.\n\n\n\nNel caso più generale anche la varianza dipende dagli input, cioè \\(\\sigma^2(i)=\\sigma^2(x(i))\\). In questa situazione la funzione obiettivo coerente con il modello gaussiano è il negativo della log-verosimiglianza: \\[\n-\\ell(\\theta)\n= \\frac{1}{2}\\sum_{i=1}^n \\left[\n\\log\\bigl(2\\pi \\sigma^2(i)\\bigr)\n+ \\frac{(Y(i)-\\mu(i))^2}{\\sigma^2(i)}\n\\right].\n\\] Questo criterio corrisponde a una forma di weighted least squares, in cui ciascun residuo è pesato con l’inverso della sua varianza. La stima congiunta dei parametri della media e della varianza non ammette in generale una soluzione in forma chiusa e richiede l’uso di metodi di ottimizzazione iterativi.",
    "crumbs": [
      "Theory",
      "Funzioni di Perdita e Inferenza"
    ]
  },
  {
    "objectID": "th/09_Funzioni_di_Perdita_e_Inferenza.html#regressione-lineare-semplice",
    "href": "th/09_Funzioni_di_Perdita_e_Inferenza.html#regressione-lineare-semplice",
    "title": "Funzioni di Perdita e Inferenza",
    "section": "",
    "text": "Nel modello di regressione lineare semplice si assume che le variabili indipendenti \\(x_i\\) siano deterministiche, mentre le variabili dipendenti \\(Y_i\\) siano casuali. L’ipotesi di base è che ciascuna osservazione segua un modello gaussiano con media lineare e varianza costante: \\[\nY_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2),\n\\qquad\n\\mu_i = \\beta_0 + \\beta_1 x_i.\n\\] In questa formulazione \\(\\beta_0\\) rappresenta l’intercetta del modello, mentre \\(\\beta_1\\) quantifica l’effetto medio di una variazione unitaria di \\(x\\) sulla risposta \\(Y\\).\nAssumendo indipendenza delle osservazioni e omoschedasticità, la massimizzazione della log-verosimiglianza gaussiana è equivalente alla minimizzazione della Mean Squared Error, definita come \\[\n\\text{loss}_{\\text{MSE}}(\\beta_0, \\beta_1)\n= \\frac{1}{n} \\sum_{i=1}^n \\bigl(Y_i - \\beta_0 - \\beta_1 x_i\\bigr)^2.\n\\] La stima dei parametri si riduce quindi a un problema di minimi quadrati.\nLa soluzione analitica di questo problema fornisce le stime di massima verosimiglianza dei coefficienti, che coincidono con le stime dei minimi quadrati ordinari: \\[\n\\hat{\\beta}_1\n= \\frac{\\sum_{i=1}^n x_i Y_i - n \\bar{x}\\,\\bar{Y}}\n{\\sum_{i=1}^n x_i^2 - n \\bar{x}^2},\n\\qquad\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x},\n\\] dove \\(\\bar{x}\\) e \\(\\bar{Y}\\) indicano le medie campionarie delle variabili indipendente e dipendente. La stima di \\(\\beta_1\\) può essere interpretata come il rapporto tra la covarianza campionaria tra \\(x\\) e \\(Y\\) e la varianza campionaria di \\(x\\).\nUna volta stimati i coefficienti, si procede alla stima della varianza degli errori. Lo stimatore di massima verosimiglianza utilizza il denominatore \\(n\\) ed è quindi distorto. Lo stimatore non distorto della varianza degli errori è invece \\[\nS_e^2\n= \\frac{1}{n - 2} \\sum_{i=1}^n \\bigl(Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\bigr)^2,\n\\] dove il termine \\(n-2\\) rappresenta i gradi di libertà residui dopo la stima dei due parametri \\(\\beta_0\\) e \\(\\beta_1\\).",
    "crumbs": [
      "Theory",
      "Funzioni di Perdita e Inferenza"
    ]
  },
  {
    "objectID": "th/09_Funzioni_di_Perdita_e_Inferenza.html#teorema-di-cochran",
    "href": "th/09_Funzioni_di_Perdita_e_Inferenza.html#teorema-di-cochran",
    "title": "Funzioni di Perdita e Inferenza",
    "section": "",
    "text": "Il teorema di Cochran fornisce la base teorica per comprendere come la variabilità totale dei dati possa essere scomposta in modo rigoroso nei modelli lineari sotto ipotesi gaussiane. L’idea centrale è che, quando le osservazioni seguono una distribuzione normale con varianza costante e i parametri vengono stimati all’interno di un modello lineare, la parte dei dati spiegata dal modello e la parte residua possono essere trattate come componenti indipendenti, ciascuna con una struttura probabilistica ben definita. Questo risultato giustifica formalmente la scomposizione della varianza utilizzata in regressione lineare, ANOVA e test F.\nSi consideri un vettore aleatorio \\(X=(X_1,\\dots,X_n)\\in\\mathbb{R}^n\\) con componenti indipendenti tali che \\[\nX_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2),\n\\] e si assuma che il vettore delle medie \\(\\mu=(\\mu_1,\\dots,\\mu_n)\\) appartenga a un sottospazio vettoriale \\(V\\subset\\mathbb{R}^n\\) di dimensione \\(k\\). In questo contesto, la stima di massima verosimiglianza di \\(\\mu\\) si ottiene come proiezione ortogonale del vettore osservato \\(X\\) sul sottospazio \\(V\\). Indicando con \\(\\pi_V(X)\\) tale proiezione, vale \\[\n\\hat{\\mu}=\\pi_V(X).\n\\] Dal punto di vista geometrico, questa stima è quella che minimizza la distanza euclidea tra \\(X\\) e l’insieme dei vettori ammissibili per \\(\\mu\\), coerentemente con il criterio dei minimi quadrati.\nLa componente di \\(X\\) non spiegata dal modello è il residuo \\(X-\\pi_V(X)\\). La sua norma al quadrato \\[\nW=\\|X-\\pi_V(X)\\|^2\n\\] rappresenta la somma dei quadrati residui. Il teorema di Cochran afferma che la quantità \\(W\\) è indipendente dalla componente stimata \\(\\pi_V(X)\\) e che, normalizzata per la varianza comune \\(\\sigma^2\\), segue una distribuzione chi-quadro con \\(n-k\\) gradi di libertà: \\[\n\\frac{W}{\\sigma^2}\\sim\\chi^2(n-k).\n\\]\nL’indipendenza tra la parte spiegata dal modello e la parte residua, insieme alla distribuzione chi-quadro della somma dei quadrati residui, costituisce il fondamento teorico della scomposizione della varianza. È proprio questo risultato che rende possibile confrontare in modo corretto la variabilità spiegata dal modello con quella attribuibile al rumore, utilizzando distribuzioni note e strumenti di inferenza statistica affidabili nei modelli di regressione lineare.",
    "crumbs": [
      "Theory",
      "Funzioni di Perdita e Inferenza"
    ]
  },
  {
    "objectID": "th/06. Verifica delle ipotesi.html",
    "href": "th/06. Verifica delle ipotesi.html",
    "title": "Verifica delle ipotesi",
    "section": "",
    "text": "Supponiamo, ancora una volta, di disporre di un campione aleatorio proveniente da una distribuzione che ci è nota tranne che per uno o più parametri incogniti. La nuova chiave di lettura non consiste più nello stimare direttamente questi parametri, ma piuttosto nell’utilizzare il campione per verificare un’ipotesi che li riguarda.\nUn’ipotesi statistica è un’affermazione su uno o più parametri della distribuzione di popolazione. Si parla di “ipotesi” proprio perché, a priori, non sappiamo se essa sia compatibile o meno con i valori del campione osservato.\nNota. Quando accettiamo (o rifiutiamo) un’ipotesi, non stiamo dicendo che essa sia necessariamente vera (o falsa), ma solo che i dati raccolti sono accettabilmente in accordo (o in disaccordo) con essa.\n\n\nData una popolazione con distribuzione \\(F_\\theta\\), che dipende da un parametro incognito \\(\\theta\\), supponiamo di voler verificare una certa ipotesi su \\(\\theta\\), detta ipotesi nulla, indicata con \\(H_0\\).\nAd esempio, se \\(F_\\theta\\) è una distribuzione normale con media \\(\\theta\\) e varianza \\(1\\), due possibili ipotesi nulle sono:\n\n\\(H_0 : \\theta = 1\\)\n\\(H_0 : \\theta \\le 1\\)\n\nLa prima ipotesi afferma che la popolazione ha distribuzione \\(\\mathcal{N}(1,1)\\); la seconda afferma che la media non supera 1, pur rimanendo all’interno di una distribuzione normale con varianza 1.\n\nNel primo caso (\\(H_0 : \\theta = 1\\)), l’ipotesi determina completamente la distribuzione → si parla di ipotesi semplice.\n\nNel secondo caso, la distribuzione non è completamente specificata → si parla di ipotesi composta.\n\nSupponiamo ora di avere un campione aleatorio \\(X_1, \\dots, X_n\\) proveniente dalla popolazione, e di volerlo usare per eseguire un test statistico sull’ipotesi nulla \\(H_0\\). Poiché la decisione si basa solo sui valori osservati, definiamo una regione critica \\(C\\) nello spazio \\(n\\)-dimensionale: se \\((X_1, \\dots, X_n) \\in C\\), rifiutiamo \\(H_0\\); altrimenti, l’accettiamo.\nIl test è quindi definito come: - Accetta \\(H_0\\) se \\((X_1, \\dots, X_n) \\notin C\\) - Rifiuta \\(H_0\\) se \\((X_1, \\dots, X_n) \\in C\\)\nDurante un test statistico, possono verificarsi due tipi di errore: - Errore di I specie: si rifiuta \\(H_0\\) quando è vera - Errore di II specie: si accetta \\(H_0\\) quando è falsa\nL’obiettivo non è stabilire con certezza se \\(H_0\\) sia vera o falsa, ma valutare se sia compatibile con i dati osservati. Per questo, è comune essere cauti nel rifiutare \\(H_0\\), richiedendo che i dati osservati siano molto improbabili se \\(H_0\\) fosse vera.\nQuesto si ottiene specificando un valore \\(\\alpha\\), detto livello di significatività, e imponendo che la probabilità di errore di prima specie non superi \\(\\alpha\\):\n\nUn test di livello \\(\\alpha\\) ha:\n\\[P(\\text{rifiutare } H_0 \\mid H_0 \\text{ vera}) \\le \\alpha\\]\n\n\n\n\nSupponiamo di avere un campione aleatorio \\(X_1, \\dots, X_n\\) proveniente da una popolazione normale con media incognita \\(\\mu\\) e varianza nota \\(\\sigma^2\\).\nVogliamo verificare:\n\nIpotesi nulla: \\(H_0 : \\mu = \\mu_0\\)\nIpotesi alternativa: \\(H_1 : \\mu \\neq \\mu_0\\)\n\nPoiché \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) è uno stimatore naturale di \\(\\mu\\), ha senso accettare \\(H_0\\) se \\(\\bar{X}\\) non è troppo distante da \\(\\mu_0\\).\nDefiniamo la regione critica come:\n\\[\nC := \\{(X_1, \\dots, X_n) : |\\bar{X} - \\mu_0| &gt; c\\}\n\\]\nIl valore di \\(c\\) viene scelto in modo da garantire che la probabilità di errore di I specie sia esattamente \\(\\alpha\\):\n\\[\n\\alpha = P_{\\mu_0}(|\\bar{X} - \\mu_0| &gt; c)\n\\]\nPoiché, sotto \\(H_0\\), \\(\\bar{X} \\sim \\mathcal{N}(\\mu_0, \\sigma^2/n)\\), possiamo standardizzare:\n\\[\n\\dfrac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim^{\\mu_0} Z \\sim \\mathcal{N}(0,1)\n\\]\nAllora,\n\\[\n\\alpha = P_{\\mu_0}\\left(\\left|\\dfrac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}\\right| &gt; \\dfrac{c\\sqrt{n}}{\\sigma}\\right)\n= 2P\\left(Z &gt; \\dfrac{c\\sqrt{n}}{\\sigma}\\right)\n\\]\nPer definizione di quantile \\(z_{\\alpha/2}\\), si ha:\n\\[\nP(Z &gt; z_{\\alpha/2}) = \\alpha/2 \\quad \\Rightarrow \\quad \\dfrac{c\\sqrt{n}}{\\sigma} = z_{\\alpha/2}\n\\]\nQuindi:\n\\[\nc = z_{\\alpha/2} \\cdot \\dfrac{\\sigma}{\\sqrt{n}}\n\\]\n\n\nIl test al livello di significatività \\(\\alpha\\) rifiuta \\(H_0\\) se:\n\\[\n|\\bar{X} - \\mu_0| &gt; z_{\\alpha/2} \\cdot \\dfrac{\\sigma}{\\sqrt{n}}\n\\]\nEquivalente a:\n\\[\n\\left|\\dfrac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}\\right| &gt; z_{\\alpha/2}\n\\]\nAltrimenti, si accetta \\(H_0\\).\nNota. La regione di accettazione della statistica del test è un intervallo centrato in 0, simmetrico rispetto allo stesso.",
    "crumbs": [
      "Theory",
      "Verifica delle ipotesi"
    ]
  },
  {
    "objectID": "th/06. Verifica delle ipotesi.html#livelli-di-significatività",
    "href": "th/06. Verifica delle ipotesi.html#livelli-di-significatività",
    "title": "Verifica delle ipotesi",
    "section": "",
    "text": "Data una popolazione con distribuzione \\(F_\\theta\\), che dipende da un parametro incognito \\(\\theta\\), supponiamo di voler verificare una certa ipotesi su \\(\\theta\\), detta ipotesi nulla, indicata con \\(H_0\\).\nAd esempio, se \\(F_\\theta\\) è una distribuzione normale con media \\(\\theta\\) e varianza \\(1\\), due possibili ipotesi nulle sono:\n\n\\(H_0 : \\theta = 1\\)\n\\(H_0 : \\theta \\le 1\\)\n\nLa prima ipotesi afferma che la popolazione ha distribuzione \\(\\mathcal{N}(1,1)\\); la seconda afferma che la media non supera 1, pur rimanendo all’interno di una distribuzione normale con varianza 1.\n\nNel primo caso (\\(H_0 : \\theta = 1\\)), l’ipotesi determina completamente la distribuzione → si parla di ipotesi semplice.\n\nNel secondo caso, la distribuzione non è completamente specificata → si parla di ipotesi composta.\n\nSupponiamo ora di avere un campione aleatorio \\(X_1, \\dots, X_n\\) proveniente dalla popolazione, e di volerlo usare per eseguire un test statistico sull’ipotesi nulla \\(H_0\\). Poiché la decisione si basa solo sui valori osservati, definiamo una regione critica \\(C\\) nello spazio \\(n\\)-dimensionale: se \\((X_1, \\dots, X_n) \\in C\\), rifiutiamo \\(H_0\\); altrimenti, l’accettiamo.\nIl test è quindi definito come: - Accetta \\(H_0\\) se \\((X_1, \\dots, X_n) \\notin C\\) - Rifiuta \\(H_0\\) se \\((X_1, \\dots, X_n) \\in C\\)\nDurante un test statistico, possono verificarsi due tipi di errore: - Errore di I specie: si rifiuta \\(H_0\\) quando è vera - Errore di II specie: si accetta \\(H_0\\) quando è falsa\nL’obiettivo non è stabilire con certezza se \\(H_0\\) sia vera o falsa, ma valutare se sia compatibile con i dati osservati. Per questo, è comune essere cauti nel rifiutare \\(H_0\\), richiedendo che i dati osservati siano molto improbabili se \\(H_0\\) fosse vera.\nQuesto si ottiene specificando un valore \\(\\alpha\\), detto livello di significatività, e imponendo che la probabilità di errore di prima specie non superi \\(\\alpha\\):\n\nUn test di livello \\(\\alpha\\) ha:\n\\[P(\\text{rifiutare } H_0 \\mid H_0 \\text{ vera}) \\le \\alpha\\]",
    "crumbs": [
      "Theory",
      "Verifica delle ipotesi"
    ]
  },
  {
    "objectID": "th/06. Verifica delle ipotesi.html#verifica-di-ipotesi-sulla-media-di-una-popolazione-normale",
    "href": "th/06. Verifica delle ipotesi.html#verifica-di-ipotesi-sulla-media-di-una-popolazione-normale",
    "title": "Verifica delle ipotesi",
    "section": "",
    "text": "Supponiamo di avere un campione aleatorio \\(X_1, \\dots, X_n\\) proveniente da una popolazione normale con media incognita \\(\\mu\\) e varianza nota \\(\\sigma^2\\).\nVogliamo verificare:\n\nIpotesi nulla: \\(H_0 : \\mu = \\mu_0\\)\nIpotesi alternativa: \\(H_1 : \\mu \\neq \\mu_0\\)\n\nPoiché \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) è uno stimatore naturale di \\(\\mu\\), ha senso accettare \\(H_0\\) se \\(\\bar{X}\\) non è troppo distante da \\(\\mu_0\\).\nDefiniamo la regione critica come:\n\\[\nC := \\{(X_1, \\dots, X_n) : |\\bar{X} - \\mu_0| &gt; c\\}\n\\]\nIl valore di \\(c\\) viene scelto in modo da garantire che la probabilità di errore di I specie sia esattamente \\(\\alpha\\):\n\\[\n\\alpha = P_{\\mu_0}(|\\bar{X} - \\mu_0| &gt; c)\n\\]\nPoiché, sotto \\(H_0\\), \\(\\bar{X} \\sim \\mathcal{N}(\\mu_0, \\sigma^2/n)\\), possiamo standardizzare:\n\\[\n\\dfrac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim^{\\mu_0} Z \\sim \\mathcal{N}(0,1)\n\\]\nAllora,\n\\[\n\\alpha = P_{\\mu_0}\\left(\\left|\\dfrac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}\\right| &gt; \\dfrac{c\\sqrt{n}}{\\sigma}\\right)\n= 2P\\left(Z &gt; \\dfrac{c\\sqrt{n}}{\\sigma}\\right)\n\\]\nPer definizione di quantile \\(z_{\\alpha/2}\\), si ha:\n\\[\nP(Z &gt; z_{\\alpha/2}) = \\alpha/2 \\quad \\Rightarrow \\quad \\dfrac{c\\sqrt{n}}{\\sigma} = z_{\\alpha/2}\n\\]\nQuindi:\n\\[\nc = z_{\\alpha/2} \\cdot \\dfrac{\\sigma}{\\sqrt{n}}\n\\]\n\n\nIl test al livello di significatività \\(\\alpha\\) rifiuta \\(H_0\\) se:\n\\[\n|\\bar{X} - \\mu_0| &gt; z_{\\alpha/2} \\cdot \\dfrac{\\sigma}{\\sqrt{n}}\n\\]\nEquivalente a:\n\\[\n\\left|\\dfrac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}\\right| &gt; z_{\\alpha/2}\n\\]\nAltrimenti, si accetta \\(H_0\\).\nNota. La regione di accettazione della statistica del test è un intervallo centrato in 0, simmetrico rispetto allo stesso.",
    "crumbs": [
      "Theory",
      "Verifica delle ipotesi"
    ]
  },
  {
    "objectID": "th/04. La distribuzione delle statistiche campionarie.html",
    "href": "th/04. La distribuzione delle statistiche campionarie.html",
    "title": "La distribuzione delle statistiche campionarie",
    "section": "",
    "text": "La statistica è la scienza che si occupa di trarre conclusioni dai dati sperimentali. Una situazione tipica riguarda lo studio di un insieme molto grande, detto popolazione, composto da oggetti a cui sono associate quantità misurabili. L’approccio statistico consiste nel selezionare un sottoinsieme ridotto di oggetti, chiamato campione, e analizzarlo per trarre conclusioni valide per l’intera popolazione.\nPer poter effettuare inferenze sulla popolazione basandosi sui dati del campione, è necessario assumere alcune condizioni sulle relazioni che legano questi due insiemi. Una ipotesi fondamentale è che esista una distribuzione di probabilità nella popolazione, nel senso che, se si estraggono oggetti in modo casuale, le quantità numeriche a essi associate possono essere pensate come variabili aleatorie indipendenti e identicamente distribuite secondo una certa distribuzione \\(F\\). Se il campione viene selezionato in modo casuale, è ragionevole supporre che i suoi dati siano valori indipendenti provenienti da tale distribuzione.\nUn insieme di variabili aleatorie indipendenti e identicamente distribuite (i.i.d.)\n\\(X_1, X_2, \\dots, X_n\\), tutte con la stessa distribuzione \\(F\\), si dice campione aleatorio estratto dalla distribuzione \\(F\\).\nIn pratica, la distribuzione \\(F\\) non è mai completamente nota, ma è possibile utilizzare i dati per fare inferenza su di essa. In alcuni casi, \\(F\\) può essere nota a meno di alcuni parametri incogniti; in altri, potremmo non sapere nulla su \\(F\\). I problemi in cui la distribuzione è nota eccetto che per un insieme di parametri incogniti sono detti problemi di inferenza parametrica, mentre quelli in cui non si sa nulla sulla distribuzione sono problemi di inferenza non parametrica.\nIl termine statistica indica una variabile aleatoria che è una funzione dei dati di un campione.\n\n\nData una popolazione di elementi con una quantità misurabile associata a ciascuno, consideriamo un campione aleatorio di dati \\(X_1, X_2, \\dots, X_n\\) estratto da questa popolazione. Denotiamo con \\(\\mu\\) e \\(\\sigma^2\\) la media e la varianza della popolazione. La media campionaria è definita come:\n\\[\n\\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_i\n\\]\nPoiché \\(\\bar{X}\\) è una funzione delle variabili aleatorie \\(X_i\\), essa stessa è una variabile aleatoria e una statistica.\nCalcoliamo l’aspettazione della media campionaria:\n\\[\nE[\\bar{X}] = E\\left[ \\dfrac{1}{n} \\sum_{i=1}^{n} X_i \\right] = \\dfrac{1}{n} \\sum_{i=1}^{n} E[X_i] = \\dfrac{n\\mu}{n} = \\mu\n\\]\nQuindi, la media campionaria è uno stimatore non distorto della media \\(\\mu\\) della popolazione.\nLa varianza della media campionaria è:\n\\[\n\\operatorname{Var}(\\bar{X}) = \\operatorname{Var}\\left( \\dfrac{1}{n} \\sum_{i=1}^{n} X_i \\right) = \\dfrac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(X_i) = \\dfrac{n\\sigma^2}{n^2} = \\dfrac{\\sigma^2}{n}\n\\]\nPertanto, la varianza di \\(\\bar{X}\\) diminuisce all’aumentare di \\(n\\). Ciò significa che la media campionaria ha la stessa media \\(\\mu\\) della popolazione, ma la sua variabilità si riduce con l’aumentare della dimensione del campione.\n\n\n\nIl teorema del limite centrale afferma che la somma di un gran numero di variabili aleatorie indipendenti e identicamente distribuite tende ad avere una distribuzione approssimativamente normale, indipendentemente dalla distribuzione originale delle variabili.\nFormalmente, siano \\(X_1, X_2, \\dots, X_n\\) variabili aleatorie i.i.d. con media \\(\\mu\\) e varianza \\(\\sigma^2\\). Allora, per \\(n\\) sufficientemente grande, la somma \\(S_n = X_1 + X_2 + \\dots + X_n\\) è approssimativamente normale con media \\(n\\mu\\) e varianza \\(n\\sigma^2\\):\n\\[\nS_n \\approx \\mathcal{N}(n\\mu, n\\sigma^2)\n\\]\nNormalizzando la somma, otteniamo una distribuzione normale standard:\n\\[\n\\dfrac{S_n - n\\mu}{\\sigma\\sqrt{n}} \\approx \\mathcal{N}(0, 1)\n\\]\nQuesto risultato implica che anche la media campionaria \\(\\bar{X}\\) è approssimativamente normale per grandi \\(n\\):\n\\[\n\\bar{X} = \\dfrac{S_n}{n} \\approx \\mathcal{N}\\left( \\mu, \\dfrac{\\sigma^2}{n} \\right)\n\\]\nQuindi, la variabile standardizzata:\n\\[\n\\dfrac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\approx \\mathcal{N}(0, 1)\n\\]\nApplicazione al caso binomiale: Se \\(X\\) è una variabile aleatoria binomiale con parametri \\(n\\) e \\(p\\), può essere vista come la somma di \\(n\\) variabili di Bernoulli indipendenti \\(X_i\\), dove:\n\\[\nX_i =\n\\begin{cases}\n1 & \\text{con probabilità } p \\\\\n0 & \\text{con probabilità } 1 - p\n\\end{cases}\n\\]\nPoiché \\(E[X_i] = p\\) e \\(\\operatorname{Var}(X_i) = p(1 - p)\\), per \\(n\\) grande, il teorema del limite centrale ci permette di approssimare la distribuzione binomiale con una normale:\n\\[\n\\dfrac{X - np}{\\sqrt{np(1 - p)}} \\approx \\mathcal{N}(0, 1)\n\\]\n\n\n\nSia \\(X_1, X_2, \\dots, X_n\\) un campione aleatorio proveniente da una distribuzione con media \\(\\mu\\) e varianza \\(\\sigma^2\\). La varianza campionaria \\(S^2\\) è definita come:\n\\[\nS^2 = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\]\ndove \\(\\bar{X}\\) è la media campionaria. La radice quadrata di \\(S^2\\), denotata con \\(S\\), è la deviazione standard campionaria.\nUna proprietà importante è che \\(S^2\\) è uno stimatore non distorto della varianza \\(\\sigma^2\\) della popolazione:\n\\[\nE[S^2] = \\sigma^2\n\\]\nCiò significa che, in media, la varianza campionaria \\(S^2\\) coincide con la varianza della popolazione.\n\n\n\nSia \\(X_1, X_2, \\dots, X_n\\) un campione estratto da una distribuzione normale \\(\\mathcal{N}(\\mu, \\sigma^2)\\), con le \\(X_i\\) indipendenti tra loro. La media e la varianza campionarie sono:\n\\[\n\\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_i, \\quad S^2 = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\]\nPoiché la somma di variabili normali indipendenti è ancora normale, la media campionaria \\(\\bar{X}\\) segue una distribuzione normale con media \\(\\mu\\) e varianza \\(\\sigma^2 / n\\):\n\\[\n\\bar{X} \\sim \\mathcal{N}\\left( \\mu, \\dfrac{\\sigma^2}{n} \\right)\n\\]\nPertanto, la variabile standardizzata:\n\\[\nZ = \\dfrac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, 1)\n\\]\nInoltre, nel caso di campioni da una popolazione normale, la varianza campionaria \\(S^2\\) (opportunamente scalata) segue una distribuzione chi quadrato con \\(n - 1\\) gradi di libertà:\n\\[\n\\dfrac{(n - 1) S^2}{\\sigma^2} \\sim \\chi^2_{n - 1}\n\\]\nUn aspetto fondamentale è che \\(\\bar{X}\\) e \\(S^2\\) sono variabili aleatorie indipendenti nel caso normale. Questa proprietà consente di utilizzare la distribuzione \\(t\\) di Student per costruire intervalli di confidenza e test statistici. In particolare, la variabile:\n\\[\nT = \\dfrac{\\bar{X} - \\mu}{S / \\sqrt{n}} \\sim t_{n - 1}\n\\]\nsegue una distribuzione \\(t\\) di Student con \\(n - 1\\) gradi di libertà.\n\n\n\nData una popolazione finita di \\(N\\) elementi, un campione aleatorio di dimensione \\(n\\) è un sottoinsieme di \\(n\\) elementi scelto in modo tale che tutti i \\(\\binom{N}{n}\\) possibili sottoinsiemi abbiano la stessa probabilità di essere selezionati.\nSupponiamo che una frazione \\(p\\) degli elementi della popolazione possieda una certa caratteristica. Allora, ci sono \\(pN\\) elementi con la caratteristica e \\((1 - p)N\\) senza. Selezionando un campione casuale di dimensione \\(n\\), definiamo:\n\\[\nX_i =\n\\begin{cases}\n1 & \\text{se l' } i\\text{-esimo elemento possiede la caratteristica} \\\\\n0 & \\text{altrimenti}\n\\end{cases}\n\\]\nLa somma \\(X = X_1 + X_2 + \\dots + X_n\\) rappresenta il numero di elementi nel campione che possiedono la caratteristica. La media campionaria è quindi:\n\\[\n\\bar{X} = \\dfrac{X}{n}\n\\]\nNotiamo che:\n\\[\nP(X_i = 1) = p\n\\]\nTuttavia, le variabili \\(X_i\\) non sono indipendenti perché la selezione è fatta senza reinserimento. La probabilità condizionata dipende dagli esiti precedenti:\n\\[\nP(X_j = 1 \\mid X_i = 1) = \\dfrac{pN - 1}{N - 1}, \\quad P(X_j = 1 \\mid X_i = 0) = \\dfrac{pN}{N - 1}\n\\]\nQuando \\(N\\) è molto grande rispetto a \\(n\\), questa dipendenza è trascurabile, e le \\(X_i\\) possono essere considerate approssimativamente indipendenti.\nLa media e la varianza di \\(X\\) sono:\n\\[\nE[X] = np\n\\]\n\\[\n\\operatorname{Var}(X) = np(1 - p) \\left( \\dfrac{N - n}{N - 1} \\right)\n\\]\nIl fattore \\(\\dfrac{N - n}{N - 1}\\) è noto come fattore di correzione per popolazioni finite. Per \\(N \\gg n\\), questo fattore è circa 1, e le formule si riducono a quelle per il campionamento con reinserimento o per popolazioni infinite.\nPer la media campionaria \\(\\bar{X}\\), otteniamo:\n\\[\nE[\\bar{X}] = p\n\\]\n\\[\n\\operatorname{Var}(\\bar{X}) = \\dfrac{\\operatorname{Var}(X)}{n^2} = \\dfrac{p(1 - p)}{n} \\left( \\dfrac{N - n}{N - 1} \\right)\n\\]\nQuesto risultato mostra che la media campionaria è uno stimatore non distorto della proporzione \\(p\\) nella popolazione, e la sua varianza tiene conto della dimensione finita della popolazione attraverso il fattore di correzione.\nNota. In tutti i casi trattati, l’aumento della dimensione del campione \\(n\\) porta a una riduzione della varianza degli stimatori, migliorando la precisione delle inferenze statistiche.",
    "crumbs": [
      "Theory",
      "La distribuzione delle statistiche campionarie"
    ]
  },
  {
    "objectID": "th/04. La distribuzione delle statistiche campionarie.html#la-media-campionaria",
    "href": "th/04. La distribuzione delle statistiche campionarie.html#la-media-campionaria",
    "title": "La distribuzione delle statistiche campionarie",
    "section": "",
    "text": "Data una popolazione di elementi con una quantità misurabile associata a ciascuno, consideriamo un campione aleatorio di dati \\(X_1, X_2, \\dots, X_n\\) estratto da questa popolazione. Denotiamo con \\(\\mu\\) e \\(\\sigma^2\\) la media e la varianza della popolazione. La media campionaria è definita come:\n\\[\n\\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_i\n\\]\nPoiché \\(\\bar{X}\\) è una funzione delle variabili aleatorie \\(X_i\\), essa stessa è una variabile aleatoria e una statistica.\nCalcoliamo l’aspettazione della media campionaria:\n\\[\nE[\\bar{X}] = E\\left[ \\dfrac{1}{n} \\sum_{i=1}^{n} X_i \\right] = \\dfrac{1}{n} \\sum_{i=1}^{n} E[X_i] = \\dfrac{n\\mu}{n} = \\mu\n\\]\nQuindi, la media campionaria è uno stimatore non distorto della media \\(\\mu\\) della popolazione.\nLa varianza della media campionaria è:\n\\[\n\\operatorname{Var}(\\bar{X}) = \\operatorname{Var}\\left( \\dfrac{1}{n} \\sum_{i=1}^{n} X_i \\right) = \\dfrac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(X_i) = \\dfrac{n\\sigma^2}{n^2} = \\dfrac{\\sigma^2}{n}\n\\]\nPertanto, la varianza di \\(\\bar{X}\\) diminuisce all’aumentare di \\(n\\). Ciò significa che la media campionaria ha la stessa media \\(\\mu\\) della popolazione, ma la sua variabilità si riduce con l’aumentare della dimensione del campione.",
    "crumbs": [
      "Theory",
      "La distribuzione delle statistiche campionarie"
    ]
  },
  {
    "objectID": "th/04. La distribuzione delle statistiche campionarie.html#il-teorema-del-limite-centrale",
    "href": "th/04. La distribuzione delle statistiche campionarie.html#il-teorema-del-limite-centrale",
    "title": "La distribuzione delle statistiche campionarie",
    "section": "",
    "text": "Il teorema del limite centrale afferma che la somma di un gran numero di variabili aleatorie indipendenti e identicamente distribuite tende ad avere una distribuzione approssimativamente normale, indipendentemente dalla distribuzione originale delle variabili.\nFormalmente, siano \\(X_1, X_2, \\dots, X_n\\) variabili aleatorie i.i.d. con media \\(\\mu\\) e varianza \\(\\sigma^2\\). Allora, per \\(n\\) sufficientemente grande, la somma \\(S_n = X_1 + X_2 + \\dots + X_n\\) è approssimativamente normale con media \\(n\\mu\\) e varianza \\(n\\sigma^2\\):\n\\[\nS_n \\approx \\mathcal{N}(n\\mu, n\\sigma^2)\n\\]\nNormalizzando la somma, otteniamo una distribuzione normale standard:\n\\[\n\\dfrac{S_n - n\\mu}{\\sigma\\sqrt{n}} \\approx \\mathcal{N}(0, 1)\n\\]\nQuesto risultato implica che anche la media campionaria \\(\\bar{X}\\) è approssimativamente normale per grandi \\(n\\):\n\\[\n\\bar{X} = \\dfrac{S_n}{n} \\approx \\mathcal{N}\\left( \\mu, \\dfrac{\\sigma^2}{n} \\right)\n\\]\nQuindi, la variabile standardizzata:\n\\[\n\\dfrac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\approx \\mathcal{N}(0, 1)\n\\]\nApplicazione al caso binomiale: Se \\(X\\) è una variabile aleatoria binomiale con parametri \\(n\\) e \\(p\\), può essere vista come la somma di \\(n\\) variabili di Bernoulli indipendenti \\(X_i\\), dove:\n\\[\nX_i =\n\\begin{cases}\n1 & \\text{con probabilità } p \\\\\n0 & \\text{con probabilità } 1 - p\n\\end{cases}\n\\]\nPoiché \\(E[X_i] = p\\) e \\(\\operatorname{Var}(X_i) = p(1 - p)\\), per \\(n\\) grande, il teorema del limite centrale ci permette di approssimare la distribuzione binomiale con una normale:\n\\[\n\\dfrac{X - np}{\\sqrt{np(1 - p)}} \\approx \\mathcal{N}(0, 1)\n\\]",
    "crumbs": [
      "Theory",
      "La distribuzione delle statistiche campionarie"
    ]
  },
  {
    "objectID": "th/04. La distribuzione delle statistiche campionarie.html#la-varianza-campionaria",
    "href": "th/04. La distribuzione delle statistiche campionarie.html#la-varianza-campionaria",
    "title": "La distribuzione delle statistiche campionarie",
    "section": "",
    "text": "Sia \\(X_1, X_2, \\dots, X_n\\) un campione aleatorio proveniente da una distribuzione con media \\(\\mu\\) e varianza \\(\\sigma^2\\). La varianza campionaria \\(S^2\\) è definita come:\n\\[\nS^2 = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\]\ndove \\(\\bar{X}\\) è la media campionaria. La radice quadrata di \\(S^2\\), denotata con \\(S\\), è la deviazione standard campionaria.\nUna proprietà importante è che \\(S^2\\) è uno stimatore non distorto della varianza \\(\\sigma^2\\) della popolazione:\n\\[\nE[S^2] = \\sigma^2\n\\]\nCiò significa che, in media, la varianza campionaria \\(S^2\\) coincide con la varianza della popolazione.",
    "crumbs": [
      "Theory",
      "La distribuzione delle statistiche campionarie"
    ]
  },
  {
    "objectID": "th/04. La distribuzione delle statistiche campionarie.html#le-distribuzioni-delle-statistiche-di-popolazioni-normali",
    "href": "th/04. La distribuzione delle statistiche campionarie.html#le-distribuzioni-delle-statistiche-di-popolazioni-normali",
    "title": "La distribuzione delle statistiche campionarie",
    "section": "",
    "text": "Sia \\(X_1, X_2, \\dots, X_n\\) un campione estratto da una distribuzione normale \\(\\mathcal{N}(\\mu, \\sigma^2)\\), con le \\(X_i\\) indipendenti tra loro. La media e la varianza campionarie sono:\n\\[\n\\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_i, \\quad S^2 = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\]\nPoiché la somma di variabili normali indipendenti è ancora normale, la media campionaria \\(\\bar{X}\\) segue una distribuzione normale con media \\(\\mu\\) e varianza \\(\\sigma^2 / n\\):\n\\[\n\\bar{X} \\sim \\mathcal{N}\\left( \\mu, \\dfrac{\\sigma^2}{n} \\right)\n\\]\nPertanto, la variabile standardizzata:\n\\[\nZ = \\dfrac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, 1)\n\\]\nInoltre, nel caso di campioni da una popolazione normale, la varianza campionaria \\(S^2\\) (opportunamente scalata) segue una distribuzione chi quadrato con \\(n - 1\\) gradi di libertà:\n\\[\n\\dfrac{(n - 1) S^2}{\\sigma^2} \\sim \\chi^2_{n - 1}\n\\]\nUn aspetto fondamentale è che \\(\\bar{X}\\) e \\(S^2\\) sono variabili aleatorie indipendenti nel caso normale. Questa proprietà consente di utilizzare la distribuzione \\(t\\) di Student per costruire intervalli di confidenza e test statistici. In particolare, la variabile:\n\\[\nT = \\dfrac{\\bar{X} - \\mu}{S / \\sqrt{n}} \\sim t_{n - 1}\n\\]\nsegue una distribuzione \\(t\\) di Student con \\(n - 1\\) gradi di libertà.",
    "crumbs": [
      "Theory",
      "La distribuzione delle statistiche campionarie"
    ]
  },
  {
    "objectID": "th/04. La distribuzione delle statistiche campionarie.html#campionamento-da-insiemi-finiti",
    "href": "th/04. La distribuzione delle statistiche campionarie.html#campionamento-da-insiemi-finiti",
    "title": "La distribuzione delle statistiche campionarie",
    "section": "",
    "text": "Data una popolazione finita di \\(N\\) elementi, un campione aleatorio di dimensione \\(n\\) è un sottoinsieme di \\(n\\) elementi scelto in modo tale che tutti i \\(\\binom{N}{n}\\) possibili sottoinsiemi abbiano la stessa probabilità di essere selezionati.\nSupponiamo che una frazione \\(p\\) degli elementi della popolazione possieda una certa caratteristica. Allora, ci sono \\(pN\\) elementi con la caratteristica e \\((1 - p)N\\) senza. Selezionando un campione casuale di dimensione \\(n\\), definiamo:\n\\[\nX_i =\n\\begin{cases}\n1 & \\text{se l' } i\\text{-esimo elemento possiede la caratteristica} \\\\\n0 & \\text{altrimenti}\n\\end{cases}\n\\]\nLa somma \\(X = X_1 + X_2 + \\dots + X_n\\) rappresenta il numero di elementi nel campione che possiedono la caratteristica. La media campionaria è quindi:\n\\[\n\\bar{X} = \\dfrac{X}{n}\n\\]\nNotiamo che:\n\\[\nP(X_i = 1) = p\n\\]\nTuttavia, le variabili \\(X_i\\) non sono indipendenti perché la selezione è fatta senza reinserimento. La probabilità condizionata dipende dagli esiti precedenti:\n\\[\nP(X_j = 1 \\mid X_i = 1) = \\dfrac{pN - 1}{N - 1}, \\quad P(X_j = 1 \\mid X_i = 0) = \\dfrac{pN}{N - 1}\n\\]\nQuando \\(N\\) è molto grande rispetto a \\(n\\), questa dipendenza è trascurabile, e le \\(X_i\\) possono essere considerate approssimativamente indipendenti.\nLa media e la varianza di \\(X\\) sono:\n\\[\nE[X] = np\n\\]\n\\[\n\\operatorname{Var}(X) = np(1 - p) \\left( \\dfrac{N - n}{N - 1} \\right)\n\\]\nIl fattore \\(\\dfrac{N - n}{N - 1}\\) è noto come fattore di correzione per popolazioni finite. Per \\(N \\gg n\\), questo fattore è circa 1, e le formule si riducono a quelle per il campionamento con reinserimento o per popolazioni infinite.\nPer la media campionaria \\(\\bar{X}\\), otteniamo:\n\\[\nE[\\bar{X}] = p\n\\]\n\\[\n\\operatorname{Var}(\\bar{X}) = \\dfrac{\\operatorname{Var}(X)}{n^2} = \\dfrac{p(1 - p)}{n} \\left( \\dfrac{N - n}{N - 1} \\right)\n\\]\nQuesto risultato mostra che la media campionaria è uno stimatore non distorto della proporzione \\(p\\) nella popolazione, e la sua varianza tiene conto della dimensione finita della popolazione attraverso il fattore di correzione.\nNota. In tutti i casi trattati, l’aumento della dimensione del campione \\(n\\) porta a una riduzione della varianza degli stimatori, migliorando la precisione delle inferenze statistiche.",
    "crumbs": [
      "Theory",
      "La distribuzione delle statistiche campionarie"
    ]
  },
  {
    "objectID": "th/07. Vettori aleatori.html",
    "href": "th/07. Vettori aleatori.html",
    "title": "Vettori aleatori",
    "section": "",
    "text": "Vettori aleatori\nNel contesto del machine learning e della data science, è comune partire da un dataset organizzato in forma di tabella, dove le colonne rappresentano variabili (o caratteristiche) e le righe rappresentano le osservazioni. Quando si studiano più variabili contemporaneamente, si parla di situazione multivariata, mentre se si analizza una sola variabile si parla di situazione univariata.\nPer descrivere formalmente la natura casuale delle variabili, si utilizza il concetto di vettori aleatori, in particolare ci si concentra quasi sempre sui primi due momenti: la media e la covarianza.\nUn vettore aleatorio di dimensione \\(m\\) è \\(X = (X_1, X_2, \\ldots, X_m)\\), le cui componenti sono variabili casuali (potenzialmente dipendenti tra loro). Ad esempio, per \\(m=6\\) si possono considerare dati ordinati \\(X_{(1)} \\le X_{(2)} \\le \\ldots \\le X_{(6)}\\), che non sono indipendenti.\nLa media o valore atteso di \\(X\\) è un vettore deterministico \\(\\mu = E[X]\\) in \\(\\mathbb{R}^m\\), con componenti \\(\\mu_i = E[X_i]\\).\nLa matrice di covarianza di \\(X\\) è \\(\\Sigma = C(X)\\), una matrice quadrata di ordine \\(m\\), dove \\(\\Sigma_{ij} = \\mathrm{Cov}(X_i, X_j)\\). Sulla diagonale di \\(\\Sigma\\) compaiono le varianze di ciascuna componente (\\(\\Sigma_{ii} = \\mathrm{Var}(X_i)\\)).\n\nLa covarianza tra due variabili aleatorie \\(X\\) e \\(Y\\) si definisce come \\(\\mathrm{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\\). In particolare, \\(\\mathrm{Cov}(X, X) = \\mathrm{Var}(X)\\) e, in caso di indipendenza, \\(\\mathrm{Cov}(X,Y) = 0\\).\nRecall.\n\n\\(Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\\)\n\\(Cov(X, X) = Var(X)\\)\n\\(Cov(X, c) = Cov(c, Y) = 0\\) (dove \\(c\\) è una costante)\n\\(X, Y\\) indipendenti \\(\\implies Cov(X, Y) = 0\\) (l’implicazione inversa non è sempre vera)\n\\(Cov(aX, bY) = ab Cov(X, Y)\\)\n\nSe le componenti di \\(X\\) sono indipendenti, \\(\\Sigma\\) risulta diagonale.\nPer quanto riguarda la proprietà della covarianza, essa è una funzione bilineare, il che implica che si distribuisce linearmente rispetto alle somme e agli scalari. Formalmente, si ha:\n\\(\\operatorname{Cov} \\left( \\sum_{i=1}^{m} a_i X_i, \\sum_{j=1}^{n} b_j Y_j \\right) = \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_i b_j \\operatorname{Cov}(X_i, Y_j)\\)\nEsempio. Legge congiunta nel caso misto\nConsideriamo il caso di una distribuzione congiunta in cui una variabile \\(X\\) è discreta e una variabile \\(Y\\) è continua. Supponiamo che \\(X\\) rappresenti il sesso di un individuo e \\(Y\\) la sua statura. In particolare, \\(X\\) può assumere i valori in \\(\\{0, 1\\}\\), dove \\(P(X = 1) = 1/2\\), ovvero la probabilità di essere maschio o femmina è uguale.\nLa variabile \\(Y\\), che rappresenta la statura, segue una distribuzione normale (gaussiana) condizionatamente a \\(X\\). Tuttavia, i parametri della distribuzione di \\(Y\\) dipendono dal valore assunto da \\(X\\). In altre parole, sia per gli uomini sia per le donne, la statura segue una distribuzione gaussiana, ma con medie e varianze differenti.\n\nL’immagine illustra un esempio di distribuzione congiunta nel caso misto, in cui la variabile \\(X\\) è discreta e la variabile \\(Y\\) è continua. In questo caso, \\(X\\) rappresenta il sesso ( \\(0\\) o \\(1\\) ), mentre \\(Y\\) rappresenta la statura.\nSi osserva che la distribuzione condizionata di \\(Y\\) dipende dal valore assunto da \\(X\\):\n\nSe \\(X = 0\\) (ad esempio, uomini), la statura \\(Y\\) segue una distribuzione normale con media \\(\\mu_0 = 175\\) e varianza \\(\\sigma^2 = 7^2\\).\nSe \\(X = 1\\) (ad esempio, donne), la statura \\(Y\\) segue una distribuzione normale con media \\(\\mu_1 = 168\\) e la stessa varianza \\(\\sigma^2 = 7^2\\).\n\nLa densità condizionata di \\(Y\\) dato \\(X\\) è quindi:\n\\(f_{Y|X} (y \\mid x) = f_{\\mathcal{N}(\\mu_x, \\sigma^2)}(y)\\)\ndove \\(\\mu_x\\) varia in base al valore di \\(X\\). Il grafico a sinistra mostra come la distribuzione di \\(Y\\) cambi a seconda di \\(X\\), mentre il grafico a destra rappresenta le due distribuzioni normali sovrapposte, evidenziando la differenza nelle medie ma la stessa dispersione intorno ai valori centrali.\n\nVettori aleatori e Machine Learning\nNell’apprendimento automatico possiamo distinguere due grandi ambiti:\n\nSupervised Learning. Si cerca di predire variabili di output (target) a partire da variabili di input (features). Gli esempi più comuni sono la regressione (prevedere valori numerici come tempo, soldi, energia) e la classificazione (prevedere la categoria di un’immagine o di un testo). Un esempio semplice è prevedere la statura \\((Y)\\) di una persona dal sesso \\((X)\\). Dopo aver stimato la relazione \\(\\mathrm{P}(Y|X)\\), si può dire che se \\(X = 0\\) (maschio) allora \\(Y \\sim N(175, 7^2)\\), mentre se \\(X = 1\\) (femmina) allora \\(Y \\sim N(168, 7^2)\\). In pratica, la previsione di \\(Y\\) dipende da \\(X\\) e dalla distribuzione condizionata \\(Y|X\\).\nUnsupervised Learning. Qui non esiste una variabile di output da predire. Si cerca piuttosto di comprendere la distribuzione multivariata dell’intero dataset, scoprendone strutture interne come cluster o pattern nascosti. Un classico esempio è la clusterizzazione di cellule, dove le righe della matrice di dati rappresentano le cellule e le colonne rappresentano i geni (fino a decine di migliaia). L’obiettivo è identificare come le cellule si raggruppano in modo naturale e individuare eventuali sottopopolazioni o tipologie cellulari.\n\n\n\nTrasformazioni lineari di vettori aleatori\nConsiderando una funzione lineare da \\(\\mathbb{R}^n\\) a \\(\\mathbb{R}^m\\) data da \\(y = \\alpha + Bx\\), con \\(\\alpha \\in \\mathbb{R}^m\\) e \\(B \\in M_{m,n}\\), se \\(X\\) è un vettore aleatorio di dimensione \\(n\\), allora \\(Y = \\alpha + BX\\) è un vettore aleatorio di dimensione \\(m\\).\n\nLa media di \\(Y\\) è \\(\\mu_Y = E[Y] = \\alpha + B\\,E[X] = \\alpha + B\\mu_X\\). La matrice di covarianza di \\(Y\\) è \\(\\Sigma_Y = C(Y) = B\\,C(X)\\,B^T = B\\,\\Sigma_X\\,B^T\\).\n\nTrasformazione che centra il vettore\nSe si considera \\(Y = X - \\mu_X\\), dove \\(\\mu_X = E[X]\\), si ottiene un nuovo vettore aleatorio con media nulla, poiché \\(E[Y] = 0\\), ma con la stessa matrice di covarianza di \\(X\\). A livello di campione, questa operazione corrisponde a sottrarre la media campionaria da ogni osservazione.\n\n\nTrasformazione che standardizza le variabili\nPer standardizzare ciascuna componente \\(X_i\\), la si centra e si divide per la sua deviazione standard, ottenendo\n\\(Y_i = \\dfrac{X_i - E[X_i]}{\\mathrm{std}(X_i)}\\)\nIn forma matriciale, si può scrivere \\(Y = B(X - E[X])\\), dove \\(B\\) è una matrice diagonale con\n\\(B_{ii} = 1/\\mathrm{std}(X_i)\\).\nCosì facendo, la media di ciascuna componente diventa zero e la nuova matrice di covarianza diventa la matrice di correlazione di \\(X\\). Gli elementi della matrice di correlazione \\(\\rho(X)\\) sono\n\\(\\rho_{ij} = \\dfrac{\\mathrm{Cov}(X_i, X_j)}{\\sqrt{\\mathrm{Var}(X_i)\\,\\mathrm{Var}(X_j)}}.\\)\nA livello di campione, si sostituiscono media e deviazione standard con i rispettivi stimatori empirici.\n\n\n\nCoefficiente di correlazione lineare\nData una coppia di variabili aleatorie \\(X\\) e \\(Y\\), il coefficiente di correlazione lineare \\(\\rho(X,Y)\\) è\n\\(\\rho(X,Y) = \\dfrac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(Y)}} \\in [-1, 1]\\)\nValori positivi alti (\\(\\rho \\approx 0.8\\)) indicano una forte correlazione positiva; valori negativi (\\(\\rho &lt; 0\\)) indicano correlazione negativa. Se \\(\\rho = -1\\) o \\(\\rho = 1\\), esiste una relazione lineare esatta tra \\(X\\) e \\(Y\\).\n\n\nVerso la PCA: Proprietà avanzate della matrice di covarianza\nSia \\(X\\) un vettore aleatorio di dimensione mm con matrice di covarianza \\(\\Sigma = C(X)\\).\n\nVarianza di una somma. La varianza di \\(X_1 + \\dots + X_m\\) è \\(\\mathrm{Var}(X_1 + \\dots + X_m) = \\mathbf{1}^T\\,\\Sigma\\,\\mathbf{1}\\) dove \\(1\\) è il vettore di tutti \\(1\\) in \\(\\mathbb{R}^m\\). Se \\(X_i\\) sono indipendenti, i termini di covarianza sono nulli e la varianza si riduce alla somma delle varianze individuali. In generale, per un vettore \\(e\\), la varianza di \\(e^T X\\) è \\(e^T \\Sigma\\, e\\).\nVarianza di una combinazione lineare. Data una combinazione \\(a^T X\\), con Le trasformazioni lineari rivestono un ruolo fondamentale nell’analisi dei vettori aleatori, soprattutto in ambito di machine learning e statistica multivariata.\n\nNel caso univariato, una funzione lineare da \\(\\mathbb{R}\\) a \\(\\mathbb{R}\\) assume la forma di una retta\n\\(y = \\alpha + \\beta x\\) ,\ndove \\(\\alpha\\) e \\(\\beta\\) sono parametri scalari e la trasformazione modifica sia la distribuzione teorica di \\(x\\) sia quella empirica nel campione osservato.\n\nNel caso multivariato, se \\(\\vec{x}\\) è un vettore aleatorio di dimensione \\(m\\), una trasformazione lineare verso uno spazio di dimensione \\(k\\) può essere scritta come\n\\(\\,\\vec{y} = \\alpha + B \\vec{x}\\,\\),\ndove \\(\\alpha \\in \\mathbb{R}^k\\) rappresenta il termine di bias (o spostamento) e \\(B \\in M_{k,m}\\) è la matrice che definisce la trasformazione lineare. \\(a \\in \\mathbb{R}^m\\), si ha\n\\(\\mathrm{Var}(a^T X) = a^T\\,\\Sigma\\,a\\)\nLa matrice di covarianza \\(\\Sigma\\) è sempre simmetrica e definita non negativa, quindi possiede \\(m\\) autovalori reali non negativi.",
    "crumbs": [
      "Theory",
      "Vettori aleatori"
    ]
  },
  {
    "objectID": "th/16_Test_del_chi-quadro.html",
    "href": "th/16_Test_del_chi-quadro.html",
    "title": "Test del Chi-quadro",
    "section": "",
    "text": "Il test del Chi-quadro è una famiglia di test statistici utilizzati per valutare se i dati osservati sono compatibili con una distribuzione teorica prefissata. L’idea di fondo è confrontare ciò che si osserva nel campione con ciò che ci si aspetterebbe se un certo modello probabilistico fosse vero, misurando la discrepanza tra osservato e atteso.\n\n\nSi consideri un campione \\(X_1,\\dots,X_n\\) di variabili aleatorie indipendenti, generate da una distribuzione incognita \\(\\varphi\\), che può assumere \\(K\\) valori distinti. L’obiettivo è verificare se \\(\\varphi\\) coincide con una distribuzione teorica nota \\(\\varphi_0\\).\nL’ipotesi nulla afferma che i dati seguano esattamente la distribuzione teorica, \\[\nH_0:\\ \\varphi = \\varphi_0,\n\\] mentre l’ipotesi alternativa sostiene che la distribuzione reale sia diversa, \\[\nH_1:\\ \\varphi \\neq \\varphi_0.\n\\]\nL’idea operativa del test è la seguente. Per ciascuno dei \\(K\\) possibili valori, si conta quante osservazioni del campione assumono quel valore: questo numero è la frequenza osservata \\(O_j\\) (e dipende solo dai dati).\nIn parallelo, assumendo vera l’ipotesi nulla \\(H_0\\), si calcola quante osservazioni ci si aspetterebbe in quella stessa categoria sulla base della distribuzione teorica. Se sotto \\(H_0\\) la probabilità della \\(j\\)-esima categoria è \\(p_j\\), allora la frequenza attesa è \\[\nA_j = n \\cdot p_j,\n\\] dove \\(n\\) è la numerosità campionaria.\nIl confronto tra \\(O_j\\) e \\(A_j\\) è il cuore del test. Se \\(H_0\\) è compatibile con i dati, le frequenze osservate dovrebbero essere vicine a quelle attese, salvo fluttuazioni casuali. Se invece il modello teorico non descrive bene i dati, emergono differenze sistematiche tra osservato e atteso in una o più categorie.\n\n\n\nLa statistica del test, detta statistica di Pearson, è definita come \\[\nW = \\sum_{j=1}^K \\frac{(O_j - A_j)^2}{A_j}.\n\\] Questa quantità cresce quanto più le frequenze osservate si discostano da quelle teoriche, pesando lo scarto in modo relativo alla frequenza attesa.\nSotto l’ipotesi nulla e assumendo che il campione sia sufficientemente grande (in modo che le frequenze attese non siano troppo piccole), la statistica \\(W\\) ha approssimativamente una distribuzione \\(\\chi^2\\) con \\(K-1\\) gradi di libertà. Questo risultato permette di confrontare il valore osservato di \\(W\\) con i quantili della distribuzione \\(\\chi^2\\) e decidere se rigettare o meno \\(H_0\\).\n\n\nUna variante più moderna della statistica di Pearson è la statistica \\(G\\), spesso detta likelihood ratio statistic. Essa nasce dal confronto tra la verosimiglianza del modello sotto l’ipotesi nulla e quella del modello saturato, cioè perfettamente aderente ai dati osservati. La sua espressione è \\[\nG = 2 \\sum_{j=1}^K O_j \\log \\frac{O_j}{A_j}.\n\\] Dal punto di vista interpretativo, anche \\(G\\) misura quanto le frequenze osservate si discostano da quelle attese, ma lo fa in termini di informazione e verosimiglianza anziché tramite scarti quadratici. Sotto l’ipotesi nulla e in regime asintotico, la statistica \\(G\\) ha approssimativamente una distribuzione \\(\\chi^2\\) con \\(K-1\\) gradi di libertà, risultando quindi equivalente al test di Pearson dal punto di vista decisionale per campioni grandi.\n\n\n\n\nQuando la distribuzione da testare è continua, come nel caso della normale o dell’esponenziale, oppure quando il numero di valori distinti è molto grande, il test del Chi-quadro non si applica direttamente alle singole osservazioni. In questi casi si suddivide lo spazio dei valori possibili in \\(K\\) intervalli, detti bin, e si riconduce il problema a un caso discreto.\nPer ciascun bin \\(B_j\\), la frequenza osservata è \\[\nO_j = \\#\\{X_i \\in B_j\\},\n\\] mentre la frequenza attesa sotto l’ipotesi nulla è \\[\nA_j = n \\cdot P_{H_0}(X \\in B_j),\n\\] dove la probabilità è calcolata usando la distribuzione teorica ipotizzata. Il test procede poi esattamente come nel caso discreto, calcolando una statistica di tipo \\(\\chi^2\\) (di Pearson o \\(G\\)).\nUna scelta cruciale riguarda la costruzione dei bin. In pratica è consigliabile definirli in modo che le frequenze attese risultino il più possibile omogenee, idealmente equiprobabili sotto \\(H_0\\), così da rispettare più facilmente le condizioni di applicabilità e migliorare la qualità dell’approssimazione.\n\n\n\nIn molti casi non si dispone di una distribuzione teorica completamente specificata, ma si vuole verificare se i dati provengono da una certa famiglia parametrica, come ad esempio la famiglia delle distribuzioni normali, lasciando però ignoti i parametri. La situazione tipica è il test di gaussianità, in cui si ipotizza \\[\nX \\sim \\mathcal{N}(\\mu,\\sigma^2),\n\\] ma i parametri \\(\\mu\\) e \\(\\sigma^2\\) non sono noti a priori.\nLa procedura segue la stessa logica del test di adattamento visto prima, con una differenza fondamentale: i parametri della distribuzione teorica vengono stimati a partire dal campione. In pratica si calcolano le stime campionarie \\(\\hat{\\mu}\\) e \\(\\hat{\\sigma}\\) e si costruiscono le frequenze attese usando la distribuzione \\(\\mathcal{N}(\\hat{\\mu},\\hat{\\sigma}^2)\\). Su queste frequenze si calcola poi la statistica di Pearson \\(W\\) oppure la statistica \\(G\\).\nUn aspetto cruciale riguarda i gradi di libertà. Ogni parametro stimato dai dati riduce l’informazione disponibile per il test. Di conseguenza, se il campione è suddiviso in \\(K\\) classi e si stimano \\(p\\) parametri, la statistica del test segue approssimativamente una distribuzione \\[\n\\chi^2(K - 1 - p).\n\\] Nel caso della normale, in cui si stimano due parametri (\\(\\mu\\) e \\(\\sigma\\)), i gradi di libertà diventano \\(K - 3\\).\n\n\n\nIl test del Chi-quadro trova un’applicazione molto importante nello studio dell’indipendenza tra due variabili categoriali. In questo contesto i dati vengono organizzati in una tabella di contingenza, in cui ogni cella contiene la frequenza osservata \\(O_{ij}\\) corrispondente alla combinazione della \\(i\\)-esima modalità della prima variabile con la \\(j\\)-esima modalità della seconda.\nL’ipotesi nulla afferma che le due variabili siano indipendenti, mentre l’ipotesi alternativa sostiene l’esistenza di un’associazione tra di esse. Sotto l’ipotesi di indipendenza, la frequenza attesa nella cella \\((i,j)\\) si ottiene combinando i totali marginali di riga e di colonna: \\[\nA_{ij} = \\frac{(\\text{totale riga } i)\\,(\\text{totale colonna } j)}{N},\n\\] dove \\(N\\) è la numerosità totale del campione.\nLa discrepanza tra frequenze osservate e attese può essere misurata tramite la statistica di Pearson \\[\nW = \\sum_{i=1}^m \\sum_{j=1}^n \\frac{(O_{ij} - A_{ij})^2}{A_{ij}},\n\\] oppure tramite la statistica \\(G\\) \\[\nG = 2 \\sum_{i=1}^m \\sum_{j=1}^n O_{ij} \\log \\frac{O_{ij}}{A_{ij}}.\n\\] In entrambi i casi, sotto l’ipotesi nulla e per campioni sufficientemente grandi, la statistica segue approssimativamente una distribuzione \\(\\chi^2\\) con \\((m-1)(n-1)\\) gradi di libertà, dove \\(m\\) e \\(n\\) sono rispettivamente il numero di righe e di colonne della tabella.\n\n\n\nQuando le frequenze attese risultano troppo piccole, l’approssimazione asintotica alla distribuzione \\(\\chi^2\\) non è più affidabile. In queste situazioni è necessario ricorrere a versioni esatte del test, che non si basano su limiti asintotici ma sulla distribuzione reale della statistica sotto l’ipotesi nulla.\nUn primo approccio consiste nella simulazione Monte Carlo. Si generano molti campioni artificiali assumendo vera \\(H_0\\), si calcola per ciascuno la statistica del test (ad esempio \\(W\\) o \\(G\\)) e si costruisce così una distribuzione empirica della statistica sotto l’ipotesi nulla. Il p-value viene stimato come la proporzione di simulazioni in cui la statistica simulata è almeno grande quanto quella osservata.\nUn secondo approccio, concettualmente più diretto ma computazionalmente più oneroso, è l’enumerazione esaustiva. In questo caso si considerano tutte le possibili configurazioni di conteggi compatibili con i vincoli imposti (ad esempio i totali di riga e colonna) e si calcola esattamente la distribuzione della statistica del test. Questo metodo è praticabile solo per problemi di dimensioni molto ridotte, poiché il numero di configurazioni cresce rapidamente.\n\n\nUn caso particolarmente importante di test esatto è il test di Fisher–Irwin, applicabile alle tabelle di contingenza \\(2 \\times 2\\) con totali fissati. In questo contesto, sotto l’ipotesi nulla di indipendenza tra le due variabili, l’intera tabella è determinata dal valore di una sola cella, ad esempio \\(O_{11}\\).\nLa distribuzione esatta di \\(O_{11}\\) sotto \\(H_0\\) è una distribuzione ipergeometrica. Indicando con \\(a\\) il totale della prima riga, con \\(c\\) e \\(d\\) i totali delle due colonne e con \\(n\\) il totale complessivo, vale \\[\nP(O_{11} = k) = \\frac{\\binom{c}{k}\\,\\binom{d}{a - k}}{\\binom{n}{a}}.\n\\] Questa formula fornisce la probabilità esatta di osservare il valore \\(k\\) nella cella \\((1,1)\\), condizionatamente ai totali marginali.\nIl calcolo del p-value dipende dalla forma della distribuzione. Se la distribuzione è simmetrica, il p-value bilaterale si ottiene raddoppiando la probabilità di osservare un valore almeno così estremo nella coda considerata, \\[\np = 2 \\cdot P(O_{11} \\le k_{\\text{oss}}).\n\\] Nel caso generale, in cui la distribuzione non è simmetrica, il p-value si calcola sommando le probabilità di tutti gli esiti che risultano almeno tanto improbabili quanto quello osservato, secondo la distribuzione ipergeometrica sotto \\(H_0\\).",
    "crumbs": [
      "Theory",
      "Test del Chi-quadro"
    ]
  },
  {
    "objectID": "th/16_Test_del_chi-quadro.html#test-del-chi-quadro-elementare",
    "href": "th/16_Test_del_chi-quadro.html#test-del-chi-quadro-elementare",
    "title": "Test del Chi-quadro",
    "section": "",
    "text": "Si consideri un campione \\(X_1,\\dots,X_n\\) di variabili aleatorie indipendenti, generate da una distribuzione incognita \\(\\varphi\\), che può assumere \\(K\\) valori distinti. L’obiettivo è verificare se \\(\\varphi\\) coincide con una distribuzione teorica nota \\(\\varphi_0\\).\nL’ipotesi nulla afferma che i dati seguano esattamente la distribuzione teorica, \\[\nH_0:\\ \\varphi = \\varphi_0,\n\\] mentre l’ipotesi alternativa sostiene che la distribuzione reale sia diversa, \\[\nH_1:\\ \\varphi \\neq \\varphi_0.\n\\]\nL’idea operativa del test è la seguente. Per ciascuno dei \\(K\\) possibili valori, si conta quante osservazioni del campione assumono quel valore: questo numero è la frequenza osservata \\(O_j\\) (e dipende solo dai dati).\nIn parallelo, assumendo vera l’ipotesi nulla \\(H_0\\), si calcola quante osservazioni ci si aspetterebbe in quella stessa categoria sulla base della distribuzione teorica. Se sotto \\(H_0\\) la probabilità della \\(j\\)-esima categoria è \\(p_j\\), allora la frequenza attesa è \\[\nA_j = n \\cdot p_j,\n\\] dove \\(n\\) è la numerosità campionaria.\nIl confronto tra \\(O_j\\) e \\(A_j\\) è il cuore del test. Se \\(H_0\\) è compatibile con i dati, le frequenze osservate dovrebbero essere vicine a quelle attese, salvo fluttuazioni casuali. Se invece il modello teorico non descrive bene i dati, emergono differenze sistematiche tra osservato e atteso in una o più categorie.",
    "crumbs": [
      "Theory",
      "Test del Chi-quadro"
    ]
  },
  {
    "objectID": "th/16_Test_del_chi-quadro.html#statistica-di-pearson",
    "href": "th/16_Test_del_chi-quadro.html#statistica-di-pearson",
    "title": "Test del Chi-quadro",
    "section": "",
    "text": "La statistica del test, detta statistica di Pearson, è definita come \\[\nW = \\sum_{j=1}^K \\frac{(O_j - A_j)^2}{A_j}.\n\\] Questa quantità cresce quanto più le frequenze osservate si discostano da quelle teoriche, pesando lo scarto in modo relativo alla frequenza attesa.\nSotto l’ipotesi nulla e assumendo che il campione sia sufficientemente grande (in modo che le frequenze attese non siano troppo piccole), la statistica \\(W\\) ha approssimativamente una distribuzione \\(\\chi^2\\) con \\(K-1\\) gradi di libertà. Questo risultato permette di confrontare il valore osservato di \\(W\\) con i quantili della distribuzione \\(\\chi^2\\) e decidere se rigettare o meno \\(H_0\\).\n\n\nUna variante più moderna della statistica di Pearson è la statistica \\(G\\), spesso detta likelihood ratio statistic. Essa nasce dal confronto tra la verosimiglianza del modello sotto l’ipotesi nulla e quella del modello saturato, cioè perfettamente aderente ai dati osservati. La sua espressione è \\[\nG = 2 \\sum_{j=1}^K O_j \\log \\frac{O_j}{A_j}.\n\\] Dal punto di vista interpretativo, anche \\(G\\) misura quanto le frequenze osservate si discostano da quelle attese, ma lo fa in termini di informazione e verosimiglianza anziché tramite scarti quadratici. Sotto l’ipotesi nulla e in regime asintotico, la statistica \\(G\\) ha approssimativamente una distribuzione \\(\\chi^2\\) con \\(K-1\\) gradi di libertà, risultando quindi equivalente al test di Pearson dal punto di vista decisionale per campioni grandi.",
    "crumbs": [
      "Theory",
      "Test del Chi-quadro"
    ]
  },
  {
    "objectID": "th/16_Test_del_chi-quadro.html#estensione-a-distribuzioni-con-molti-valori-o-continue",
    "href": "th/16_Test_del_chi-quadro.html#estensione-a-distribuzioni-con-molti-valori-o-continue",
    "title": "Test del Chi-quadro",
    "section": "",
    "text": "Quando la distribuzione da testare è continua, come nel caso della normale o dell’esponenziale, oppure quando il numero di valori distinti è molto grande, il test del Chi-quadro non si applica direttamente alle singole osservazioni. In questi casi si suddivide lo spazio dei valori possibili in \\(K\\) intervalli, detti bin, e si riconduce il problema a un caso discreto.\nPer ciascun bin \\(B_j\\), la frequenza osservata è \\[\nO_j = \\#\\{X_i \\in B_j\\},\n\\] mentre la frequenza attesa sotto l’ipotesi nulla è \\[\nA_j = n \\cdot P_{H_0}(X \\in B_j),\n\\] dove la probabilità è calcolata usando la distribuzione teorica ipotizzata. Il test procede poi esattamente come nel caso discreto, calcolando una statistica di tipo \\(\\chi^2\\) (di Pearson o \\(G\\)).\nUna scelta cruciale riguarda la costruzione dei bin. In pratica è consigliabile definirli in modo che le frequenze attese risultino il più possibile omogenee, idealmente equiprobabili sotto \\(H_0\\), così da rispettare più facilmente le condizioni di applicabilità e migliorare la qualità dell’approssimazione.",
    "crumbs": [
      "Theory",
      "Test del Chi-quadro"
    ]
  },
  {
    "objectID": "th/16_Test_del_chi-quadro.html#estensione-a-famiglie-parametriche",
    "href": "th/16_Test_del_chi-quadro.html#estensione-a-famiglie-parametriche",
    "title": "Test del Chi-quadro",
    "section": "",
    "text": "In molti casi non si dispone di una distribuzione teorica completamente specificata, ma si vuole verificare se i dati provengono da una certa famiglia parametrica, come ad esempio la famiglia delle distribuzioni normali, lasciando però ignoti i parametri. La situazione tipica è il test di gaussianità, in cui si ipotizza \\[\nX \\sim \\mathcal{N}(\\mu,\\sigma^2),\n\\] ma i parametri \\(\\mu\\) e \\(\\sigma^2\\) non sono noti a priori.\nLa procedura segue la stessa logica del test di adattamento visto prima, con una differenza fondamentale: i parametri della distribuzione teorica vengono stimati a partire dal campione. In pratica si calcolano le stime campionarie \\(\\hat{\\mu}\\) e \\(\\hat{\\sigma}\\) e si costruiscono le frequenze attese usando la distribuzione \\(\\mathcal{N}(\\hat{\\mu},\\hat{\\sigma}^2)\\). Su queste frequenze si calcola poi la statistica di Pearson \\(W\\) oppure la statistica \\(G\\).\nUn aspetto cruciale riguarda i gradi di libertà. Ogni parametro stimato dai dati riduce l’informazione disponibile per il test. Di conseguenza, se il campione è suddiviso in \\(K\\) classi e si stimano \\(p\\) parametri, la statistica del test segue approssimativamente una distribuzione \\[\n\\chi^2(K - 1 - p).\n\\] Nel caso della normale, in cui si stimano due parametri (\\(\\mu\\) e \\(\\sigma\\)), i gradi di libertà diventano \\(K - 3\\).",
    "crumbs": [
      "Theory",
      "Test del Chi-quadro"
    ]
  },
  {
    "objectID": "th/16_Test_del_chi-quadro.html#test-su-tabelle-di-contingenza",
    "href": "th/16_Test_del_chi-quadro.html#test-su-tabelle-di-contingenza",
    "title": "Test del Chi-quadro",
    "section": "",
    "text": "Il test del Chi-quadro trova un’applicazione molto importante nello studio dell’indipendenza tra due variabili categoriali. In questo contesto i dati vengono organizzati in una tabella di contingenza, in cui ogni cella contiene la frequenza osservata \\(O_{ij}\\) corrispondente alla combinazione della \\(i\\)-esima modalità della prima variabile con la \\(j\\)-esima modalità della seconda.\nL’ipotesi nulla afferma che le due variabili siano indipendenti, mentre l’ipotesi alternativa sostiene l’esistenza di un’associazione tra di esse. Sotto l’ipotesi di indipendenza, la frequenza attesa nella cella \\((i,j)\\) si ottiene combinando i totali marginali di riga e di colonna: \\[\nA_{ij} = \\frac{(\\text{totale riga } i)\\,(\\text{totale colonna } j)}{N},\n\\] dove \\(N\\) è la numerosità totale del campione.\nLa discrepanza tra frequenze osservate e attese può essere misurata tramite la statistica di Pearson \\[\nW = \\sum_{i=1}^m \\sum_{j=1}^n \\frac{(O_{ij} - A_{ij})^2}{A_{ij}},\n\\] oppure tramite la statistica \\(G\\) \\[\nG = 2 \\sum_{i=1}^m \\sum_{j=1}^n O_{ij} \\log \\frac{O_{ij}}{A_{ij}}.\n\\] In entrambi i casi, sotto l’ipotesi nulla e per campioni sufficientemente grandi, la statistica segue approssimativamente una distribuzione \\(\\chi^2\\) con \\((m-1)(n-1)\\) gradi di libertà, dove \\(m\\) e \\(n\\) sono rispettivamente il numero di righe e di colonne della tabella.",
    "crumbs": [
      "Theory",
      "Test del Chi-quadro"
    ]
  },
  {
    "objectID": "th/16_Test_del_chi-quadro.html#versioni-esatte-del-test",
    "href": "th/16_Test_del_chi-quadro.html#versioni-esatte-del-test",
    "title": "Test del Chi-quadro",
    "section": "",
    "text": "Quando le frequenze attese risultano troppo piccole, l’approssimazione asintotica alla distribuzione \\(\\chi^2\\) non è più affidabile. In queste situazioni è necessario ricorrere a versioni esatte del test, che non si basano su limiti asintotici ma sulla distribuzione reale della statistica sotto l’ipotesi nulla.\nUn primo approccio consiste nella simulazione Monte Carlo. Si generano molti campioni artificiali assumendo vera \\(H_0\\), si calcola per ciascuno la statistica del test (ad esempio \\(W\\) o \\(G\\)) e si costruisce così una distribuzione empirica della statistica sotto l’ipotesi nulla. Il p-value viene stimato come la proporzione di simulazioni in cui la statistica simulata è almeno grande quanto quella osservata.\nUn secondo approccio, concettualmente più diretto ma computazionalmente più oneroso, è l’enumerazione esaustiva. In questo caso si considerano tutte le possibili configurazioni di conteggi compatibili con i vincoli imposti (ad esempio i totali di riga e colonna) e si calcola esattamente la distribuzione della statistica del test. Questo metodo è praticabile solo per problemi di dimensioni molto ridotte, poiché il numero di configurazioni cresce rapidamente.\n\n\nUn caso particolarmente importante di test esatto è il test di Fisher–Irwin, applicabile alle tabelle di contingenza \\(2 \\times 2\\) con totali fissati. In questo contesto, sotto l’ipotesi nulla di indipendenza tra le due variabili, l’intera tabella è determinata dal valore di una sola cella, ad esempio \\(O_{11}\\).\nLa distribuzione esatta di \\(O_{11}\\) sotto \\(H_0\\) è una distribuzione ipergeometrica. Indicando con \\(a\\) il totale della prima riga, con \\(c\\) e \\(d\\) i totali delle due colonne e con \\(n\\) il totale complessivo, vale \\[\nP(O_{11} = k) = \\frac{\\binom{c}{k}\\,\\binom{d}{a - k}}{\\binom{n}{a}}.\n\\] Questa formula fornisce la probabilità esatta di osservare il valore \\(k\\) nella cella \\((1,1)\\), condizionatamente ai totali marginali.\nIl calcolo del p-value dipende dalla forma della distribuzione. Se la distribuzione è simmetrica, il p-value bilaterale si ottiene raddoppiando la probabilità di osservare un valore almeno così estremo nella coda considerata, \\[\np = 2 \\cdot P(O_{11} \\le k_{\\text{oss}}).\n\\] Nel caso generale, in cui la distribuzione non è simmetrica, il p-value si calcola sommando le probabilità di tutti gli esiti che risultano almeno tanto improbabili quanto quello osservato, secondo la distribuzione ipergeometrica sotto \\(H_0\\).",
    "crumbs": [
      "Theory",
      "Test del Chi-quadro"
    ]
  },
  {
    "objectID": "th/05. Stima parametrica.html",
    "href": "th/05. Stima parametrica.html",
    "title": "Stima parametrica",
    "section": "",
    "text": "Consideriamo un campione aleatorio \\(X_1, \\dots, X_n\\) estratto da una distribuzione \\(F_\\theta\\), la quale dipende da un vettore di parametri incogniti \\(\\theta\\) (ad esempio, una distribuzione di Poisson in cui il valore di \\(\\lambda\\) non è noto).\nA differenza del calcolo delle probabilità, dove spesso si assume che le distribuzioni siano note, in statistica l’obiettivo principale è fare inferenza, cioè ottenere informazioni sui parametri sconosciuti a partire dai dati osservati.\n\n\nUna qualunque statistica il cui scopo sia fornire una stima di un parametro \\(\\theta\\) si chiama stimatore di \\(\\theta\\). Gli stimatori sono quindi variabili aleatorie. Il valore numerico assunto da uno stimatore in seguito all’osservazione di un campione è detto stima.\nSiano \\(X_1, \\dots, X_n\\) variabili aleatorie con distribuzione congiunta nota, eccetto per un parametro incognito \\(\\theta\\). Il problema è stimare \\(\\theta\\) a partire dai valori osservati di queste variabili.\nUna classe particolarmente utile di stimatori è quella degli stimatori di massima verosimiglianza. Indichiamo con \\(f(x_1, \\dots, x_n | \\theta)\\) la funzione di massa (se le variabili sono discrete) o di densità (se continue) congiunta di \\(X_1, \\dots, X_n\\).\nSe interpretiamo \\(f(x_1, \\dots, x_n | \\theta)\\) come la verosimiglianza (cioè quanto è “plausibile” osservare i dati \\(x_1, \\dots, x_n\\) dato un certo valore di \\(\\theta\\)), allora sembra ragionevole scegliere come stima di \\(\\theta\\) quel valore che rende massima questa verosimiglianza. In altre parole, lo stimatore di massima verosimiglianza \\(\\hat{\\theta}\\) è il valore di \\(\\theta\\) che massimizza \\(f(x_1, \\dots, x_n | \\theta)\\), considerando fissati i dati osservati \\(x_1, \\dots, x_n\\).\nNota. La funzione \\(f(x_1, \\dots, x_n | \\theta)\\), vista in funzione di \\(\\theta\\) con i dati fissati, è chiamata funzione di likelihood.\nPer semplificare i calcoli, si sfrutta spesso il fatto che massimizzare \\(f(x_1, \\dots, x_n | \\theta)\\) è equivalente a massimizzare il suo logaritmo: le due funzioni hanno lo stesso punto di massimo. Quindi possiamo trovare \\(\\hat{\\theta}\\) anche massimizzando:\n\\[\\log\\left[f(x_1, \\dots, x_n | \\theta)\\right]\\]\nNota. Questa è detta funzione di log-likelihood.\n\n\n\nSia \\(X_1, \\dots, X_n\\) un campione estratto da una popolazione normalmente distribuita, con media incognita \\(\\mu\\) e varianza nota \\(\\sigma^2\\). Lo stimatore di massima verosimiglianza per \\(\\mu\\) è la media campionaria:\n\\[\\bar{X} := \\frac{1}{n} \\sum_i X_i\\]\nQuesto non significa che \\(\\bar{X}\\) coinciderà esattamente con \\(\\mu\\), ma che sarà “vicina” con alta probabilità. Per questo motivo, spesso si preferisce dare un intervallo di valori plausibili per \\(\\mu\\) anziché una sola stima puntuale. Questo intervallo è detto intervallo di confidenza.\nPer costruirlo, si sfrutta la distribuzione dello stimatore puntuale. In particolare:\nSe \\(\\sigma\\) è nota, si usa la distribuzione normale standard. L’intervallo di confidenza al 95% è dato da:\n\\[IC = \\bar{x} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\ndove \\(z\\) è il valore critico corrispondente al livello di confidenza desiderato. Per un livello del 95%, si ha \\(z = 1{,}96\\), perché:\n\\[P(-1{,}96 \\leq Z \\leq 1{,}96) = 0{,}95\\]\ncioè il 95% dell’area sotto la curva normale standard è compreso tra \\(-1{,}96\\) e \\(+1{,}96\\).\nSe invece \\(\\sigma\\) non è nota, si utilizza la deviazione standard campionaria \\(s\\) come stima di \\(\\sigma\\), e la distribuzione \\(t\\) di Student con \\(n - 1\\) gradi di libertà. In questo caso:\n\\[IC = \\bar{x} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\]\ndove \\(t\\) è il valore critico corrispondente al livello di confidenza e ai gradi di libertà.\n\n\n\nCalcolo della media campionaria\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nCalcolo dell’errore standard (SE)\n\nSe \\(\\sigma\\) è noto: \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\n\nSe \\(\\sigma\\) non è noto: \\[SE = \\frac{s}{\\sqrt{n}}\\]\n\nDeterminazione del valore critico\n\nPer livello di confidenza del 95%, \\(z = 1{,}96\\)\n\nCalcolo dei limiti dell’intervallo di confidenza\n\\[IC = (\\bar{x} - z \\cdot SE,\\ \\bar{x} + z \\cdot SE)\\]\n\nPer capire meglio il perché del valore \\(z = 1{,}96\\): la distribuzione normale standard ha media \\(0\\) e deviazione standard \\(1\\). Il livello di confidenza del 95% implica che vogliamo catturare il 95% dell’area sotto la curva, centrato attorno allo 0. Questo lascia il 5% fuori, diviso tra le due code: il 2,5% a sinistra e il 2,5% a destra.\nCercando nella tavola della normale, troviamo che:\n\\[P(Z \\leq 1{,}96) = 0{,}975\\]\nIl che implica che tra \\(-1{,}96\\) e \\(+1{,}96\\) è contenuto esattamente il 95% della distribuzione.",
    "crumbs": [
      "Theory",
      "Stima parametrica"
    ]
  },
  {
    "objectID": "th/05. Stima parametrica.html#stimatori-di-massima-verosimiglianza",
    "href": "th/05. Stima parametrica.html#stimatori-di-massima-verosimiglianza",
    "title": "Stima parametrica",
    "section": "",
    "text": "Una qualunque statistica il cui scopo sia fornire una stima di un parametro \\(\\theta\\) si chiama stimatore di \\(\\theta\\). Gli stimatori sono quindi variabili aleatorie. Il valore numerico assunto da uno stimatore in seguito all’osservazione di un campione è detto stima.\nSiano \\(X_1, \\dots, X_n\\) variabili aleatorie con distribuzione congiunta nota, eccetto per un parametro incognito \\(\\theta\\). Il problema è stimare \\(\\theta\\) a partire dai valori osservati di queste variabili.\nUna classe particolarmente utile di stimatori è quella degli stimatori di massima verosimiglianza. Indichiamo con \\(f(x_1, \\dots, x_n | \\theta)\\) la funzione di massa (se le variabili sono discrete) o di densità (se continue) congiunta di \\(X_1, \\dots, X_n\\).\nSe interpretiamo \\(f(x_1, \\dots, x_n | \\theta)\\) come la verosimiglianza (cioè quanto è “plausibile” osservare i dati \\(x_1, \\dots, x_n\\) dato un certo valore di \\(\\theta\\)), allora sembra ragionevole scegliere come stima di \\(\\theta\\) quel valore che rende massima questa verosimiglianza. In altre parole, lo stimatore di massima verosimiglianza \\(\\hat{\\theta}\\) è il valore di \\(\\theta\\) che massimizza \\(f(x_1, \\dots, x_n | \\theta)\\), considerando fissati i dati osservati \\(x_1, \\dots, x_n\\).\nNota. La funzione \\(f(x_1, \\dots, x_n | \\theta)\\), vista in funzione di \\(\\theta\\) con i dati fissati, è chiamata funzione di likelihood.\nPer semplificare i calcoli, si sfrutta spesso il fatto che massimizzare \\(f(x_1, \\dots, x_n | \\theta)\\) è equivalente a massimizzare il suo logaritmo: le due funzioni hanno lo stesso punto di massimo. Quindi possiamo trovare \\(\\hat{\\theta}\\) anche massimizzando:\n\\[\\log\\left[f(x_1, \\dots, x_n | \\theta)\\right]\\]\nNota. Questa è detta funzione di log-likelihood.",
    "crumbs": [
      "Theory",
      "Stima parametrica"
    ]
  },
  {
    "objectID": "th/05. Stima parametrica.html#intervalli-di-confidenza",
    "href": "th/05. Stima parametrica.html#intervalli-di-confidenza",
    "title": "Stima parametrica",
    "section": "",
    "text": "Sia \\(X_1, \\dots, X_n\\) un campione estratto da una popolazione normalmente distribuita, con media incognita \\(\\mu\\) e varianza nota \\(\\sigma^2\\). Lo stimatore di massima verosimiglianza per \\(\\mu\\) è la media campionaria:\n\\[\\bar{X} := \\frac{1}{n} \\sum_i X_i\\]\nQuesto non significa che \\(\\bar{X}\\) coinciderà esattamente con \\(\\mu\\), ma che sarà “vicina” con alta probabilità. Per questo motivo, spesso si preferisce dare un intervallo di valori plausibili per \\(\\mu\\) anziché una sola stima puntuale. Questo intervallo è detto intervallo di confidenza.\nPer costruirlo, si sfrutta la distribuzione dello stimatore puntuale. In particolare:\nSe \\(\\sigma\\) è nota, si usa la distribuzione normale standard. L’intervallo di confidenza al 95% è dato da:\n\\[IC = \\bar{x} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\ndove \\(z\\) è il valore critico corrispondente al livello di confidenza desiderato. Per un livello del 95%, si ha \\(z = 1{,}96\\), perché:\n\\[P(-1{,}96 \\leq Z \\leq 1{,}96) = 0{,}95\\]\ncioè il 95% dell’area sotto la curva normale standard è compreso tra \\(-1{,}96\\) e \\(+1{,}96\\).\nSe invece \\(\\sigma\\) non è nota, si utilizza la deviazione standard campionaria \\(s\\) come stima di \\(\\sigma\\), e la distribuzione \\(t\\) di Student con \\(n - 1\\) gradi di libertà. In questo caso:\n\\[IC = \\bar{x} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\]\ndove \\(t\\) è il valore critico corrispondente al livello di confidenza e ai gradi di libertà.\n\n\n\nCalcolo della media campionaria\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nCalcolo dell’errore standard (SE)\n\nSe \\(\\sigma\\) è noto: \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\n\nSe \\(\\sigma\\) non è noto: \\[SE = \\frac{s}{\\sqrt{n}}\\]\n\nDeterminazione del valore critico\n\nPer livello di confidenza del 95%, \\(z = 1{,}96\\)\n\nCalcolo dei limiti dell’intervallo di confidenza\n\\[IC = (\\bar{x} - z \\cdot SE,\\ \\bar{x} + z \\cdot SE)\\]\n\nPer capire meglio il perché del valore \\(z = 1{,}96\\): la distribuzione normale standard ha media \\(0\\) e deviazione standard \\(1\\). Il livello di confidenza del 95% implica che vogliamo catturare il 95% dell’area sotto la curva, centrato attorno allo 0. Questo lascia il 5% fuori, diviso tra le due code: il 2,5% a sinistra e il 2,5% a destra.\nCercando nella tavola della normale, troviamo che:\n\\[P(Z \\leq 1{,}96) = 0{,}975\\]\nIl che implica che tra \\(-1{,}96\\) e \\(+1{,}96\\) è contenuto esattamente il 95% della distribuzione.",
    "crumbs": [
      "Theory",
      "Stima parametrica"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html",
    "href": "th/10. Ancora sul teorema di Cochran.html",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Si consideri una sequenza di \\(n\\) osservazioni indicizzate da \\(t = 1,2,\\dots,n\\). Per ciascun istante \\(t\\) sono dati una variabile esplicativa \\(x(t)\\), assunta deterministica, e una variabile risposta osservata \\(Y(t)\\), che viene modellata come variabile aleatoria.\nSi assume che le osservazioni seguano un modello gaussiano con varianza costante: \\[\nY(t) \\sim \\mathcal{N}(\\mu(t), \\sigma^2),\n\\] dove \\(\\sigma^2\\) è una varianza incognita ma comune a tutte le osservazioni, mentre la media \\(\\mu(t)\\) dipende in modo deterministico dal covariato \\(x(t)\\), cioè \\(\\mu(t) = \\mu(x(t))\\).\nRaccogliendo le medie in un vettore colonna \\(\\mu \\in \\mathbb{R}^n\\), il modello può essere riscritto in forma vettoriale come \\[\n\\mu =\n\\begin{pmatrix}\n\\mu(1) \\\\\n\\mu(2) \\\\\n\\vdots \\\\\n\\mu(n)\n\\end{pmatrix}\n= C\\beta.\n\\] Qui \\(C \\in \\mathbb{R}^{n \\times k}\\) è la design matrix (o matrice di regressione), completamente nota, le cui colonne rappresentano funzioni fissate delle variabili esplicative \\(x(t)\\), mentre \\(\\beta \\in \\mathbb{R}^k\\) è il vettore dei parametri incogniti del modello.\nDal punto di vista geometrico, questa formulazione implica che il vettore delle medie \\(\\mu\\) appartiene al sottospazio vettoriale di \\(\\mathbb{R}^n\\) generato dalle colonne di \\(C\\), che ha dimensione \\(k\\). È proprio questa struttura lineare che consente di applicare il teorema di Cochran: la stima di \\(\\mu\\) può essere vista come una proiezione ortogonale del vettore osservato \\(Y\\) su tale sottospazio, mentre la parte residua misura la variabilità non spiegata dal modello.\nIl teorema di Cochran garantisce che, sotto le ipotesi di normalità e omoschedasticità, la componente spiegata dal modello e quella residua sono indipendenti e che la somma dei quadrati dei residui, opportunamente normalizzata, segue una distribuzione chi-quadro con un numero di gradi di libertà pari a \\(n-k\\). Questo risultato è alla base dell’inferenza statistica nei modelli di regressione lineare.\n\n\nIl vettore delle medie \\(\\mu\\) rappresenta la parte deterministica del modello e, per costruzione, è vincolato a appartenere allo spazio generato dalle colonne della matrice di regressione \\(C\\). In termini geometrici, ciò significa che \\[\n\\mu \\in \\mathrm{Im}(C) \\subset \\mathbb{R}^n,\n\\] dove \\(\\mathrm{Im}(C)\\) è un sottospazio vettoriale di dimensione al più \\(k\\).\nL’idea fondamentale della stima nei modelli lineari è di approssimare il vettore dei dati osservati \\(Y\\) con un vettore che appartenga a questo sottospazio. La miglior approssimazione, nel senso dei minimi quadrati, si ottiene risolvendo il problema \\[\nw(\\beta) = \\|Y - C\\beta\\|^2,\n\\] che misura la distanza quadratica tra i dati osservati e quelli predetti dal modello lineare.\nLa soluzione di questo problema è unica (assumendo \\(C^\\top C\\) invertibile) ed è data dalla ben nota formula \\[\n\\hat{\\beta} = (C^\\top C)^{-1} C^\\top Y.\n\\] Geometricamente, questo equivale a proiettare ortogonalmente il vettore \\(Y\\) sul sottospazio \\(\\mathrm{Im}(C)\\). Il vettore \\[\n\\hat{\\mu} = C\\hat{\\beta}\n\\] rappresenta quindi la stima della componente deterministica del modello, cioè la parte di \\(Y\\) spiegata dalla regressione.\nLa differenza tra i dati osservati e la loro proiezione sul sottospazio del modello è il residuo \\[\nY - C\\hat{\\beta},\n\\] la cui norma al quadrato \\[\nSSR = \\|Y - C\\hat{\\beta}\\|^2\n\\] misura la variabilità non spiegata dal modello.\nSotto le ipotesi di normalità e varianza costante, il teorema di Cochran garantisce che questa quantità, una volta normalizzata per la varianza \\(\\sigma^2\\), segue una distribuzione chi-quadro: \\[\n\\frac{SSR}{\\sigma^2} \\sim \\chi^2(n - k),\n\\] dove \\(n-k\\) è il numero di gradi di libertà residui, corrispondente alla dimensione dello spazio ortogonale a \\(\\mathrm{Im}(C)\\).\n\n\n\nPer completezza, si può verificare la forma di \\(\\hat{\\beta}\\) calcolando esplicitamente il gradiente della funzione da minimizzare:\n\\[\nw(\\beta) = \\sum_{i=1}^n \\left( Y_i - \\sum_{j=1}^k C_{ij} \\beta_j \\right)^2\n\\]\nCalcoliamo la derivata rispetto a ogni parametro \\(\\beta_k\\):\n\\[\n\\frac{\\partial w}{\\partial \\beta_k} = -2 \\sum_{i=1}^n \\left( Y_i - \\sum_{j=1}^k C_{ij} \\beta_j \\right) C_{ik}\n\\]\nPonendo le derivate uguali a zero, si ottiene il sistema normale:\n\\[\nC^\\top C \\, \\hat{\\beta} = C^\\top Y\n\\]\nche fornisce:\n\\[\n\\hat{\\beta} = (C^\\top C)^{-1} C^\\top Y\n\\]\ncome già anticipato.\n\n\n\nQuando si stima la varianza di un campione per standardizzare una statistica, si ottiene una distribuzione di tipo t.\nSiano:\n\\[\nZ \\sim \\mathcal{N}(0,1), \\quad W \\sim \\chi^2(k), \\quad Z \\perp W\n\\]\nAllora:\n\\[\n\\frac{Z}{\\sqrt{W/k}} \\sim t(k)\n\\]\n\n\nNel caso del campione Gaussiano con media \\(\\mu\\), abbiamo:\n\\[\n\\bar{Y} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\nStandardizzando:\n\\[\n\\frac{\\bar{Y} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0,1)\n\\]\nSe però \\(\\sigma\\) è incognita e viene sostituita con lo stimatore empirico \\(S_X\\), si ottiene:\n\\[\n\\frac{\\bar{Y} - \\mu}{S_X / \\sqrt{n}} \\sim t(n - 1)\n\\]\nQuesta è la base per costruire un intervallo di confidenza sulla media quando la varianza è ignota.\n\n\n\n\nSupponiamo ora che le osservazioni \\(Y_i\\) siano spiegate da una variabile esplicativa \\(x_i\\):\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\ncon \\(x_i\\) noti e fissati. Questo è il modello di regressione lineare semplice.\nIn forma matriciale:\n\\[\nY = C\\beta + \\varepsilon\n\\]\ncon:\n\\[\nC = \\begin{pmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}, \\quad\n\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n\\]\nLo stimatore ai minimi quadrati è ancora:\n\\[\n\\hat{\\beta} = (C^\\top C)^{-1} C^\\top Y\n\\]\nLa stima della varianza degli errori è:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\n\\]\ndove \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\).\n\n\n\nPer valutare se la variabile \\(x\\) ha un effetto significativo su \\(Y\\), si considera il test:\n\\[\nH_0: \\beta_1 = 0 \\quad \\text{vs} \\quad H_1: \\beta_1 \\neq 0\n\\]\nSotto l’ipotesi nulla, la statistica del test è:\n\\[\nT = \\frac{\\hat{\\beta}_1}{S_e \\cdot \\sqrt{\\operatorname{Var}(\\hat{\\beta}_1)}} \\sim t(n - 2)\n\\]\ndove la varianza dello stimatore \\(\\hat{\\beta}_1\\) è:\n\\[\n\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}\n\\]\nIn pratica, \\(\\sigma^2\\) viene sostituita da \\(S_e^2\\), calcolata come sopra.\n\n\n\n\n\nIl modello lineare multiplo generalizza la regressione semplice consentendo di includere più variabili esplicative. Si scrive nella forma:\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p + \\varepsilon\n\\]\ndove \\(Y\\) è la variabile risposta, \\(x_1, \\ldots, x_p\\) sono le variabili esplicative (note), \\(\\beta_0, \\ldots, \\beta_p\\) sono i parametri incogniti e \\(\\varepsilon\\) è il termine di errore, che si assume distribuito come:\n\\[\n\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\ncioè con media nulla e varianza costante \\(\\sigma^2\\) (omoscedasticità), e indipendente dai regressori.\nPer ogni osservazione \\(i = 1, \\ldots, n\\), il modello osservazionale diventa:\n\\[\nY_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}, \\sigma^2)\n\\]\nDi norma, si pone \\(x_{i0} = 1\\) per modellare l’intercetta come una variabile costante.\n\n\n\nLa forma compatta del modello è particolarmente utile per il calcolo e la teoria. Definiamo:\n\n\\(Y \\in \\mathbb{R}^n\\): vettore delle osservazioni \\(Y_1, \\ldots, Y_n\\)\n\\(X \\in \\mathbb{R}^{n \\times (p+1)}\\): matrice delle variabili esplicative, con la prima colonna formata da 1 (per l’intercetta)\n\\(\\beta \\in \\mathbb{R}^{p+1}\\): vettore dei parametri\n\\(\\varepsilon \\in \\mathbb{R}^n\\): vettore degli errori\n\nIl modello diventa:\n\\[\nY = X\\beta + \\varepsilon\n\\]\ne, sotto le ipotesi del modello, \\(Y\\) ha distribuzione:\n\\[\nY \\sim \\mathcal{N}(X\\beta, \\sigma^2 I)\n\\]\n\n\n\nLa stima di \\(\\beta\\) avviene tramite il metodo dei minimi quadrati, che minimizza la somma dei quadrati dei residui:\n\\[\n\\hat{\\beta} = \\arg\\min_\\beta \\|Y - X\\beta\\|^2\n\\]\nLa soluzione è esplicita e data da:\n\\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y\n\\]\nSotto le ipotesi del modello lineare, si può dimostrare che:\n\\[\n\\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta, \\sigma^2 (X^\\top X)^{-1}\\right)\n\\]\nPer stimare la varianza \\(\\sigma^2\\), si utilizza la somma dei quadrati residui:\n\\[\nSSR = \\|Y - X\\hat{\\beta}\\|^2\n\\]\nLo stimatore della varianza è allora:\n\\[\nS_e^2 = \\frac{SSR}{n - p - 1}\n\\]\ne si ha che:\n\\[\n\\frac{SSR}{\\sigma^2} \\sim \\chi^2(n - p - 1)\n\\]\nInoltre, \\(S_e^2\\) è indipendente da \\(\\hat{\\beta}\\): è una proprietà fondamentale che discende dal teorema di Cochran.\n\n\n\n\nPer ogni coefficiente \\(\\beta_j\\) (con \\(j = 1, \\ldots, p\\)), si può verificare se il corrispondente predittore ha un contributo significativo alla spiegazione della variabile risposta. Si considerano le ipotesi:\n\\[\nH_0: \\beta_j = 0 \\quad \\text{(nessun effetto)} \\\\\nH_1: \\beta_j \\neq 0 \\quad \\text{(effetto significativo)}\n\\]\nPoiché \\(\\hat{\\beta}_j\\) è una variabile normale sotto \\(H_0\\):\n\\[\n\\hat{\\beta}_j \\sim \\mathcal{N}\\left(0, \\sigma^2 [(X^\\top X)^{-1}]_{jj} \\right)\n\\]\nStandardizzando, si ottiene la statistica del test:\n\\[\nT_j = \\frac{\\hat{\\beta}_j}{\\sqrt{\\sigma^2 [(X^\\top X)^{-1}]_{jj}}} \\sim \\mathcal{N}(0,1)\n\\]\nPoiché \\(\\sigma^2\\) è incognita, si sostituisce con \\(S_e^2\\), ottenendo una statistica t:\n\\[\nT_j := \\frac{\\hat{\\beta}_j}{S_e \\cdot \\sqrt{[(X^\\top X)^{-1}]_{jj}}} \\sim t(n - p - 1)\n\\]\nQuesta statistica è nota anche come coefficiente standardizzato o t-statistica.\n\n\n\nPer decidere se rifiutare \\(H_0\\), si può:\n\nconfrontare il valore assoluto di \\(T_j\\) con un valore critico \\(q\\) tale che:\n\n\\[\n\\text{Regione di accettazione} = [-q, +q], \\quad q = F^{-1}_{t(n - p - 1)}\\left(1 - \\frac{\\alpha}{2}\\right)\n\\]\n\noppure calcolare il p-value associato alla statistica osservata:\n\n\\[\np^* = 2 \\cdot \\left[1 - F_{t(n - p - 1)}(|T_j|)\\right]\n\\]\nDove \\(F_t\\) è la funzione di distribuzione cumulativa della \\(t\\) di Student.\nLa soglia \\(\\alpha\\) rappresenta il livello di significatività prefissato (es. 0.05).\n\n\nIn base al valore del p-value, possiamo trarre conclusioni qualitative:\n\n\\(p^* \\geq 30\\%\\): il predittore \\(x_j\\) non è utile → si può escludere dal modello.\n\\(p^* \\leq 0.4\\%\\): forte evidenza contro \\(H_0\\) → \\(x_j\\) è altamente significativo.\n\\(0.4\\% &lt; p^* &lt; 30\\%\\): situazione intermedia, da valutare con cautela.\n\n\n\n\n\nQuando si eseguono più test simultaneamente, il rischio di ottenere falsi positivi aumenta. Anche se tutti gli \\(H_0\\) fossero veri, c’è comunque una probabilità crescente di ottenere almeno un risultato “significativo” per puro caso.\nFormalmente:\n\\[\nP(\\text{almeno un errore di I specie} \\mid H_0 \\text{ tutte vere}) \\leq \\sum_{i=1}^n \\alpha_i\n\\]\nSe si mantiene un livello \\(\\bar{\\alpha}\\) costante per ogni test, si ottiene:\n\\[\nP(\\text{errore complessivo}) \\leq n \\bar{\\alpha}\n\\]\nQuesto fenomeno è noto come inflazione dell’errore di I specie.\n\n\n\nPer contenere l’errore complessivo entro un valore fissato \\(\\bar{\\alpha}\\) (tipicamente 5%), si può correggere il livello di ciascun test dividendo per il numero totale di confronti \\(n\\):\n\\[\n\\alpha_i = \\frac{\\bar{\\alpha}}{n}, \\quad \\text{per ogni } i\n\\]\nQuesta è la correzione di Bonferroni, una tecnica semplice ma conservativa.\n\n\n\nIl livello di errore di I specie è mantenuto sotto controllo\nTuttavia, si riduce la potenza dei test, ovvero aumenta la probabilità di non rilevare effetti reali (errore di II specie)\n\n\n\n\n\nPer evitare conclusioni errate:\n\nè consigliabile pianificare in anticipo il numero e il tipo di test statistici,\ndefinire le ipotesi da verificare prima di esaminare i dati,\nevitare selezioni post-hoc (data dredging, cherry-picking) che portano a interpretazioni spurie.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#distribuzione-t-di-student",
    "href": "th/10. Ancora sul teorema di Cochran.html#distribuzione-t-di-student",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Quando si stima la varianza di un campione per standardizzare una statistica, si ottiene una distribuzione di tipo t.\nSiano:\n\\[\nZ \\sim \\mathcal{N}(0,1), \\quad W \\sim \\chi^2(k), \\quad Z \\perp W\n\\]\nAllora:\n\\[\n\\frac{Z}{\\sqrt{W/k}} \\sim t(k)\n\\]\n\n\nNel caso del campione Gaussiano con media \\(\\mu\\), abbiamo:\n\\[\n\\bar{Y} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\nStandardizzando:\n\\[\n\\frac{\\bar{Y} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0,1)\n\\]\nSe però \\(\\sigma\\) è incognita e viene sostituita con lo stimatore empirico \\(S_X\\), si ottiene:\n\\[\n\\frac{\\bar{Y} - \\mu}{S_X / \\sqrt{n}} \\sim t(n - 1)\n\\]\nQuesta è la base per costruire un intervallo di confidenza sulla media quando la varianza è ignota.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#estensione-regressione-lineare-semplice",
    "href": "th/10. Ancora sul teorema di Cochran.html#estensione-regressione-lineare-semplice",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Supponiamo ora che le osservazioni \\(Y_i\\) siano spiegate da una variabile esplicativa \\(x_i\\):\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\ncon \\(x_i\\) noti e fissati. Questo è il modello di regressione lineare semplice.\nIn forma matriciale:\n\\[\nY = C\\beta + \\varepsilon\n\\]\ncon:\n\\[\nC = \\begin{pmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}, \\quad\n\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n\\]\nLo stimatore ai minimi quadrati è ancora:\n\\[\n\\hat{\\beta} = (C^\\top C)^{-1} C^\\top Y\n\\]\nLa stima della varianza degli errori è:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\n\\]\ndove \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\).",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#test-di-significatività-su-beta_1",
    "href": "th/10. Ancora sul teorema di Cochran.html#test-di-significatività-su-beta_1",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Per valutare se la variabile \\(x\\) ha un effetto significativo su \\(Y\\), si considera il test:\n\\[\nH_0: \\beta_1 = 0 \\quad \\text{vs} \\quad H_1: \\beta_1 \\neq 0\n\\]\nSotto l’ipotesi nulla, la statistica del test è:\n\\[\nT = \\frac{\\hat{\\beta}_1}{S_e \\cdot \\sqrt{\\operatorname{Var}(\\hat{\\beta}_1)}} \\sim t(n - 2)\n\\]\ndove la varianza dello stimatore \\(\\hat{\\beta}_1\\) è:\n\\[\n\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}\n\\]\nIn pratica, \\(\\sigma^2\\) viene sostituita da \\(S_e^2\\), calcolata come sopra.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#regressione-lineare-multipla",
    "href": "th/10. Ancora sul teorema di Cochran.html#regressione-lineare-multipla",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Il modello lineare multiplo generalizza la regressione semplice consentendo di includere più variabili esplicative. Si scrive nella forma:\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p + \\varepsilon\n\\]\ndove \\(Y\\) è la variabile risposta, \\(x_1, \\ldots, x_p\\) sono le variabili esplicative (note), \\(\\beta_0, \\ldots, \\beta_p\\) sono i parametri incogniti e \\(\\varepsilon\\) è il termine di errore, che si assume distribuito come:\n\\[\n\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\ncioè con media nulla e varianza costante \\(\\sigma^2\\) (omoscedasticità), e indipendente dai regressori.\nPer ogni osservazione \\(i = 1, \\ldots, n\\), il modello osservazionale diventa:\n\\[\nY_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}, \\sigma^2)\n\\]\nDi norma, si pone \\(x_{i0} = 1\\) per modellare l’intercetta come una variabile costante.\n\n\n\nLa forma compatta del modello è particolarmente utile per il calcolo e la teoria. Definiamo:\n\n\\(Y \\in \\mathbb{R}^n\\): vettore delle osservazioni \\(Y_1, \\ldots, Y_n\\)\n\\(X \\in \\mathbb{R}^{n \\times (p+1)}\\): matrice delle variabili esplicative, con la prima colonna formata da 1 (per l’intercetta)\n\\(\\beta \\in \\mathbb{R}^{p+1}\\): vettore dei parametri\n\\(\\varepsilon \\in \\mathbb{R}^n\\): vettore degli errori\n\nIl modello diventa:\n\\[\nY = X\\beta + \\varepsilon\n\\]\ne, sotto le ipotesi del modello, \\(Y\\) ha distribuzione:\n\\[\nY \\sim \\mathcal{N}(X\\beta, \\sigma^2 I)\n\\]\n\n\n\nLa stima di \\(\\beta\\) avviene tramite il metodo dei minimi quadrati, che minimizza la somma dei quadrati dei residui:\n\\[\n\\hat{\\beta} = \\arg\\min_\\beta \\|Y - X\\beta\\|^2\n\\]\nLa soluzione è esplicita e data da:\n\\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y\n\\]\nSotto le ipotesi del modello lineare, si può dimostrare che:\n\\[\n\\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta, \\sigma^2 (X^\\top X)^{-1}\\right)\n\\]\nPer stimare la varianza \\(\\sigma^2\\), si utilizza la somma dei quadrati residui:\n\\[\nSSR = \\|Y - X\\hat{\\beta}\\|^2\n\\]\nLo stimatore della varianza è allora:\n\\[\nS_e^2 = \\frac{SSR}{n - p - 1}\n\\]\ne si ha che:\n\\[\n\\frac{SSR}{\\sigma^2} \\sim \\chi^2(n - p - 1)\n\\]\nInoltre, \\(S_e^2\\) è indipendente da \\(\\hat{\\beta}\\): è una proprietà fondamentale che discende dal teorema di Cochran.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#test-di-significatività-per-i-coefficienti",
    "href": "th/10. Ancora sul teorema di Cochran.html#test-di-significatività-per-i-coefficienti",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Per ogni coefficiente \\(\\beta_j\\) (con \\(j = 1, \\ldots, p\\)), si può verificare se il corrispondente predittore ha un contributo significativo alla spiegazione della variabile risposta. Si considerano le ipotesi:\n\\[\nH_0: \\beta_j = 0 \\quad \\text{(nessun effetto)} \\\\\nH_1: \\beta_j \\neq 0 \\quad \\text{(effetto significativo)}\n\\]\nPoiché \\(\\hat{\\beta}_j\\) è una variabile normale sotto \\(H_0\\):\n\\[\n\\hat{\\beta}_j \\sim \\mathcal{N}\\left(0, \\sigma^2 [(X^\\top X)^{-1}]_{jj} \\right)\n\\]\nStandardizzando, si ottiene la statistica del test:\n\\[\nT_j = \\frac{\\hat{\\beta}_j}{\\sqrt{\\sigma^2 [(X^\\top X)^{-1}]_{jj}}} \\sim \\mathcal{N}(0,1)\n\\]\nPoiché \\(\\sigma^2\\) è incognita, si sostituisce con \\(S_e^2\\), ottenendo una statistica t:\n\\[\nT_j := \\frac{\\hat{\\beta}_j}{S_e \\cdot \\sqrt{[(X^\\top X)^{-1}]_{jj}}} \\sim t(n - p - 1)\n\\]\nQuesta statistica è nota anche come coefficiente standardizzato o t-statistica.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#decisione-e-interpretazione-del-p-value",
    "href": "th/10. Ancora sul teorema di Cochran.html#decisione-e-interpretazione-del-p-value",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Per decidere se rifiutare \\(H_0\\), si può:\n\nconfrontare il valore assoluto di \\(T_j\\) con un valore critico \\(q\\) tale che:\n\n\\[\n\\text{Regione di accettazione} = [-q, +q], \\quad q = F^{-1}_{t(n - p - 1)}\\left(1 - \\frac{\\alpha}{2}\\right)\n\\]\n\noppure calcolare il p-value associato alla statistica osservata:\n\n\\[\np^* = 2 \\cdot \\left[1 - F_{t(n - p - 1)}(|T_j|)\\right]\n\\]\nDove \\(F_t\\) è la funzione di distribuzione cumulativa della \\(t\\) di Student.\nLa soglia \\(\\alpha\\) rappresenta il livello di significatività prefissato (es. 0.05).\n\n\nIn base al valore del p-value, possiamo trarre conclusioni qualitative:\n\n\\(p^* \\geq 30\\%\\): il predittore \\(x_j\\) non è utile → si può escludere dal modello.\n\\(p^* \\leq 0.4\\%\\): forte evidenza contro \\(H_0\\) → \\(x_j\\) è altamente significativo.\n\\(0.4\\% &lt; p^* &lt; 30\\%\\): situazione intermedia, da valutare con cautela.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#problema-dei-test-multipli",
    "href": "th/10. Ancora sul teorema di Cochran.html#problema-dei-test-multipli",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Quando si eseguono più test simultaneamente, il rischio di ottenere falsi positivi aumenta. Anche se tutti gli \\(H_0\\) fossero veri, c’è comunque una probabilità crescente di ottenere almeno un risultato “significativo” per puro caso.\nFormalmente:\n\\[\nP(\\text{almeno un errore di I specie} \\mid H_0 \\text{ tutte vere}) \\leq \\sum_{i=1}^n \\alpha_i\n\\]\nSe si mantiene un livello \\(\\bar{\\alpha}\\) costante per ogni test, si ottiene:\n\\[\nP(\\text{errore complessivo}) \\leq n \\bar{\\alpha}\n\\]\nQuesto fenomeno è noto come inflazione dell’errore di I specie.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#correzione-di-bonferroni",
    "href": "th/10. Ancora sul teorema di Cochran.html#correzione-di-bonferroni",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Per contenere l’errore complessivo entro un valore fissato \\(\\bar{\\alpha}\\) (tipicamente 5%), si può correggere il livello di ciascun test dividendo per il numero totale di confronti \\(n\\):\n\\[\n\\alpha_i = \\frac{\\bar{\\alpha}}{n}, \\quad \\text{per ogni } i\n\\]\nQuesta è la correzione di Bonferroni, una tecnica semplice ma conservativa.\n\n\n\nIl livello di errore di I specie è mantenuto sotto controllo\nTuttavia, si riduce la potenza dei test, ovvero aumenta la probabilità di non rilevare effetti reali (errore di II specie)",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/10. Ancora sul teorema di Cochran.html#buone-pratiche-sperimentali",
    "href": "th/10. Ancora sul teorema di Cochran.html#buone-pratiche-sperimentali",
    "title": "Ancora sul teorema di Cochran",
    "section": "",
    "text": "Per evitare conclusioni errate:\n\nè consigliabile pianificare in anticipo il numero e il tipo di test statistici,\ndefinire le ipotesi da verificare prima di esaminare i dati,\nevitare selezioni post-hoc (data dredging, cherry-picking) che portano a interpretazioni spurie.",
    "crumbs": [
      "Theory",
      "Ancora sul teorema di Cochran"
    ]
  },
  {
    "objectID": "th/01. Richiami di probabilità.html",
    "href": "th/01. Richiami di probabilità.html",
    "title": "Richiami di Probabilità",
    "section": "",
    "text": "Consideriamo un esperimento il cui esito è casuale. L’insieme di tutti i possibili esiti si chiama spazio degli esiti e si indica con \\(S\\). I sottoinsiemi di \\(S\\) sono chiamati eventi e si denotano tipicamente con lettere maiuscole come \\(E\\) o \\(F\\). Se l’esito dell’esperimento appartiene a \\(E\\), si dice che l’evento \\(E\\) si è verificato.\nSiano \\(E\\) e \\(F\\) due eventi:\n\nL’unione di \\(E\\) e \\(F\\), indicata con \\(E \\cup F\\), è l’insieme degli esiti che appartengono a \\(E\\) oppure a \\(F\\) (o a entrambi). L’evento \\(E \\cup F\\) si verifica se almeno uno tra \\(E\\) e \\(F\\) si verifica.\nL’intersezione di \\(E\\) e \\(F\\), indicata con \\(E \\cap F\\), è l’insieme degli esiti che appartengono sia a \\(E\\) che a \\(F\\). L’evento \\(E \\cap F\\) si verifica se entrambi gli eventi \\(E\\) e \\(F\\) si verificano.\n\nNota: Se \\(E \\cap F = \\emptyset\\), allora i due eventi non possono verificarsi contemporaneamente e si dicono mutuamente esclusivi o incompatibili.\nNota: Se tutti gli esiti di \\(E\\) appartengono anche a \\(F\\), si dice che \\(E\\) è contenuto in \\(F\\) e si indica con \\(E \\subseteq F\\).\n\n\nA ogni evento \\(E\\) nello spazio degli esiti \\(S\\) è associato un numero, indicato con \\(P(E)\\), chiamato probabilità di \\(E\\), che rappresenta la “plausibilità” che \\(E\\) si verifichi.\nGli assiomi fondamentali della probabilità sono:\n\nNon negatività: \\(0 \\leq P(E) \\leq 1\\) per ogni evento \\(E\\).\nNormalizzazione: \\(P(S) = 1\\), cioè l’evento certo ha probabilità 1.\nAdditività: Se \\(E_1, E_2, \\dots, E_n\\) sono eventi mutuamente esclusivi (cioè \\(E_i \\cap E_j = \\emptyset\\) per \\(i \\neq j\\)), allora: \\[\nP\\left( \\bigcup_{i=1}^n E_i \\right) = \\sum_{i=1}^n P(E_i).\n\\]\n\nNota: Per ogni evento \\(E \\subseteq S\\), l’evento complementare \\(E^c\\) (l’insieme degli esiti che non appartengono a \\(E\\)) ha probabilità \\(P(E^c) = 1 - P(E)\\).\nPer due eventi qualsiasi \\(E\\) e \\(F\\), vale la formula dell’unione: \\[\nP(E \\cup F) = P(E) + P(F) - P(E \\cap F).\n\\]\n\n\n\nSe ogni esito di \\(S\\) ha la stessa probabilità di verificarsi, lo spazio degli esiti si dice equiprobabile. In questo caso, la probabilità di un evento \\(E\\) è data dal rapporto tra il numero di esiti favorevoli (gli esiti in \\(E\\)) e il numero totale di esiti possibili: \\[\nP(E) = \\dfrac{|E|}{|S|},\n\\] dove \\(|E|\\) e \\(|S|\\) indicano la cardinalità (il numero di elementi) degli insiemi \\(E\\) e \\(S\\).\nPrincipio di Enumerazione: Se due esperimenti \\(A\\) e \\(B\\) hanno rispettivamente \\(m\\) e \\(n\\) esiti possibili, allora il numero totale di esiti possibili quando si eseguono contemporaneamente i due esperimenti è \\(m \\times n\\). Questo principio si estende a più esperimenti.\nUna permutazione è un modo di ordinare un insieme di oggetti. Il numero totale di permutazioni di \\(n\\) oggetti distinti è \\(n!\\) (fattoriale di \\(n\\)).\n\n\nIl coefficiente binomiale \\(\\binom{n}{k}\\) rappresenta il numero di modi in cui si possono scegliere \\(k\\) elementi da un insieme di \\(n\\) elementi, indipendentemente dall’ordine (combinazioni). È calcolato con la formula: \\[\n\\binom{n}{k} = \\dfrac{n!}{k! (n - k)!}.\n\\]\n\n\n\n\nLa probabilità condizionata di un evento \\(E\\) dato che un altro evento \\(F\\) si è verificato, indicata con \\(P(E \\mid F)\\), è definita come: \\[\nP(E \\mid F) = \\dfrac{P(E \\cap F)}{P(F)},\n\\] purché \\(P(F) &gt; 0\\). Questa misura rappresenta la probabilità che si verifichi \\(E\\) sapendo che \\(F\\) è accaduto.\n\n\n\nSiano \\(E\\) e \\(F\\) due eventi con \\(P(F) &gt; 0\\). Possiamo scomporre \\(E\\) in: \\[\nE = (E \\cap F) \\cup (E \\cap F^c).\n\\] Poiché \\((E \\cap F)\\) e \\((E \\cap F^c)\\) sono eventi mutuamente esclusivi, si ha: \\[\nP(E) = P(E \\cap F) + P(E \\cap F^c).\n\\] Utilizzando la definizione di probabilità condizionata, otteniamo il teorema delle probabilità totali: \\[\nP(E) = P(E \\mid F) \\cdot P(F) + P(E \\mid F^c) \\cdot P(F^c).\n\\]\nIl teorema di Bayes consente di invertire le probabilità condizionate: \\[\nP(F \\mid E) = \\dfrac{P(E \\mid F) \\cdot P(F)}{P(E)},\n\\] dove \\(P(E)\\) è calcolato come sopra.\n\n\n\nDue eventi \\(E\\) e \\(F\\) si dicono indipendenti se il verificarsi di uno non influenza la probabilità dell’altro. Formalmente, \\(E\\) e \\(F\\) sono indipendenti se: \\[\nP(E \\cap F) = P(E) \\cdot P(F).\n\\] In questo caso, la probabilità condizionata di \\(E\\) dato \\(F\\) è uguale a \\(P(E)\\): \\[\nP(E \\mid F) = P(E).\n\\] Se questa condizione non è soddisfatta, gli eventi si dicono dipendenti.",
    "crumbs": [
      "Theory",
      "Richiami di Probabilità"
    ]
  },
  {
    "objectID": "th/01. Richiami di probabilità.html#assiomi-della-probabilità",
    "href": "th/01. Richiami di probabilità.html#assiomi-della-probabilità",
    "title": "Richiami di Probabilità",
    "section": "",
    "text": "A ogni evento \\(E\\) nello spazio degli esiti \\(S\\) è associato un numero, indicato con \\(P(E)\\), chiamato probabilità di \\(E\\), che rappresenta la “plausibilità” che \\(E\\) si verifichi.\nGli assiomi fondamentali della probabilità sono:\n\nNon negatività: \\(0 \\leq P(E) \\leq 1\\) per ogni evento \\(E\\).\nNormalizzazione: \\(P(S) = 1\\), cioè l’evento certo ha probabilità 1.\nAdditività: Se \\(E_1, E_2, \\dots, E_n\\) sono eventi mutuamente esclusivi (cioè \\(E_i \\cap E_j = \\emptyset\\) per \\(i \\neq j\\)), allora: \\[\nP\\left( \\bigcup_{i=1}^n E_i \\right) = \\sum_{i=1}^n P(E_i).\n\\]\n\nNota: Per ogni evento \\(E \\subseteq S\\), l’evento complementare \\(E^c\\) (l’insieme degli esiti che non appartengono a \\(E\\)) ha probabilità \\(P(E^c) = 1 - P(E)\\).\nPer due eventi qualsiasi \\(E\\) e \\(F\\), vale la formula dell’unione: \\[\nP(E \\cup F) = P(E) + P(F) - P(E \\cap F).\n\\]",
    "crumbs": [
      "Theory",
      "Richiami di Probabilità"
    ]
  },
  {
    "objectID": "th/01. Richiami di probabilità.html#spazi-degli-esiti-equiprobabili",
    "href": "th/01. Richiami di probabilità.html#spazi-degli-esiti-equiprobabili",
    "title": "Richiami di Probabilità",
    "section": "",
    "text": "Se ogni esito di \\(S\\) ha la stessa probabilità di verificarsi, lo spazio degli esiti si dice equiprobabile. In questo caso, la probabilità di un evento \\(E\\) è data dal rapporto tra il numero di esiti favorevoli (gli esiti in \\(E\\)) e il numero totale di esiti possibili: \\[\nP(E) = \\dfrac{|E|}{|S|},\n\\] dove \\(|E|\\) e \\(|S|\\) indicano la cardinalità (il numero di elementi) degli insiemi \\(E\\) e \\(S\\).\nPrincipio di Enumerazione: Se due esperimenti \\(A\\) e \\(B\\) hanno rispettivamente \\(m\\) e \\(n\\) esiti possibili, allora il numero totale di esiti possibili quando si eseguono contemporaneamente i due esperimenti è \\(m \\times n\\). Questo principio si estende a più esperimenti.\nUna permutazione è un modo di ordinare un insieme di oggetti. Il numero totale di permutazioni di \\(n\\) oggetti distinti è \\(n!\\) (fattoriale di \\(n\\)).\n\n\nIl coefficiente binomiale \\(\\binom{n}{k}\\) rappresenta il numero di modi in cui si possono scegliere \\(k\\) elementi da un insieme di \\(n\\) elementi, indipendentemente dall’ordine (combinazioni). È calcolato con la formula: \\[\n\\binom{n}{k} = \\dfrac{n!}{k! (n - k)!}.\n\\]",
    "crumbs": [
      "Theory",
      "Richiami di Probabilità"
    ]
  },
  {
    "objectID": "th/01. Richiami di probabilità.html#probabilità-condizionata",
    "href": "th/01. Richiami di probabilità.html#probabilità-condizionata",
    "title": "Richiami di Probabilità",
    "section": "",
    "text": "La probabilità condizionata di un evento \\(E\\) dato che un altro evento \\(F\\) si è verificato, indicata con \\(P(E \\mid F)\\), è definita come: \\[\nP(E \\mid F) = \\dfrac{P(E \\cap F)}{P(F)},\n\\] purché \\(P(F) &gt; 0\\). Questa misura rappresenta la probabilità che si verifichi \\(E\\) sapendo che \\(F\\) è accaduto.",
    "crumbs": [
      "Theory",
      "Richiami di Probabilità"
    ]
  },
  {
    "objectID": "th/01. Richiami di probabilità.html#fattorizzazione-di-un-evento-e-teorema-di-bayes",
    "href": "th/01. Richiami di probabilità.html#fattorizzazione-di-un-evento-e-teorema-di-bayes",
    "title": "Richiami di Probabilità",
    "section": "",
    "text": "Siano \\(E\\) e \\(F\\) due eventi con \\(P(F) &gt; 0\\). Possiamo scomporre \\(E\\) in: \\[\nE = (E \\cap F) \\cup (E \\cap F^c).\n\\] Poiché \\((E \\cap F)\\) e \\((E \\cap F^c)\\) sono eventi mutuamente esclusivi, si ha: \\[\nP(E) = P(E \\cap F) + P(E \\cap F^c).\n\\] Utilizzando la definizione di probabilità condizionata, otteniamo il teorema delle probabilità totali: \\[\nP(E) = P(E \\mid F) \\cdot P(F) + P(E \\mid F^c) \\cdot P(F^c).\n\\]\nIl teorema di Bayes consente di invertire le probabilità condizionate: \\[\nP(F \\mid E) = \\dfrac{P(E \\mid F) \\cdot P(F)}{P(E)},\n\\] dove \\(P(E)\\) è calcolato come sopra.",
    "crumbs": [
      "Theory",
      "Richiami di Probabilità"
    ]
  },
  {
    "objectID": "th/01. Richiami di probabilità.html#eventi-indipendenti",
    "href": "th/01. Richiami di probabilità.html#eventi-indipendenti",
    "title": "Richiami di Probabilità",
    "section": "",
    "text": "Due eventi \\(E\\) e \\(F\\) si dicono indipendenti se il verificarsi di uno non influenza la probabilità dell’altro. Formalmente, \\(E\\) e \\(F\\) sono indipendenti se: \\[\nP(E \\cap F) = P(E) \\cdot P(F).\n\\] In questo caso, la probabilità condizionata di \\(E\\) dato \\(F\\) è uguale a \\(P(E)\\): \\[\nP(E \\mid F) = P(E).\n\\] Se questa condizione non è soddisfatta, gli eventi si dicono dipendenti.",
    "crumbs": [
      "Theory",
      "Richiami di Probabilità"
    ]
  },
  {
    "objectID": "th/08_Analisi_delle_componenti_principali.html",
    "href": "th/08_Analisi_delle_componenti_principali.html",
    "title": "Analisi delle componenti principali",
    "section": "",
    "text": "La PCA (Principal Component Analysis) è una trasformazione lineare che permette di ridurre la dimensionalità dei dati mantenendo quanta più informazione possibile. L’idea è quella di individuare nuove variabili, dette componenti principali, ottenute come combinazioni lineari delle variabili originali, tali da spiegare in modo ottimale la variabilità dei dati. Operativamente, la PCA può essere interpretata come una rotazione del sistema di riferimento che rende la struttura dei dati più semplice.\nSi consideri la matrice di covarianza dei dati \\(X\\), indicata con \\(\\Sigma = C(X)\\). Questa matrice è quadrata, simmetrica e definita non negativa; di conseguenza ammette una decomposizione spettrale. Esistono quindi \\(m\\) autovettori \\(v_j \\in \\mathbb{R}^m\\) e autovalori associati \\(\\lambda_j\\) tali che\n\\[\n\\Sigma v_j = \\lambda_j v_j, \\qquad j = 1,\\dots,m.\n\\]\nGli autovettori associati a una matrice simmetrica sono ortogonali tra loro e possono essere scelti a norma unitaria. In particolare vale\n\\[\nv_j \\cdot v_k =\n\\begin{cases}\n1 & \\text{se } j = k, \\\\\n0 & \\text{se } j \\neq k.\n\\end{cases}\n\\]\nRaccogliendo gli autovettori in una matrice\n\\[\nV = [v_1, v_2, \\dots, v_m],\n\\]\nsi ottiene una matrice ortogonale, per cui\n\\[\nV^T V = I \\qquad \\text{e} \\qquad V^{-1} = V^T.\n\\]\nLa matrice \\(V\\) rappresenta quindi una rotazione nello spazio delle variabili. In termini di autovalori e autovettori, la matrice di covarianza può essere scritta come\n\\[\n\\Sigma = \\sum_{i=1}^m \\lambda_i \\, v_i v_i^T.\n\\]\nLa trasformazione PCA è definita introducendo le nuove variabili\n\\[\nY = V^T X,\n\\]\nche corrispondono alle coordinate di \\(X\\) nel nuovo sistema di riferimento dato dagli autovettori. In questo nuovo spazio la matrice di covarianza risulta diagonalizzata,\n\\[\nC(Y) = V^T \\Sigma V = \\Lambda = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_m),\n\\]\ndove ciascun autovalore \\(\\lambda_j\\) rappresenta la varianza spiegata dalla \\(j\\)-esima componente principale. Le componenti sono quindi scorrelate e ordinate per importanza decrescente in base al valore degli autovalori.\nDal punto di vista operativo, esistono due approcci principali alla PCA. Nel primo approccio i dati vengono prima traslati nell’origine sottraendo la media, poi ruotati tramite la PCA calcolata sulla matrice di covarianza, ed eventualmente standardizzati in un secondo momento. Nel secondo approccio, invece, si effettua prima la traslazione nell’origine e una standardizzazione delle variabili, quindi si applica la PCA sulla matrice di correlazione \\(g(X)\\), seguita da una standardizzazione finale.\nIl secondo approccio è generalmente preferibile quando le variabili originali hanno scale molto diverse, poiché evita che le componenti principali siano dominate dalle variabili con varianza numericamente più grande e conduce a un’analisi più equilibrata.\n\n\nL’analisi fattoriale ha l’obiettivo di descrivere un insieme potenzialmente ampio di variabili osservate tramite un numero ridotto di fattori latenti, interpretabili come dimensioni concettuali sottostanti e tra loro indipendenti. L’idea è che le variabili originali siano manifestazioni osservabili di pochi fattori non direttamente misurabili.\nIn un contesto come quello delle misure corporee, ad esempio, è naturale pensare che molte variabili siano influenzate da caratteristiche comuni. Un fattore potrebbe rappresentare la dimensione globale del corpo, un altro la larghezza, un altro ancora l’età o il grado di sviluppo muscolare, mentre fattori residui possono catturare differenze più specifiche o locali. In questo modo, strutture complesse vengono ricondotte a poche componenti interpretabili.\nQuando il numero di variabili \\(m\\) è elevato, molte delle componenti ottenute tramite PCA presentano una varianza molto bassa. Tali componenti contribuiscono poco alla variabilità complessiva dei dati e possono essere trascurate senza perdita informativa rilevante, riducendo così la dimensionalità effettiva da \\(m\\) a un numero più contenuto \\(k\\).\nI factor loadings, ossia i coefficienti che legano ciascuna variabile osservata ai fattori, descrivono il peso e il segno del contributo di ogni variabile all’interno di un fattore. Valori elevati in modulo indicano che la variabile è fortemente associata a quel fattore, facilitandone l’interpretazione sostantiva.\nDal punto di vista della varianza, la PCA conserva la varianza totale del sistema. In particolare, la somma delle varianze delle variabili originali coincide con la somma delle varianze delle componenti trasformate,\n\\[\n\\mathrm{Var}(x_1) + \\dots + \\mathrm{Var}(x_m)\n=\n\\mathrm{Var}(y_1) + \\dots + \\mathrm{Var}(y_m).\n\\]\nGli autovalori \\(\\lambda_j\\) della matrice di covarianza (o di correlazione) misurano la varianza spiegata da ciascuna componente e sono ordinati in modo decrescente. Uno strumento grafico comunemente utilizzato per decidere quante componenti o fattori trattenere è lo scree plot, che rappresenta gli autovalori in funzione dell’indice della componente e consente di individuare un punto di flesso oltre il quale il contributo informativo delle componenti diventa trascurabile.\n\n\n\nUn vettore aleatorio \\(X = (X_1, \\dots, X_p)\\) segue una distribuzione gaussiana multidimensionale con parametri \\((\\mu, \\Sigma)\\), indicata con \\(N(\\mu, \\Sigma)\\), se la sua distribuzione è completamente determinata da un vettore medio e da una matrice di covarianza. Il vettore medio \\(\\mu \\in \\mathbb{R}^p\\) raccoglie le medie delle singole componenti, mentre la matrice di covarianza \\(\\Sigma \\in \\mathbb{R}^{p \\times p}\\) descrive sia le varianze marginali sia le dipendenze lineari tra le componenti.\nLa funzione di densità di probabilità associata a \\(X\\) è data da\n\\[\nf_X(x) = \\frac{1}{(2\\pi)^{p/2} \\sqrt{\\det \\Sigma}}\n\\exp\\!\\left(\n-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\n\\right),\n\\]\ndove il termine quadratico nell’esponente misura la distanza di \\(x\\) dalla media \\(\\mu\\) tenendo conto della struttura di correlazione imposta da \\(\\Sigma\\). La matrice \\(\\Sigma\\) deve essere simmetrica e definita positiva, in modo da garantire l’esistenza dell’inversa e la normalizzazione della densità.\nSe le componenti di \\(X\\) sono indipendenti, tutte le covarianze incrociate sono nulle e la matrice \\(\\Sigma\\) risulta diagonale. In questo caso la densità multidimensionale si fattorizza nel prodotto delle densità marginali e le curve di livello della distribuzione sono ellissi (o iper-ellissi) allineate con gli assi coordinati.\nQuando invece le componenti non sono indipendenti, la matrice di covarianza contiene termini fuori diagonale. Geometricamente, ciò si traduce in ellissi di livello ruotate e deformate, il cui orientamento e allungamento riflettono le correlazioni e le diverse scale di variabilità delle variabili originali.\n\n\n\nLa legge di Dirichlet è una distribuzione di probabilità definita su vettori di proporzioni, ed è utilizzata quando si vuole modellare l’incertezza su come una quantità totale venga suddivisa tra più categorie. È particolarmente importante in ambito bayesiano, dove compare spesso come distribuzione a priori per vettori di probabilità, ad esempio nei modelli di clustering, nei modelli a miscela e nei topic model.\nFormalmente, la distribuzione di Dirichlet genera vettori aleatori\n\\[\nX = (X_1, \\dots, X_m)\n\\]\ntali che ogni componente sia non negativa e che la somma totale valga uno,\n\\[\nX_i \\ge 0, \\qquad \\sum_{i=1}^m X_i = 1.\n\\]\nSi scrive\n\\[\nX \\sim \\text{Dirichlet}(\\alpha),\n\\]\ndove il parametro \\(\\alpha = (\\alpha_1, \\dots, \\alpha_m)\\) soddisfa \\(\\alpha_i &gt; 0\\) per ogni \\(i\\). I parametri \\(\\alpha_i\\) controllano la forma della distribuzione e riflettono il peso relativo attribuito a ciascuna categoria.\nLa funzione di densità di probabilità è data da\n\\[\nf_X(x) = C_\\alpha \\, x_1^{\\alpha_1 - 1} \\cdots x_m^{\\alpha_m - 1},\n\\]\ndove \\(C_\\alpha\\) è una costante di normalizzazione che dipende dal vettore \\(\\alpha\\) e garantisce che la densità integri a uno sul simplesso \\(\\{x : x_i \\ge 0,\\ \\sum x_i = 1\\}\\).\nLa media della distribuzione di Dirichlet ha una forma particolarmente semplice ed è pari a\n\\[\n\\mathbb{E}(X_i) = \\frac{\\alpha_i}{\\sum_{j=1}^m \\alpha_j},\n\\]\noppure, in forma vettoriale,\n\\[\n\\mathbb{E}(X) = \\frac{\\alpha}{\\sum_{i=1}^m \\alpha_i}.\n\\]\nQuesto mostra che il parametro \\(\\alpha\\) può essere interpretato come un insieme di pseudoconteggi: il rapporto tra ciascun \\(\\alpha_i\\) e la loro somma determina il valore medio delle proporzioni.\nNel caso particolare \\(m = 2\\), la distribuzione di Dirichlet si riduce alla distribuzione Beta, che modella una singola proporzione su \\([0,1]\\). La Dirichlet può quindi essere vista come una generalizzazione multivariata della Beta, adatta a descrivere simultaneamente più probabilità che devono sommare a uno.\n\n\n\nLa distribuzione multinomiale descrive il comportamento dei conteggi associati a più categorie quando si ripete uno stesso esperimento un numero fissato di volte. Può essere vista come una naturale estensione della distribuzione binomiale al caso in cui gli esiti possibili non siano due, ma \\(m \\ge 2\\). Un esempio tipico è il lancio ripetuto di un dado: ogni lancio produce uno tra \\(m=6\\) risultati possibili e, dopo \\(n\\) lanci, si è interessati a contare quante volte compare ciascuna faccia.\nFormalmente, si considera un vettore aleatorio\n\\[\nX = (X_1, \\dots, X_m),\n\\]\ndove \\(X_i\\) rappresenta il numero di volte in cui si osserva l’esito appartenente alla categoria \\(i\\) su un totale di \\(n\\) prove indipendenti. Il modello è parametrizzato da un vettore di probabilità\n\\[\np = (p_1, \\dots, p_m), \\qquad \\sum_{i=1}^m p_i = 1,\n\\]\nche descrive la probabilità di ciascun esito in una singola prova. Si scrive allora\n\\[\nX \\sim \\text{Multinomiale}(n, p).\n\\]\nLa funzione di massa di probabilità è data da\n\\[\nP(X = x)\n=\n\\frac{n!}{x_1! \\cdots x_m!}\n\\, p_1^{x_1} \\cdots p_m^{x_m},\n\\qquad\n\\sum_{i=1}^m x_i = n.\n\\]\nIl coefficiente combinatorio tiene conto di tutti i possibili ordini in cui i diversi esiti possono verificarsi nelle \\(n\\) prove, mentre il prodotto dei termini \\(p_i^{x_i}\\) pesa ciascuna configurazione in base alle probabilità delle singole categorie.\nCiascuna componente \\(X_i\\) ha distribuzione binomiale marginale con parametri \\((n, p_i)\\), ma le componenti non sono indipendenti tra loro. Il vincolo \\(\\sum_i X_i = n\\) implica infatti che un aumento in un conteggio debba essere compensato da una diminuzione in almeno un altro. La distribuzione multinomiale è quindi lo strumento naturale per modellare esperimenti con più di due esiti possibili, quando l’interesse è rivolto alla distribuzione congiunta dei conteggi.",
    "crumbs": [
      "Theory",
      "Analisi delle componenti principali"
    ]
  },
  {
    "objectID": "th/08_Analisi_delle_componenti_principali.html#factor-analysis",
    "href": "th/08_Analisi_delle_componenti_principali.html#factor-analysis",
    "title": "Analisi delle componenti principali",
    "section": "",
    "text": "L’analisi fattoriale ha l’obiettivo di descrivere un insieme potenzialmente ampio di variabili osservate tramite un numero ridotto di fattori latenti, interpretabili come dimensioni concettuali sottostanti e tra loro indipendenti. L’idea è che le variabili originali siano manifestazioni osservabili di pochi fattori non direttamente misurabili.\nIn un contesto come quello delle misure corporee, ad esempio, è naturale pensare che molte variabili siano influenzate da caratteristiche comuni. Un fattore potrebbe rappresentare la dimensione globale del corpo, un altro la larghezza, un altro ancora l’età o il grado di sviluppo muscolare, mentre fattori residui possono catturare differenze più specifiche o locali. In questo modo, strutture complesse vengono ricondotte a poche componenti interpretabili.\nQuando il numero di variabili \\(m\\) è elevato, molte delle componenti ottenute tramite PCA presentano una varianza molto bassa. Tali componenti contribuiscono poco alla variabilità complessiva dei dati e possono essere trascurate senza perdita informativa rilevante, riducendo così la dimensionalità effettiva da \\(m\\) a un numero più contenuto \\(k\\).\nI factor loadings, ossia i coefficienti che legano ciascuna variabile osservata ai fattori, descrivono il peso e il segno del contributo di ogni variabile all’interno di un fattore. Valori elevati in modulo indicano che la variabile è fortemente associata a quel fattore, facilitandone l’interpretazione sostantiva.\nDal punto di vista della varianza, la PCA conserva la varianza totale del sistema. In particolare, la somma delle varianze delle variabili originali coincide con la somma delle varianze delle componenti trasformate,\n\\[\n\\mathrm{Var}(x_1) + \\dots + \\mathrm{Var}(x_m)\n=\n\\mathrm{Var}(y_1) + \\dots + \\mathrm{Var}(y_m).\n\\]\nGli autovalori \\(\\lambda_j\\) della matrice di covarianza (o di correlazione) misurano la varianza spiegata da ciascuna componente e sono ordinati in modo decrescente. Uno strumento grafico comunemente utilizzato per decidere quante componenti o fattori trattenere è lo scree plot, che rappresenta gli autovalori in funzione dell’indice della componente e consente di individuare un punto di flesso oltre il quale il contributo informativo delle componenti diventa trascurabile.",
    "crumbs": [
      "Theory",
      "Analisi delle componenti principali"
    ]
  },
  {
    "objectID": "th/08_Analisi_delle_componenti_principali.html#distribuzione-gaussiana-multidimensionale",
    "href": "th/08_Analisi_delle_componenti_principali.html#distribuzione-gaussiana-multidimensionale",
    "title": "Analisi delle componenti principali",
    "section": "",
    "text": "Un vettore aleatorio \\(X = (X_1, \\dots, X_p)\\) segue una distribuzione gaussiana multidimensionale con parametri \\((\\mu, \\Sigma)\\), indicata con \\(N(\\mu, \\Sigma)\\), se la sua distribuzione è completamente determinata da un vettore medio e da una matrice di covarianza. Il vettore medio \\(\\mu \\in \\mathbb{R}^p\\) raccoglie le medie delle singole componenti, mentre la matrice di covarianza \\(\\Sigma \\in \\mathbb{R}^{p \\times p}\\) descrive sia le varianze marginali sia le dipendenze lineari tra le componenti.\nLa funzione di densità di probabilità associata a \\(X\\) è data da\n\\[\nf_X(x) = \\frac{1}{(2\\pi)^{p/2} \\sqrt{\\det \\Sigma}}\n\\exp\\!\\left(\n-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\n\\right),\n\\]\ndove il termine quadratico nell’esponente misura la distanza di \\(x\\) dalla media \\(\\mu\\) tenendo conto della struttura di correlazione imposta da \\(\\Sigma\\). La matrice \\(\\Sigma\\) deve essere simmetrica e definita positiva, in modo da garantire l’esistenza dell’inversa e la normalizzazione della densità.\nSe le componenti di \\(X\\) sono indipendenti, tutte le covarianze incrociate sono nulle e la matrice \\(\\Sigma\\) risulta diagonale. In questo caso la densità multidimensionale si fattorizza nel prodotto delle densità marginali e le curve di livello della distribuzione sono ellissi (o iper-ellissi) allineate con gli assi coordinati.\nQuando invece le componenti non sono indipendenti, la matrice di covarianza contiene termini fuori diagonale. Geometricamente, ciò si traduce in ellissi di livello ruotate e deformate, il cui orientamento e allungamento riflettono le correlazioni e le diverse scale di variabilità delle variabili originali.",
    "crumbs": [
      "Theory",
      "Analisi delle componenti principali"
    ]
  },
  {
    "objectID": "th/08_Analisi_delle_componenti_principali.html#legge-di-dirichlet",
    "href": "th/08_Analisi_delle_componenti_principali.html#legge-di-dirichlet",
    "title": "Analisi delle componenti principali",
    "section": "",
    "text": "La legge di Dirichlet è una distribuzione di probabilità definita su vettori di proporzioni, ed è utilizzata quando si vuole modellare l’incertezza su come una quantità totale venga suddivisa tra più categorie. È particolarmente importante in ambito bayesiano, dove compare spesso come distribuzione a priori per vettori di probabilità, ad esempio nei modelli di clustering, nei modelli a miscela e nei topic model.\nFormalmente, la distribuzione di Dirichlet genera vettori aleatori\n\\[\nX = (X_1, \\dots, X_m)\n\\]\ntali che ogni componente sia non negativa e che la somma totale valga uno,\n\\[\nX_i \\ge 0, \\qquad \\sum_{i=1}^m X_i = 1.\n\\]\nSi scrive\n\\[\nX \\sim \\text{Dirichlet}(\\alpha),\n\\]\ndove il parametro \\(\\alpha = (\\alpha_1, \\dots, \\alpha_m)\\) soddisfa \\(\\alpha_i &gt; 0\\) per ogni \\(i\\). I parametri \\(\\alpha_i\\) controllano la forma della distribuzione e riflettono il peso relativo attribuito a ciascuna categoria.\nLa funzione di densità di probabilità è data da\n\\[\nf_X(x) = C_\\alpha \\, x_1^{\\alpha_1 - 1} \\cdots x_m^{\\alpha_m - 1},\n\\]\ndove \\(C_\\alpha\\) è una costante di normalizzazione che dipende dal vettore \\(\\alpha\\) e garantisce che la densità integri a uno sul simplesso \\(\\{x : x_i \\ge 0,\\ \\sum x_i = 1\\}\\).\nLa media della distribuzione di Dirichlet ha una forma particolarmente semplice ed è pari a\n\\[\n\\mathbb{E}(X_i) = \\frac{\\alpha_i}{\\sum_{j=1}^m \\alpha_j},\n\\]\noppure, in forma vettoriale,\n\\[\n\\mathbb{E}(X) = \\frac{\\alpha}{\\sum_{i=1}^m \\alpha_i}.\n\\]\nQuesto mostra che il parametro \\(\\alpha\\) può essere interpretato come un insieme di pseudoconteggi: il rapporto tra ciascun \\(\\alpha_i\\) e la loro somma determina il valore medio delle proporzioni.\nNel caso particolare \\(m = 2\\), la distribuzione di Dirichlet si riduce alla distribuzione Beta, che modella una singola proporzione su \\([0,1]\\). La Dirichlet può quindi essere vista come una generalizzazione multivariata della Beta, adatta a descrivere simultaneamente più probabilità che devono sommare a uno.",
    "crumbs": [
      "Theory",
      "Analisi delle componenti principali"
    ]
  },
  {
    "objectID": "th/08_Analisi_delle_componenti_principali.html#distribuzione-multinomiale",
    "href": "th/08_Analisi_delle_componenti_principali.html#distribuzione-multinomiale",
    "title": "Analisi delle componenti principali",
    "section": "",
    "text": "La distribuzione multinomiale descrive il comportamento dei conteggi associati a più categorie quando si ripete uno stesso esperimento un numero fissato di volte. Può essere vista come una naturale estensione della distribuzione binomiale al caso in cui gli esiti possibili non siano due, ma \\(m \\ge 2\\). Un esempio tipico è il lancio ripetuto di un dado: ogni lancio produce uno tra \\(m=6\\) risultati possibili e, dopo \\(n\\) lanci, si è interessati a contare quante volte compare ciascuna faccia.\nFormalmente, si considera un vettore aleatorio\n\\[\nX = (X_1, \\dots, X_m),\n\\]\ndove \\(X_i\\) rappresenta il numero di volte in cui si osserva l’esito appartenente alla categoria \\(i\\) su un totale di \\(n\\) prove indipendenti. Il modello è parametrizzato da un vettore di probabilità\n\\[\np = (p_1, \\dots, p_m), \\qquad \\sum_{i=1}^m p_i = 1,\n\\]\nche descrive la probabilità di ciascun esito in una singola prova. Si scrive allora\n\\[\nX \\sim \\text{Multinomiale}(n, p).\n\\]\nLa funzione di massa di probabilità è data da\n\\[\nP(X = x)\n=\n\\frac{n!}{x_1! \\cdots x_m!}\n\\, p_1^{x_1} \\cdots p_m^{x_m},\n\\qquad\n\\sum_{i=1}^m x_i = n.\n\\]\nIl coefficiente combinatorio tiene conto di tutti i possibili ordini in cui i diversi esiti possono verificarsi nelle \\(n\\) prove, mentre il prodotto dei termini \\(p_i^{x_i}\\) pesa ciascuna configurazione in base alle probabilità delle singole categorie.\nCiascuna componente \\(X_i\\) ha distribuzione binomiale marginale con parametri \\((n, p_i)\\), ma le componenti non sono indipendenti tra loro. Il vincolo \\(\\sum_i X_i = n\\) implica infatti che un aumento in un conteggio debba essere compensato da una diminuzione in almeno un altro. La distribuzione multinomiale è quindi lo strumento naturale per modellare esperimenti con più di due esiti possibili, quando l’interesse è rivolto alla distribuzione congiunta dei conteggi.",
    "crumbs": [
      "Theory",
      "Analisi delle componenti principali"
    ]
  },
  {
    "objectID": "th/13_Regressione_logistica.html",
    "href": "th/13_Regressione_logistica.html",
    "title": "Regressione logistica",
    "section": "",
    "text": "La regressione logistica viene utilizzata quando la variabile risposta è dicotomica, tipicamente codificata come \\(0/1\\). Per ogni osservazione \\(i=1,\\dots,N\\), associata a un vettore di input \\(x^{(i)}=(x_1^{(i)},\\dots,x_p^{(i)})\\), si assume che la risposta \\(Z_i\\) segua una distribuzione di Bernoulli con parametro \\(p_i\\): \\[\nZ_i \\sim \\text{Bernoulli}(p_i), \\qquad p_i = f\\!\\big(x^{(i)}\\big).\n\\]\nLa probabilità \\(p_i\\) è modellata tramite una sigmoide applicata a un predittore lineare. In particolare si pone\n\\[\np_i = \\sigma(\\eta_i), \\qquad\n\\eta_i = \\beta_0 + \\beta_1 x_1^{(i)} + \\cdots + \\beta_p x_p^{(i)},\n\\] dove la funzione sigmoide è definita come \\[\n\\sigma(z) = \\frac{1}{1+e^{-z}}.\n\\] In questo modo il predittore lineare \\(\\eta_i\\) può assumere qualunque valore reale, mentre la sigmoide garantisce che \\(p_i \\in (0,1)\\).\n\n\nIl vettore dei parametri \\(\\beta=(\\beta_0,\\dots,\\beta_p)\\) viene stimato tramite massima verosimiglianza. La log-verosimiglianza associata al campione è \\[\n\\ell(\\beta)\n= \\sum_{i=1}^N \\Big[ Z_i \\log p_i + (1-Z_i)\\log(1-p_i) \\Big],\n\\qquad\np_i=\\sigma(\\eta_i).\n\\]\nMassimizzare questa quantità è equivalente a minimizzare la cross-entropy, ossia la negativa della log-verosimiglianza media: \\[\n\\text{loss}_{\\text{CE}}(\\beta)\n= -\\frac{1}{N}\\sum_{i=1}^N \\Big[ Z_i \\log p_i + (1-Z_i)\\log(1-p_i) \\Big].\n\\] Questa funzione obiettivo è convessa rispetto ai parametri e coincide con la funzione di perdita comunemente usata nei modelli di classificazione binaria.\nDal punto di vista computazionale, la stima dei parametri richiede metodi di ottimizzazione numerica, come Newton–Raphson o il metodo IRLS (Iteratively Reweighted Least Squares), oppure algoritmi di discesa del gradiente. In pratica si utilizzano formulazioni numericamente stabili della loss, ad esempio basate su trasformazioni tipo log-sum-exp, per evitare problemi di overflow o underflow.\nPur non esistendo una distribuzione esatta in forma chiusa per gli stimatori \\(\\hat\\beta\\), gli stimatori di massima verosimiglianza sono asintoticamente normali. Questo consente di costruire intervalli di confidenza e test di ipotesi (Wald, Likelihood Ratio, Score) utilizzando la matrice di informazione di Fisher. Un aspetto critico è la presenza di separazione completa o quasi dei dati: in tali casi l’MLE può non esistere o divergere, e si ricorre a tecniche di penalizzazione, come la correzione di Firth, o alla regolarizzazione.\n\n\n\nDato un nuovo vettore di input \\(x\\), il modello di regressione logistica restituisce direttamente una probabilità di appartenenza alla classe positiva: \\[\nP(Z=1\\mid x)=\\sigma\\!\\big(\\beta_0+\\beta_1 x_1+\\cdots+\\beta_p x_p\\big).\n\\] Questa quantità rappresenta una stima di \\(P(Z=1\\mid X=x)\\) e non una decisione discreta. La classificazione si ottiene introducendo una soglia sulla probabilità, ad esempio \\(0.5\\), ma tale scelta non è intrinseca al modello. In pratica la soglia va calibrata in funzione dell’obiettivo applicativo, tenendo conto del compromesso tra errori di tipo diverso, spesso tramite curve ROC o Precision–Recall e funzioni di costo asimmetriche.\n\n\n\nQuando la variabile risposta assume più di due categorie (\\(m&gt;2\\)), \\(Z_i\\in\\{1,\\dots,m\\}\\), la regressione logistica si estende al caso multiclasse. Si utilizza una codifica one-hot della risposta, indicata con \\(b^{(i)}=(b^{(i)}_1,\\dots,b^{(i)}_m)\\), dove un solo elemento vale 1 e gli altri 0. In questo caso si assume \\[\nB^{(i)} \\sim \\text{Multinomiale}\\!\\big(1;\\ \\pi(x^{(i)},W)\\big),\n\\] dove \\(\\pi(x^{(i)},W)\\) è il vettore delle probabilità di classe e \\(W\\) è la matrice dei parametri del modello. Inserendo esplicitamente l’intercetta, si può considerare \\(W\\in\\mathbb{R}^{(p+1)\\times m}\\).\nPer ciascuna classe \\(k\\) si definisce un logit lineare \\[\ng_k^{(i)} = w_{0k} + w_{1k} x_1^{(i)} + \\cdots + w_{pk} x_p^{(i)},\n\\] e le probabilità di appartenenza alle classi sono ottenute tramite la funzione softmax: \\[\n\\pi_k\\!\\big(x^{(i)},W\\big) =\n\\frac{e^{g_k^{(i)}}}{\\sum_{h=1}^m e^{g_h^{(i)}}},\n\\qquad \\sum_{k=1}^m \\pi_k(\\cdot)=1.\n\\] La softmax generalizza la sigmoide al caso multiclasse, garantendo probabilità positive che sommano a uno.\nLa log-verosimiglianza del campione assume la forma \\[\n\\ell(W) = \\sum_{i=1}^N \\sum_{k=1}^m b^{(i)}_k \\log \\pi_k\\!\\big(x^{(i)},W\\big),\n\\] e la funzione obiettivo da minimizzare è la cross-entropy media: \\[\n\\text{loss}(W) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^m b^{(i)}_k \\log \\pi_k\\!\\big(x^{(i)},W\\big).\n\\]\nUn aspetto teorico rilevante è l’identificabilità del modello: i logit sono definiti a meno di una traslazione comune, che non modifica le probabilità softmax. Per ottenere stime univoche è quindi necessario imporre un vincolo, ad esempio fissando una classe di riferimento (modello baseline-category logit) oppure imponendo condizioni di somma o di centratura sui parametri.\n\n\n\nUn approccio alternativo alla regressione logistica multinomiale consiste nello stimare \\(m\\) modelli binari distinti, uno per ciascuna classe, confrontando la classe \\(k\\) contro il resto delle classi. Questo schema, noto come one-vs-rest, è concettualmente semplice e facile da implementare, poiché riduce il problema multiclasse a una collezione di problemi binari standard. Tuttavia, le probabilità stimate dai singoli modelli non sono coerenti tra loro e in generale non sommano a 1, per cui non ammettono un’interpretazione probabilistica congiunta. Per questo motivo il metodo è spesso usato come baseline o in contesti applicativi semplici, mentre risulta meno rigoroso rispetto alla formulazione multinomiale con softmax.",
    "crumbs": [
      "Theory",
      "Regressione logistica"
    ]
  },
  {
    "objectID": "th/13_Regressione_logistica.html#stima-dei-parametri-e-funzione-obiettivo",
    "href": "th/13_Regressione_logistica.html#stima-dei-parametri-e-funzione-obiettivo",
    "title": "Regressione logistica",
    "section": "",
    "text": "Il vettore dei parametri \\(\\beta=(\\beta_0,\\dots,\\beta_p)\\) viene stimato tramite massima verosimiglianza. La log-verosimiglianza associata al campione è \\[\n\\ell(\\beta)\n= \\sum_{i=1}^N \\Big[ Z_i \\log p_i + (1-Z_i)\\log(1-p_i) \\Big],\n\\qquad\np_i=\\sigma(\\eta_i).\n\\]\nMassimizzare questa quantità è equivalente a minimizzare la cross-entropy, ossia la negativa della log-verosimiglianza media: \\[\n\\text{loss}_{\\text{CE}}(\\beta)\n= -\\frac{1}{N}\\sum_{i=1}^N \\Big[ Z_i \\log p_i + (1-Z_i)\\log(1-p_i) \\Big].\n\\] Questa funzione obiettivo è convessa rispetto ai parametri e coincide con la funzione di perdita comunemente usata nei modelli di classificazione binaria.\nDal punto di vista computazionale, la stima dei parametri richiede metodi di ottimizzazione numerica, come Newton–Raphson o il metodo IRLS (Iteratively Reweighted Least Squares), oppure algoritmi di discesa del gradiente. In pratica si utilizzano formulazioni numericamente stabili della loss, ad esempio basate su trasformazioni tipo log-sum-exp, per evitare problemi di overflow o underflow.\nPur non esistendo una distribuzione esatta in forma chiusa per gli stimatori \\(\\hat\\beta\\), gli stimatori di massima verosimiglianza sono asintoticamente normali. Questo consente di costruire intervalli di confidenza e test di ipotesi (Wald, Likelihood Ratio, Score) utilizzando la matrice di informazione di Fisher. Un aspetto critico è la presenza di separazione completa o quasi dei dati: in tali casi l’MLE può non esistere o divergere, e si ricorre a tecniche di penalizzazione, come la correzione di Firth, o alla regolarizzazione.",
    "crumbs": [
      "Theory",
      "Regressione logistica"
    ]
  },
  {
    "objectID": "th/13_Regressione_logistica.html#predizione",
    "href": "th/13_Regressione_logistica.html#predizione",
    "title": "Regressione logistica",
    "section": "",
    "text": "Dato un nuovo vettore di input \\(x\\), il modello di regressione logistica restituisce direttamente una probabilità di appartenenza alla classe positiva: \\[\nP(Z=1\\mid x)=\\sigma\\!\\big(\\beta_0+\\beta_1 x_1+\\cdots+\\beta_p x_p\\big).\n\\] Questa quantità rappresenta una stima di \\(P(Z=1\\mid X=x)\\) e non una decisione discreta. La classificazione si ottiene introducendo una soglia sulla probabilità, ad esempio \\(0.5\\), ma tale scelta non è intrinseca al modello. In pratica la soglia va calibrata in funzione dell’obiettivo applicativo, tenendo conto del compromesso tra errori di tipo diverso, spesso tramite curve ROC o Precision–Recall e funzioni di costo asimmetriche.",
    "crumbs": [
      "Theory",
      "Regressione logistica"
    ]
  },
  {
    "objectID": "th/13_Regressione_logistica.html#regressione-logistica-multinomiale",
    "href": "th/13_Regressione_logistica.html#regressione-logistica-multinomiale",
    "title": "Regressione logistica",
    "section": "",
    "text": "Quando la variabile risposta assume più di due categorie (\\(m&gt;2\\)), \\(Z_i\\in\\{1,\\dots,m\\}\\), la regressione logistica si estende al caso multiclasse. Si utilizza una codifica one-hot della risposta, indicata con \\(b^{(i)}=(b^{(i)}_1,\\dots,b^{(i)}_m)\\), dove un solo elemento vale 1 e gli altri 0. In questo caso si assume \\[\nB^{(i)} \\sim \\text{Multinomiale}\\!\\big(1;\\ \\pi(x^{(i)},W)\\big),\n\\] dove \\(\\pi(x^{(i)},W)\\) è il vettore delle probabilità di classe e \\(W\\) è la matrice dei parametri del modello. Inserendo esplicitamente l’intercetta, si può considerare \\(W\\in\\mathbb{R}^{(p+1)\\times m}\\).\nPer ciascuna classe \\(k\\) si definisce un logit lineare \\[\ng_k^{(i)} = w_{0k} + w_{1k} x_1^{(i)} + \\cdots + w_{pk} x_p^{(i)},\n\\] e le probabilità di appartenenza alle classi sono ottenute tramite la funzione softmax: \\[\n\\pi_k\\!\\big(x^{(i)},W\\big) =\n\\frac{e^{g_k^{(i)}}}{\\sum_{h=1}^m e^{g_h^{(i)}}},\n\\qquad \\sum_{k=1}^m \\pi_k(\\cdot)=1.\n\\] La softmax generalizza la sigmoide al caso multiclasse, garantendo probabilità positive che sommano a uno.\nLa log-verosimiglianza del campione assume la forma \\[\n\\ell(W) = \\sum_{i=1}^N \\sum_{k=1}^m b^{(i)}_k \\log \\pi_k\\!\\big(x^{(i)},W\\big),\n\\] e la funzione obiettivo da minimizzare è la cross-entropy media: \\[\n\\text{loss}(W) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^m b^{(i)}_k \\log \\pi_k\\!\\big(x^{(i)},W\\big).\n\\]\nUn aspetto teorico rilevante è l’identificabilità del modello: i logit sono definiti a meno di una traslazione comune, che non modifica le probabilità softmax. Per ottenere stime univoche è quindi necessario imporre un vincolo, ad esempio fissando una classe di riferimento (modello baseline-category logit) oppure imponendo condizioni di somma o di centratura sui parametri.",
    "crumbs": [
      "Theory",
      "Regressione logistica"
    ]
  },
  {
    "objectID": "th/13_Regressione_logistica.html#one-vs-rest",
    "href": "th/13_Regressione_logistica.html#one-vs-rest",
    "title": "Regressione logistica",
    "section": "",
    "text": "Un approccio alternativo alla regressione logistica multinomiale consiste nello stimare \\(m\\) modelli binari distinti, uno per ciascuna classe, confrontando la classe \\(k\\) contro il resto delle classi. Questo schema, noto come one-vs-rest, è concettualmente semplice e facile da implementare, poiché riduce il problema multiclasse a una collezione di problemi binari standard. Tuttavia, le probabilità stimate dai singoli modelli non sono coerenti tra loro e in generale non sommano a 1, per cui non ammettono un’interpretazione probabilistica congiunta. Per questo motivo il metodo è spesso usato come baseline o in contesti applicativi semplici, mentre risulta meno rigoroso rispetto alla formulazione multinomiale con softmax.",
    "crumbs": [
      "Theory",
      "Regressione logistica"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html",
    "href": "th/02. Variabili aleatorie.html",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Le quantità di interesse determinate dall’esito di un esperimento casuale sono dette variabili aleatorie. Poiché il valore di una variabile aleatoria dipende dall’esito dell’esperimento, è possibile associare una probabilità ai suoi possibili valori.\nLa funzione di ripartizione \\(F\\) (o CDF, Cumulative Distribution Function) di una variabile aleatoria \\(X\\) è definita, per ogni numero reale \\(i\\), come:\n\\[\nF(i) = P(X \\leq i)\n\\]\nQuindi, \\(F(i)\\) esprime la probabilità che \\(X\\) assuma un valore minore o uguale a \\(i\\). Si utilizza la notazione \\(X \\sim F\\) per indicare che \\(F\\) è la funzione di ripartizione di \\(X\\).\nLe variabili aleatorie che possono assumere un numero finito o numerabile di valori sono dette discrete, mentre quelle che possono assumere un insieme continuo (infinito non numerabile) di valori sono dette continue.\nLa funzione di ripartizione ha utili applicazioni sia nel caso continuo\n\nche in quello discreto\n\nInoltre, calcolando l’inverso della funzione di ripartizione, è possibile individuare i percentili di una distribuzione di probabilità.\n\n\nSe \\(X\\) è una variabile aleatoria discreta, la sua funzione di massa di probabilità (o PMF, Probability Mass Function) è definita come:\n\\[\np(a) = P(X = a)\n\\]\nNota: Poiché \\(X\\) deve assumere uno dei valori \\(x_1, x_2, \\dots\\), la funzione di massa di probabilità deve soddisfare la seguente condizione:\n\\[\n\\sum_{i} p(x_i) = 1\n\\]\nSe \\(X\\) è una variabile aleatoria continua, definita su tutto \\(\\mathbb{R}\\), per ogni insieme misurabile \\(B\\) di numeri reali vale:\n\\[\nP(X \\in B) = \\int_B f(x) \\, dx\n\\]\nNota: Analogamente alla funzione di massa di probabilità, la funzione di densità deve soddisfare la condizione:\n\\[\n\\int_{-\\infty}^{+\\infty} f(x) \\, dx = 1\n\\]\nQuando si conosce la funzione di massa di una variabile aleatoria discreta, la funzione di densità di una variabile continua, o la funzione di ripartizione di una variabile aleatoria qualsiasi, si hanno informazioni sufficienti per calcolare la probabilità di qualsiasi evento dipendente da quella variabile. In tal caso, si dice che è nota la distribuzione della variabile considerata. Inoltre, dire che due variabili aleatorie \\(X\\) e \\(Y\\) hanno la stessa distribuzione equivale a dire che le loro funzioni sopra citate sono identiche.\n\n\n\nSiano \\(X\\) e \\(Y\\) due variabili aleatorie relative allo stesso esperimento. La funzione di ripartizione congiunta di \\(X\\) e \\(Y\\) è definita come:\n\\[\nF(a, b) = P(X \\leq a, Y \\leq b)\n\\]\nNota: La virgola nell’argomento di \\(P\\) rappresenta l’intersezione tra i due eventi.\nSe \\(X\\) e \\(Y\\) sono variabili aleatorie discrete, la loro funzione di massa congiunta è:\n\\[\np(x_i, y_j) = P(X = x_i, Y = y_j)\n\\]\nDue variabili aleatorie sono congiuntamente continue se esiste una funzione \\(f(x, y)\\) non negativa tale che, per ogni sottoinsieme misurabile \\(C\\) del piano cartesiano, vale:\n\\[\nP\\big((X, Y) \\in C\\big) = \\iint_{(x, y) \\in C} f(x, y) \\, dx \\, dy\n\\]\nLa funzione \\(f(x, y)\\) è detta densità congiunta di \\(X\\) e \\(Y\\).\nAd esempio, se \\(X\\) rappresenta l’altezza di una persona e \\(Y\\) il suo peso, la densità congiunta \\(f_{X,Y}(a, b)\\) indica quanto è probabile che una persona scelta a caso abbia altezza \\(a\\) e peso \\(b\\). Maggiore è il valore di \\(f\\) in un punto \\((a, b)\\), maggiore è la probabilità di osservare quella particolare combinazione di altezza e peso. Integrando \\(f\\) su un’area specifica, otteniamo la probabilità che altezza e peso cadano in quell’intervallo. Ad esempio, integrando su altezze tra 160 cm e 170 cm e pesi tra 60 kg e 70 kg, si ottiene la probabilità che una persona abbia altezza e peso in quegli intervalli.\nIn generale, due variabili prese contemporaneamente hanno una distribuzione congiunta. Una distribuzione congiunta di variabili gaussiane, ha spesso forma ellittica\n\nDue variabili aleatorie relative allo stesso esperimento si dicono indipendenti se, per ogni coppia di insiemi misurabili \\(A\\) e \\(B\\), vale:\n\\[\nP(X \\in A, Y \\in B) = P(X \\in A) \\cdot P(Y \\in B)\n\\]\nIn caso contrario, si dicono dipendenti. Questa definizione è equivalente alla richiesta che, per ogni coppia di numeri reali \\(a\\) e \\(b\\):\n\\[\nP(X \\leq a, Y \\leq b) = P(X \\leq a) \\cdot P(Y \\leq b)\n\\]\novvero, che la funzione di ripartizione congiunta sia uguale al prodotto delle funzioni di ripartizione marginali:\n\\[\nF(a, b) = F_X(a) \\cdot F_Y(b)\n\\]\nSe le variabili considerate sono discrete, l’indipendenza è equivalente a richiedere che la funzione di massa congiunta sia uguale al prodotto delle funzioni di massa marginali:\n\\[\np(a, b) = p_X(a) \\cdot p_Y(b)\n\\]\n\n\n\nSe \\(X\\) è una variabile aleatoria discreta che può assumere i valori \\(x_1, x_2, \\dots\\), il valore atteso di \\(X\\), indicato con \\(E[X]\\), è (se esiste) dato da:\n\\[\nE[X] = \\sum_{i} x_i \\cdot P(X = x_i)\n\\]\n\\(E[X]\\) è anche detto media o aspettazione di \\(X\\), e rappresenta la media ponderata dei valori possibili di \\(X\\), usando come pesi le probabilità associate.\nNota: \\(E[X]\\) non è necessariamente uguale a uno dei valori possibili di \\(X\\).\nSe \\(X\\) è una variabile aleatoria continua con densità \\(f(x)\\), il valore atteso di \\(X\\) è (se esiste):\n\\[\nE[X] = \\int_{-\\infty}^{+\\infty} x \\cdot f(x) \\, dx\n\\]\nIn pratica, il concetto di valore atteso è analogo a quello fisico del baricentro, ma applicato alle distribuzioni di probabilità.\nPer \\(n = 1, 2, \\dots\\), la quantità \\(E[X^n]\\) è detta, se esiste, momento di ordine \\(n\\) della variabile \\(X\\). In particolare:\n\\[\nE[X^n] =\n\\begin{cases}\n\\sum_{i} x_i^n \\cdot p(x_i), & \\text{se } X \\text{ è discreta} \\\\\n\\int_{-\\infty}^{+\\infty} x^n \\cdot f(x) \\, dx, & \\text{se } X \\text{ è continua}\n\\end{cases}\n\\]\nDi conseguenza, \\(E[X]\\) è il momento di primo ordine di \\(X\\). Inoltre, vale la proprietà:\n\\[\nE[X + Y] = E[X] + E[Y]\n\\]\n\n\n\nData una variabile aleatoria \\(X\\) con media \\(\\mu = E[X]\\), la varianza di \\(X\\) è, se esiste, definita come:\n\\[\n\\operatorname{Var}(X) = E\\big[(X - \\mu)^2\\big]\n\\]\nEsiste una formula alternativa per calcolare la varianza:\n\\[\n\\operatorname{Var}(X) = E[X^2] - [E[X]]^2\n\\]\nLa quantità \\(\\sigma = \\sqrt{\\operatorname{Var}(X)}\\) è detta deviazione standard della variabile aleatoria \\(X\\).\n\n\n\nDate le variabili aleatorie \\(X\\) e \\(Y\\) con medie \\(\\mu_X\\) e \\(\\mu_Y\\), la covarianza di \\(X\\) e \\(Y\\) è, se esiste, la quantità:\n\\[\n\\operatorname{Cov}(X, Y) = E\\big[(X - \\mu_X)(Y - \\mu_Y)\\big]\n\\]\nAnche in questo caso, esiste una formula alternativa per calcolare la covarianza:\n\\[\n\\operatorname{Cov}(X, Y) = E[XY] - E[X] \\cdot E[Y]\n\\]\nInoltre, se \\(X\\) e \\(Y\\) sono variabili aleatorie indipendenti, allora:\n\\[\nE[XY] = E[X] \\cdot E[Y]\n\\]\ne quindi:\n\\[\n\\operatorname{Cov}(X, Y) = 0\n\\]\nData la somma di due variabili aleatorie \\(X\\) e \\(Y\\), vale:\n\\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) + 2 \\cdot \\operatorname{Cov}(X, Y)\n\\]\nSe \\(X\\) e \\(Y\\) sono indipendenti, allora:\n\\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)\n\\]\nInoltre, date le variabili aleatorie \\(X_1, X_2, \\dots, X_n\\):\n\\[\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_i) + 2 \\sum_{1 \\leq i &lt; j \\leq n} \\operatorname{Cov}(X_i, X_j)\n\\]\nSe \\(X_1, X_2, \\dots, X_n\\) sono tutte indipendenti, allora:\n\\[\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_i)\n\\]\nCasi particolari. Di seguito sono mostrati alcuni casi particolari:\n\n\\(\\operatorname{Var}(aX + bY) = a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y) + 2ab \\operatorname{Cov}(X, Y)\\)\n\\(\\operatorname{Var}(X - Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) - 2 \\operatorname{Cov}(X, Y)\\)\n\\(\\operatorname{Var}(a + bX) = b^2 \\operatorname{Var}(X)\\)\n\nIn generare, si può mostrare che un valore positivo di \\(Cov(X,Y)\\) indica che \\(X\\) ed \\(Y\\) tendenzialmente assumono valori grandi o piccoli contemporaneamente. La “forza” della relazione tra \\(X\\) ed \\(Y\\) è misurata più propriamente dal coefficiente di correlazione lineare, un numero puro (senza unità di misura) che tiene conto anche delle deviazioni standard di \\(X\\) ed \\(Y\\). Esso si indica con \\(Corr(X,Y)\\) ed è definito come\n\\[\nCorr(X,Y) := \\dfrac{Cov(X,Y)}{\\sqrt{Var(X) \\cdot Var(Y)}}\n\\]\nSi può inoltre dimostrare che questa quantità è sempre compresa tra \\(-1\\) e \\(+1\\).\n\n\n\nDisuguaglianza di Markov. Se \\(X\\) è una variabile aleatoria che non è mai negativa, allora per ogni \\(a &gt; 0\\)\n\\[\nP(X \\ge a) \\le \\dfrac{E[X]}{a}\n\\]\nDisuguaglianza di Chebyshev. Se \\(X\\) è una variabile aleatoria con media \\(\\mu\\) e varianza \\(\\sigma^2\\), allora per ogni \\(r&gt;0\\)\n\\[\nP(|X - \\mu| \\ge r ) \\le \\dfrac{\\sigma^2}{r^2}\n\\]\nLegge debole dei grandi numeri. Sia \\(X_1, \\dots, X_n\\) una successione di variabili aleatorie i.i.d (indipendenti e identicamente distribuite), tutte con media \\(E[X_i] = \\mu\\). Allora, per ogni \\(\\varepsilon &gt; 0\\),\n\\[\nP\\left(\\left\\vert \\dfrac{X_1 + \\dots + X_n}{n} - \\mu \\right\\vert &gt; \\varepsilon\\right) \\rightarrow 0\\] quando \\(n \\rightarrow \\infty\\).\n\n\n\nPer generare dati coerenti con una distribuzione di probabilità, si parte dalla generazione di numeri casuali distribuiti uniformemente tra 0 e 1. Successivamente, si applica la funzione di distribuzione cumulativa inversa della distribuzione desiderata, in modo da trasformare i numeri casuali uniformi nei valori coerenti con la distribuzione target.\n\n\n\nDato un campione di dati osservati\n\nè possibile studiare la sua distribuzione attraverso l’istogramma dei dati\n\nOppure attraverso la funzione di ripartizione empirica (ECDF, Empirical Cumulative Distribution Function), una stima della funzione di ripartizione del campione. Per ottenere la ECDF a partire da un insieme di dati campionari, occorre ordinare i dati in ordine crescente.\n\nInvertendo gli assi, otteniamo una CdF empirica “grossolana”\n\nScalando i dati tra 0 e 1, si ricava la ECDF, che approssima la CdF reale, che ha generato il campione\n\nL’ECDF è utile per studiare la distribuzione dei dati osservati, e per visualizzare tale distribuzione in modo intuitivo.\n\n\n\nIl diagramma Q-Q (Quantile-Quantile) consente di confrontare la distribuzione di un insieme di dati con la distribuzione normale, mostrando sull’asse orizzontale i quantili teorici della distribuzione di riferimento e su quello verticale i quantili della distribuzione osservata. Se i dati seguono effettivamente una distribuzione normale, i punti del grafico si allineano lungo una retta, poiché i punti\n\\[\\left(\\Phi^{-1}\\left(\\dfrac{i-0.5}{n}\\right), x_{(i)}\\right)\n\\]\ncorrispondono ai quantili teorici, calcolati tramite la funzione inversa \\(\\Phi^{-1}\\) della distribuzione \\(\\mathcal{N}(0,1)\\), e ai dati osservati ordinati.\n\nPer costruire il diagramma, si parte da un insieme di dati con una qualsiasi distribuzione, li si ordina in modo crescente, si calcola la funzione di distribuzione cumulativa empirica (CdF empirica) e si confrontano i quantili empirici con quelli teorici della distribuzione normale.",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#funzioni-di-massa-e-densità",
    "href": "th/02. Variabili aleatorie.html#funzioni-di-massa-e-densità",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Se \\(X\\) è una variabile aleatoria discreta, la sua funzione di massa di probabilità (o PMF, Probability Mass Function) è definita come:\n\\[\np(a) = P(X = a)\n\\]\nNota: Poiché \\(X\\) deve assumere uno dei valori \\(x_1, x_2, \\dots\\), la funzione di massa di probabilità deve soddisfare la seguente condizione:\n\\[\n\\sum_{i} p(x_i) = 1\n\\]\nSe \\(X\\) è una variabile aleatoria continua, definita su tutto \\(\\mathbb{R}\\), per ogni insieme misurabile \\(B\\) di numeri reali vale:\n\\[\nP(X \\in B) = \\int_B f(x) \\, dx\n\\]\nNota: Analogamente alla funzione di massa di probabilità, la funzione di densità deve soddisfare la condizione:\n\\[\n\\int_{-\\infty}^{+\\infty} f(x) \\, dx = 1\n\\]\nQuando si conosce la funzione di massa di una variabile aleatoria discreta, la funzione di densità di una variabile continua, o la funzione di ripartizione di una variabile aleatoria qualsiasi, si hanno informazioni sufficienti per calcolare la probabilità di qualsiasi evento dipendente da quella variabile. In tal caso, si dice che è nota la distribuzione della variabile considerata. Inoltre, dire che due variabili aleatorie \\(X\\) e \\(Y\\) hanno la stessa distribuzione equivale a dire che le loro funzioni sopra citate sono identiche.",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#coppie-e-vettori-di-variabili-aleatorie",
    "href": "th/02. Variabili aleatorie.html#coppie-e-vettori-di-variabili-aleatorie",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Siano \\(X\\) e \\(Y\\) due variabili aleatorie relative allo stesso esperimento. La funzione di ripartizione congiunta di \\(X\\) e \\(Y\\) è definita come:\n\\[\nF(a, b) = P(X \\leq a, Y \\leq b)\n\\]\nNota: La virgola nell’argomento di \\(P\\) rappresenta l’intersezione tra i due eventi.\nSe \\(X\\) e \\(Y\\) sono variabili aleatorie discrete, la loro funzione di massa congiunta è:\n\\[\np(x_i, y_j) = P(X = x_i, Y = y_j)\n\\]\nDue variabili aleatorie sono congiuntamente continue se esiste una funzione \\(f(x, y)\\) non negativa tale che, per ogni sottoinsieme misurabile \\(C\\) del piano cartesiano, vale:\n\\[\nP\\big((X, Y) \\in C\\big) = \\iint_{(x, y) \\in C} f(x, y) \\, dx \\, dy\n\\]\nLa funzione \\(f(x, y)\\) è detta densità congiunta di \\(X\\) e \\(Y\\).\nAd esempio, se \\(X\\) rappresenta l’altezza di una persona e \\(Y\\) il suo peso, la densità congiunta \\(f_{X,Y}(a, b)\\) indica quanto è probabile che una persona scelta a caso abbia altezza \\(a\\) e peso \\(b\\). Maggiore è il valore di \\(f\\) in un punto \\((a, b)\\), maggiore è la probabilità di osservare quella particolare combinazione di altezza e peso. Integrando \\(f\\) su un’area specifica, otteniamo la probabilità che altezza e peso cadano in quell’intervallo. Ad esempio, integrando su altezze tra 160 cm e 170 cm e pesi tra 60 kg e 70 kg, si ottiene la probabilità che una persona abbia altezza e peso in quegli intervalli.\nIn generale, due variabili prese contemporaneamente hanno una distribuzione congiunta. Una distribuzione congiunta di variabili gaussiane, ha spesso forma ellittica\n\nDue variabili aleatorie relative allo stesso esperimento si dicono indipendenti se, per ogni coppia di insiemi misurabili \\(A\\) e \\(B\\), vale:\n\\[\nP(X \\in A, Y \\in B) = P(X \\in A) \\cdot P(Y \\in B)\n\\]\nIn caso contrario, si dicono dipendenti. Questa definizione è equivalente alla richiesta che, per ogni coppia di numeri reali \\(a\\) e \\(b\\):\n\\[\nP(X \\leq a, Y \\leq b) = P(X \\leq a) \\cdot P(Y \\leq b)\n\\]\novvero, che la funzione di ripartizione congiunta sia uguale al prodotto delle funzioni di ripartizione marginali:\n\\[\nF(a, b) = F_X(a) \\cdot F_Y(b)\n\\]\nSe le variabili considerate sono discrete, l’indipendenza è equivalente a richiedere che la funzione di massa congiunta sia uguale al prodotto delle funzioni di massa marginali:\n\\[\np(a, b) = p_X(a) \\cdot p_Y(b)\n\\]",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#valore-atteso",
    "href": "th/02. Variabili aleatorie.html#valore-atteso",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Se \\(X\\) è una variabile aleatoria discreta che può assumere i valori \\(x_1, x_2, \\dots\\), il valore atteso di \\(X\\), indicato con \\(E[X]\\), è (se esiste) dato da:\n\\[\nE[X] = \\sum_{i} x_i \\cdot P(X = x_i)\n\\]\n\\(E[X]\\) è anche detto media o aspettazione di \\(X\\), e rappresenta la media ponderata dei valori possibili di \\(X\\), usando come pesi le probabilità associate.\nNota: \\(E[X]\\) non è necessariamente uguale a uno dei valori possibili di \\(X\\).\nSe \\(X\\) è una variabile aleatoria continua con densità \\(f(x)\\), il valore atteso di \\(X\\) è (se esiste):\n\\[\nE[X] = \\int_{-\\infty}^{+\\infty} x \\cdot f(x) \\, dx\n\\]\nIn pratica, il concetto di valore atteso è analogo a quello fisico del baricentro, ma applicato alle distribuzioni di probabilità.\nPer \\(n = 1, 2, \\dots\\), la quantità \\(E[X^n]\\) è detta, se esiste, momento di ordine \\(n\\) della variabile \\(X\\). In particolare:\n\\[\nE[X^n] =\n\\begin{cases}\n\\sum_{i} x_i^n \\cdot p(x_i), & \\text{se } X \\text{ è discreta} \\\\\n\\int_{-\\infty}^{+\\infty} x^n \\cdot f(x) \\, dx, & \\text{se } X \\text{ è continua}\n\\end{cases}\n\\]\nDi conseguenza, \\(E[X]\\) è il momento di primo ordine di \\(X\\). Inoltre, vale la proprietà:\n\\[\nE[X + Y] = E[X] + E[Y]\n\\]",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#varianza",
    "href": "th/02. Variabili aleatorie.html#varianza",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Data una variabile aleatoria \\(X\\) con media \\(\\mu = E[X]\\), la varianza di \\(X\\) è, se esiste, definita come:\n\\[\n\\operatorname{Var}(X) = E\\big[(X - \\mu)^2\\big]\n\\]\nEsiste una formula alternativa per calcolare la varianza:\n\\[\n\\operatorname{Var}(X) = E[X^2] - [E[X]]^2\n\\]\nLa quantità \\(\\sigma = \\sqrt{\\operatorname{Var}(X)}\\) è detta deviazione standard della variabile aleatoria \\(X\\).",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#covarianza-e-varianza-della-somma-di-variabili-aleatorie",
    "href": "th/02. Variabili aleatorie.html#covarianza-e-varianza-della-somma-di-variabili-aleatorie",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Date le variabili aleatorie \\(X\\) e \\(Y\\) con medie \\(\\mu_X\\) e \\(\\mu_Y\\), la covarianza di \\(X\\) e \\(Y\\) è, se esiste, la quantità:\n\\[\n\\operatorname{Cov}(X, Y) = E\\big[(X - \\mu_X)(Y - \\mu_Y)\\big]\n\\]\nAnche in questo caso, esiste una formula alternativa per calcolare la covarianza:\n\\[\n\\operatorname{Cov}(X, Y) = E[XY] - E[X] \\cdot E[Y]\n\\]\nInoltre, se \\(X\\) e \\(Y\\) sono variabili aleatorie indipendenti, allora:\n\\[\nE[XY] = E[X] \\cdot E[Y]\n\\]\ne quindi:\n\\[\n\\operatorname{Cov}(X, Y) = 0\n\\]\nData la somma di due variabili aleatorie \\(X\\) e \\(Y\\), vale:\n\\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) + 2 \\cdot \\operatorname{Cov}(X, Y)\n\\]\nSe \\(X\\) e \\(Y\\) sono indipendenti, allora:\n\\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)\n\\]\nInoltre, date le variabili aleatorie \\(X_1, X_2, \\dots, X_n\\):\n\\[\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_i) + 2 \\sum_{1 \\leq i &lt; j \\leq n} \\operatorname{Cov}(X_i, X_j)\n\\]\nSe \\(X_1, X_2, \\dots, X_n\\) sono tutte indipendenti, allora:\n\\[\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_i)\n\\]\nCasi particolari. Di seguito sono mostrati alcuni casi particolari:\n\n\\(\\operatorname{Var}(aX + bY) = a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y) + 2ab \\operatorname{Cov}(X, Y)\\)\n\\(\\operatorname{Var}(X - Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) - 2 \\operatorname{Cov}(X, Y)\\)\n\\(\\operatorname{Var}(a + bX) = b^2 \\operatorname{Var}(X)\\)\n\nIn generare, si può mostrare che un valore positivo di \\(Cov(X,Y)\\) indica che \\(X\\) ed \\(Y\\) tendenzialmente assumono valori grandi o piccoli contemporaneamente. La “forza” della relazione tra \\(X\\) ed \\(Y\\) è misurata più propriamente dal coefficiente di correlazione lineare, un numero puro (senza unità di misura) che tiene conto anche delle deviazioni standard di \\(X\\) ed \\(Y\\). Esso si indica con \\(Corr(X,Y)\\) ed è definito come\n\\[\nCorr(X,Y) := \\dfrac{Cov(X,Y)}{\\sqrt{Var(X) \\cdot Var(Y)}}\n\\]\nSi può inoltre dimostrare che questa quantità è sempre compresa tra \\(-1\\) e \\(+1\\).",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#legge-debole-dei-grandi-numeri",
    "href": "th/02. Variabili aleatorie.html#legge-debole-dei-grandi-numeri",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Disuguaglianza di Markov. Se \\(X\\) è una variabile aleatoria che non è mai negativa, allora per ogni \\(a &gt; 0\\)\n\\[\nP(X \\ge a) \\le \\dfrac{E[X]}{a}\n\\]\nDisuguaglianza di Chebyshev. Se \\(X\\) è una variabile aleatoria con media \\(\\mu\\) e varianza \\(\\sigma^2\\), allora per ogni \\(r&gt;0\\)\n\\[\nP(|X - \\mu| \\ge r ) \\le \\dfrac{\\sigma^2}{r^2}\n\\]\nLegge debole dei grandi numeri. Sia \\(X_1, \\dots, X_n\\) una successione di variabili aleatorie i.i.d (indipendenti e identicamente distribuite), tutte con media \\(E[X_i] = \\mu\\). Allora, per ogni \\(\\varepsilon &gt; 0\\),\n\\[\nP\\left(\\left\\vert \\dfrac{X_1 + \\dots + X_n}{n} - \\mu \\right\\vert &gt; \\varepsilon\\right) \\rightarrow 0\\] quando \\(n \\rightarrow \\infty\\).",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#generare-dati-data-una-distribuzione-di-probabilità",
    "href": "th/02. Variabili aleatorie.html#generare-dati-data-una-distribuzione-di-probabilità",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Per generare dati coerenti con una distribuzione di probabilità, si parte dalla generazione di numeri casuali distribuiti uniformemente tra 0 e 1. Successivamente, si applica la funzione di distribuzione cumulativa inversa della distribuzione desiderata, in modo da trasformare i numeri casuali uniformi nei valori coerenti con la distribuzione target.",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#studiare-la-distribuzione-di-un-campione",
    "href": "th/02. Variabili aleatorie.html#studiare-la-distribuzione-di-un-campione",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Dato un campione di dati osservati\n\nè possibile studiare la sua distribuzione attraverso l’istogramma dei dati\n\nOppure attraverso la funzione di ripartizione empirica (ECDF, Empirical Cumulative Distribution Function), una stima della funzione di ripartizione del campione. Per ottenere la ECDF a partire da un insieme di dati campionari, occorre ordinare i dati in ordine crescente.\n\nInvertendo gli assi, otteniamo una CdF empirica “grossolana”\n\nScalando i dati tra 0 e 1, si ricava la ECDF, che approssima la CdF reale, che ha generato il campione\n\nL’ECDF è utile per studiare la distribuzione dei dati osservati, e per visualizzare tale distribuzione in modo intuitivo.",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/02. Variabili aleatorie.html#diagramma-q-q",
    "href": "th/02. Variabili aleatorie.html#diagramma-q-q",
    "title": "Variabili Aleatorie",
    "section": "",
    "text": "Il diagramma Q-Q (Quantile-Quantile) consente di confrontare la distribuzione di un insieme di dati con la distribuzione normale, mostrando sull’asse orizzontale i quantili teorici della distribuzione di riferimento e su quello verticale i quantili della distribuzione osservata. Se i dati seguono effettivamente una distribuzione normale, i punti del grafico si allineano lungo una retta, poiché i punti\n\\[\\left(\\Phi^{-1}\\left(\\dfrac{i-0.5}{n}\\right), x_{(i)}\\right)\n\\]\ncorrispondono ai quantili teorici, calcolati tramite la funzione inversa \\(\\Phi^{-1}\\) della distribuzione \\(\\mathcal{N}(0,1)\\), e ai dati osservati ordinati.\n\nPer costruire il diagramma, si parte da un insieme di dati con una qualsiasi distribuzione, li si ordina in modo crescente, si calcola la funzione di distribuzione cumulativa empirica (CdF empirica) e si confrontano i quantili empirici con quelli teorici della distribuzione normale.",
    "crumbs": [
      "Theory",
      "Variabili Aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html",
    "href": "th/03. Modelli di variabili aleatorie.html",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Alcuni tipi di variabili aleatorie compaiono molto frequentemente in natura\n\n\nUna variabile aleatoria \\(X\\) si dice di Bernoulli se può assumere solo i valori \\(0\\) e \\(1\\), con probabilità rispettivamente \\(1 - p\\) e \\(p\\), dove \\(p \\in [0, 1]\\). La sua funzione di massa di probabilità è:\n\\[\nP(X = x) = \\begin{cases}\np & \\text{se } x = 1, \\\\\n1 - p & \\text{se } x = 0.\n\\end{cases}\n\\]\nIl suo valore atteso è \\(E[X] = p\\), mentre la sua varianza è \\(\\operatorname{Var}(X) = p(1 - p)\\).\nUn processo di Bernoulli è una successione di variabili aleatorie indipendenti \\(X_i\\) con uguale distribuzione di Bernoulli \\(\\mathcal{B}(p)\\).\nLa distribuzione binomiale descrive la probabilità del numero di successi in \\(n\\) prove di Bernoulli indipendenti, ovvero della variabile aleatoria:\n\\[\nS_n = X_1 + X_2 + \\dots + X_n.\n\\]\n\n\n\nQuando si effettuano \\(n\\) ripetizioni indipendenti di un esperimento binario, ciascuna con probabilità di successo \\(p\\) e di fallimento \\(1 - p\\), il numero totale di successi \\(S_n\\) è una variabile aleatoria binomiale \\(\\mathcal{B}(n, p)\\).\nIl coefficiente binomiale è definito come:\n\\[\n\\binom{n}{i} = \\dfrac{n!}{i! (n - i)!}.\n\\]\nLa funzione di massa di probabilità di una variabile aleatoria binomiale è:\n\\[\nP(S_n = i) = \\binom{n}{i} p^i (1 - p)^{n - i}, \\quad \\text{per } i = 0, 1, \\dots, n.\n\\]\nIl suo valore atteso è:\n\\[\nE[S_n] = n p,\n\\]\nmentre la sua varianza è:\n\\[\n\\operatorname{Var}(S_n) = n p (1 - p).\n\\]\nLa figura rappresenta il grafico della funzione di massa di una variabile binomiale con parametri \\(n = 10\\) e \\(p = 0{,}5\\), che risulta simmetrica rispetto al valore medio.\n\n\n\n\nUna variabile aleatoria \\(X\\) che assume valori interi non negativi \\(i = 0, 1, 2, \\dots\\) si dice di Poisson di parametro \\(\\lambda &gt; 0\\) se la sua funzione di massa di probabilità è:\n\\[\nP(X = i) = \\dfrac{e^{-\\lambda} \\lambda^i}{i!}.\n\\]\nIl parametro \\(\\lambda\\) rappresenta sia il valore atteso \\(E[X] = \\lambda\\) sia la varianza \\(\\operatorname{Var}(X) = \\lambda\\).\nNota. Le variabili di Poisson vengono spesso utilizzate come approssimazione delle variabili binomiali \\(\\mathcal{B}(n, p)\\) quando \\(n\\) è molto grande e \\(p\\) è molto piccolo, mantenendo \\(\\lambda = n p\\) costante.\nLa figura illustra il grafico della funzione di massa di una variabile di Poisson con parametro \\(\\lambda = 4\\).\n\nNota. La distribuzione di Poisson viene spesso utilizzata per modellare il numero di eventi in un intervallo di tempo, dato un tasso medio di occorrenza \\(\\lambda\\)\n\n\n\nUn processo di Poisson con tasso \\(\\lambda &gt; 0\\) descrive il verificarsi di eventi indipendenti nel tempo con un ritmo medio costante. Se \\(N(t)\\) indica il numero di eventi avvenuti nell’intervallo di tempo \\([0,t]\\), allora \\(N(t)\\) segue una distribuzione di Poisson con parametro \\(\\lambda t\\), vale a dire:\n\\(P\\left(N(t) = k\\right) = \\dfrac{(\\lambda t)^k}{k!}\\,e^{-\\lambda t}, \\quad k = 0,1,2,\\ldots\\)\nIl suo valore atteso è \\(E[N(t)] = \\lambda t\\), mentre la varianza è \\(\\operatorname{Var}(N(t)) = \\lambda t\\). I tempi di attesa fra un evento e il successivo, invece, seguono una distribuzione esponenziale con lo stesso parametro \\(\\lambda\\). Uno dei tratti distintivi di questo modello è il suo carattere “senza memoria”, per cui la probabilità di un evento futuro non dipende da quanto tempo è già trascorso.\n\n\n\nUna variabile aleatoria continua \\(X\\) si dice uniformemente distribuita sull’intervallo \\([a,b]\\)(con \\(a &lt; b\\)) se la sua funzione di densità di probabilità (pdf) è costante su \\([a,b]\\) e nulla altrove. Formalmente,\n\\(f_X(x) = \\begin{cases} \\dfrac{1}{b - a}, & x \\in [a, b],\\\\ 0, & \\text{altrimenti}. \\end{cases}\\)\nLa distribuzione uniforme è così chiamata perché ogni valore all’interno dell’intervallo \\([a, b]\\) è ugualmente probabile.\n\nIl valore atteso della variabile è dato da \\(E[X] = \\dfrac{a + b}{2}\\), mentre la varianza è \\(\\operatorname{Var}(X) = \\dfrac{(b - a)^2}{12}\\).\nOltre alla versione continua, esiste anche una versione discreta della distribuzione uniforme. Se \\(X\\) può assumere \\(n\\) valori distinti \\(\\{1, 2, \\dots, n\\}\\) con la stessa probabilità, allora la funzione di massa di probabilità è\n\\(P(X = k) = \\frac{1}{n}, \\quad k = 1, 2, \\dots, n.\\)\nIn questo caso, il valore atteso è \\(E[X] = \\dfrac{n+1}{2}\\) e la varianza è \\(\\operatorname{Var}(X) = \\dfrac{n^2 - 1}{12}\\).\nQuesta distribuzione è spesso utilizzata quando si vuole assegnare la stessa probabilità a tutti i possibili valori di una variabile, senza favorirne nessuno.\n\n\n\nUna variabile aleatoria \\(T\\) si dice di distribuzione geometrica, con parametro \\(p \\in (0,1]\\), se rappresenta il numero di prove necessarie fino al primo successo in un processo di Bernoulli, cioè una sequenza di prove indipendenti con probabilità di successo costante \\(p\\).\nFunzione di massa di probabilità (pmf)\nSe \\(T\\) rappresenta il numero della prima prova che ha successo, allora:\n\\[\nP(T = k) = (1 - p)^{k - 1} p, \\quad \\text{per } k = 1, 2, 3, \\dots\n\\]\nIn alternativa, se si considera \\(Y = T - 1\\), che rappresenta il numero di fallimenti prima del primo successo, allora:\n\\[\nP(Y = k) = (1 - p)^k p, \\quad \\text{per } k = 0, 1, 2, \\dots\n\\]\nValore atteso e varianza\nPer la variabile \\(T\\) (che inizia da 1):\n\\[\nE[T] = \\frac{1}{p}, \\quad \\operatorname{Var}(T) = \\frac{1 - p}{p^2}\n\\]\nPer la variabile \\(Y = T - 1\\) (che inizia da 0):\n\\[\nE[Y] = \\frac{1 - p}{p}, \\quad \\operatorname{Var}(Y) = \\frac{1 - p}{p^2}\n\\]\nLa distribuzione geometrica è l’unica distribuzione discreta che gode della proprietà di assenza di memoria:\n\\[\nP(T &gt; m + n \\mid T &gt; m) = P(T &gt; n), \\quad \\text{per ogni } m,n \\in \\mathbb{N}\n\\]\nInoltre, la distribuzione geometrica:\n\nÈ collegata al processo di Bernoulli, in cui si eseguono prove ripetute fino al primo successo.\nÈ un caso particolare della binomiale negativa con un solo successo.\nÈ l’analogo discreto della distribuzione esponenziale, che ha la stessa proprietà di memoria nulla.\n\n\n\n\nLa variabile aleatoria \\(X\\) segue una distribuzione ipergeometrica se rappresenta il numero di successi ottenuti estraendo senza reinserimento da una popolazione finita composta da successi e insuccessi.\nSupponiamo di avere: - una popolazione di dimensione \\(N\\), - che contiene \\(K\\) elementi di tipo “successo” (e quindi \\(N - K\\) di tipo “insuccesso”), - da cui si estraggono \\(n\\) elementi senza reinserimento,\nallora \\(X\\) è il numero di successi nelle \\(n\\) estrazioni.\nFunzione di massa di probabilità (pmf)\n\\[\nP(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n - k}}{\\binom{N}{n}}\n\\]\ndove\n\\[\n\\max(0,\\; n - (N - K)) \\le k \\le \\min(K,\\; n)\n\\]\nValore atteso e varianza\n\\[\nE[X] = n \\cdot \\frac{K}{N}\n\\]\n\\[\n\\operatorname{Var}(X) = n \\cdot \\frac{K}{N} \\cdot \\left(1 - \\frac{K}{N}\\right) \\cdot \\frac{N - n}{N - 1}\n\\]\nA differenza della distribuzione binomiale:\n\nNella binomiale le prove sono indipendenti (estrazione con reinserimento).\nNella ipergeometrica le estrazioni sono senza reinserimento, quindi dipendenti.\nSe \\(N\\) è molto grande rispetto a \\(n\\), l’ipergometrica si approssima alla binomiale con \\(p = \\frac{K}{N}\\).\n\n\n\n\nUna variabile aleatoria \\(X\\) si dice di binomiale negativa se rappresenta il numero di fallimenti prima di ottenere \\(r\\) successi in una sequenza di prove di Bernoulli indipendenti, ciascuna con probabilità di successo \\(p \\in (0, 1]\\). Si indica con:\n\\[\nX \\sim \\operatorname{NB}(r, p)\n\\]\nIl supporto di \\(X\\) è l’insieme \\(\\{0, 1, 2, \\dots\\}\\).\nLa funzione di massa di probabilità è:\n\\[\nP(X = k) = \\binom{k + r - 1}{r - 1} p^r (1 - p)^k, \\quad k = 0, 1, 2, \\dots\n\\]\ndove: - \\(k\\) è il numero di fallimenti, - \\(r\\) è il numero di successi desiderati.\nIl valore atteso è:\n\\[\nE[X] = \\frac{r(1 - p)}{p}\n\\]\nLa varianza è:\n\\[\n\\operatorname{Var}(X) = \\frac{r(1 - p)}{p^2}\n\\]\nLa distribuzione geometrica è un caso particolare della binomiale negativa con \\(r = 1\\).\nInterpretazione alternativa\nIn alcuni contesti si considera una variabile \\(Y = X + r\\), che rappresenta il numero totale di prove necessarie per ottenere \\(r\\) successi. In tal caso, la funzione di massa è:\n\\[\nP(Y = n) = \\binom{n - 1}{r - 1} p^r (1 - p)^{n - r}, \\quad n = r, r + 1, r + 2, \\dots\n\\]\nIl valore atteso e la varianza di \\(Y\\) sono:\n\\[\nE[Y] = \\frac{r}{p}, \\qquad \\operatorname{Var}(Y) = \\frac{r(1 - p)}{p^2}\n\\]\n\n\n\nLa distribuzione Gamma-Poisson è una mistura tra una distribuzione di Poisson e una distribuzione Gamma. Si ottiene considerando che il parametro \\(\\lambda\\) della Poisson non è fisso, ma segue una distribuzione Gamma.\nFormalmente, si definisce una variabile aleatoria \\(X\\) come:\n\\[\nX \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda), \\quad \\lambda \\sim \\operatorname{Gamma}(r, \\theta)\n\\]\ndove: - \\(r &gt; 0\\) è il parametro di forma della Gamma, - \\(\\theta &gt; 0\\) è il parametro di scala della Gamma (oppure \\(\\beta = 1/\\theta\\) se si usa la parametrizzazione alternativa con il parametro di tasso).\nDistribuzione marginale di \\(X\\)\nIntegrando su \\(\\lambda\\), la distribuzione marginale di \\(X\\) è la binomiale negativa:\n\\[\nX \\sim \\operatorname{NB}(r, p), \\quad \\text{con } p = \\frac{\\theta}{1 + \\theta}\n\\]\noppure, invertendo la formula:\n\\[\n\\theta = \\frac{p}{1 - p}\n\\]\nLa funzione di massa di probabilità della distribuzione marginale è:\n\\[\nP(X = k) = \\binom{k + r - 1}{k} \\left( \\frac{\\theta}{1 + \\theta} \\right)^r \\left( \\frac{1}{1 + \\theta} \\right)^k, \\quad k = 0, 1, 2, \\dots\n\\]\noppure, in termini di \\(p\\):\n\\[\nP(X = k) = \\binom{k + r - 1}{k} (1 - p)^r p^k\n\\]\nIl valore atteso di \\(X\\) è:\n\\[\nE[X] = r \\cdot \\theta\n\\]\nLa varianza è:\n\\[\n\\operatorname{Var}(X) = r \\cdot \\theta (1 + \\theta)\n\\]\noppure, usando \\(p = \\theta / (1 + \\theta)\\):\n\\[\nE[X] = \\frac{r(1 - p)}{p}, \\qquad \\operatorname{Var}(X) = \\frac{r(1 - p)}{p^2}\n\\]\nLa Gamma-Poisson modella conteggi dove il tasso \\(\\lambda\\) non è fisso ma casuale, diverso da soggetto a soggetto:\n\\[\n\\lambda \\sim \\operatorname{Gamma}(r, \\theta), \\quad X \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda)\n\\]\nQuesta eterogeneità tra unità spiega perché:\n\nla media è \\(E[X] = r\\theta\\),\nma la varianza è più grande: \\(\\operatorname{Var}(X) = r\\theta(1 + \\theta)\\).\n\nLa Poisson ha \\(\\operatorname{Var}(X) = E[X]\\) (dispersione fissa), mentre qui si ha sovradispersione dovuta alla variabilità di \\(\\lambda\\).\n\n\n\nUna variabile aleatoria \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) si dice gaussiana (o normale) di parametri \\(\\mu\\) e \\(\\sigma^2\\) se ha la seguente funzione di densità di probabilità:\n\\[\nf(x) = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\dfrac{(x - \\mu)^2}{2\\sigma^2} \\right).\n\\]\nLa funzione di densità è una curva a campana, detta curva di Gauss, simmetrica rispetto a \\(x = \\mu\\), con massimo in \\(x = \\mu\\) di altezza \\((\\sigma \\sqrt{2\\pi})^{-1} \\approx 0{,}399/\\sigma\\).\nIl valore atteso è \\(E[X] = \\mu\\), mentre la varianza è \\(\\operatorname{Var}(X) = \\sigma^2\\).\nNota. Il momento secondo è \\(E[X^2] = \\sigma^2 + \\mu^2\\).\nProposizione. Se \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) e \\(Y = \\alpha X + \\beta\\) con \\(\\alpha \\neq 0\\), allora \\(Y \\sim \\mathcal{N}(\\alpha \\mu + \\beta, \\alpha^2 \\sigma^2)\\).\nLa variabile standardizzata:\n\\[\nZ = \\dfrac{X - \\mu}{\\sigma}\n\\]\nsegue una distribuzione normale standard \\(\\mathcal{N}(0, 1)\\).\nIl grafico della funzione di densità di una normale standard mostra la classica forma a campana centrata in zero.\n\nLa funzione di ripartizione della normale standard è indicata con \\(\\Phi\\) ed è definita come:\n\\[\n\\Phi(x) = P(Z \\leq x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} e^{-y^2/2} \\, dy.\n\\]\nPoiché \\(Z = \\dfrac{X - \\mu}{\\sigma}\\), possiamo esprimere le probabilità relative a \\(X\\) in termini di \\(\\Phi\\):\n\\[\nP(X &lt; b) = \\Phi\\left( \\dfrac{b - \\mu}{\\sigma} \\right).\n\\]\nPer \\(a &lt; b\\):\n\\[\nP(a &lt; X &lt; b) = \\Phi\\left( \\dfrac{b - \\mu}{\\sigma} \\right) - \\Phi\\left( \\dfrac{a - \\mu}{\\sigma} \\right).\n\\]\nL’integrale che definisce \\(\\Phi(x)\\) non ha una soluzione analitica esatta; si utilizzano tabelle o approssimazioni numeriche.\n\nPoiché la normale standard è simmetrica rispetto a zero:\n\\[\n\\Phi(-x) = 1 - \\Phi(x).\n\\]\nPer ogni \\(\\alpha \\in (0, 1)\\), definiamo \\(z_\\alpha\\) come:\n\\[\nP(Z &gt; z_\\alpha) = \\alpha \\quad \\Rightarrow \\quad z_\\alpha = \\Phi^{-1}(1 - \\alpha).\n\\]\nIl quantile \\(k\\)-esimo della normale standard è il valore \\(m\\) tale che:\n\\[\n\\Phi(m) = \\dfrac{k}{100}.\n\\]\nPonendo \\(k = 100(1 - \\alpha)\\), otteniamo \\(z_\\alpha\\), indicando che la normale standard è inferiore a \\(z_\\alpha\\) nel \\(k\\%\\) dei casi.\n\n\n\nLe variabili aleatorie beta sono distribuite secondo la distribuzione beta, una distribuzione continua definita su un intervallo \\([0,1]\\) e caratterizzata da due parametri positivi \\(\\alpha\\) e \\(\\beta\\). La funzione di densità di probabilità (pdf) della distribuzione beta è data da:\n\\(f_X(x) = \\dfrac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}, \\quad 0 \\leq x \\leq 1,\\)\ndove il denominatore è la funzione beta, definita come:\n\\(B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha - 1} (1 - t)^{\\beta - 1} dt\\)\nQuesta funzione normalizza la densità affinché l’area totale sotto la curva sia uguale a \\(1\\).\nIl valore atteso della distribuzione Beta è dato da:\n\\(E[X] = \\dfrac{\\alpha}{\\alpha + \\beta}\\)\nLa varianza è:\n\\(\\operatorname{Var}(X) = \\dfrac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\)\nLa distribuzione Beta è molto flessibile perché, in base ai valori di \\(\\alpha\\) e \\(\\beta\\), può assumere forme molto diverse.\n\nPer esempio, se \\(\\alpha = \\beta = 1\\), la distribuzione diventa uniforme su \\([0,1]\\). Se \\(\\alpha &gt; 1\\) e \\(\\beta &gt; 1\\), la distribuzione è concentrata al centro. Se invece uno dei due parametri è minore di \\(1\\), la distribuzione diventa asimmetrica, con una maggiore probabilità vicino a \\(0\\) o \\(1\\).\n\n\n\nUna variabile aleatoria continua \\(X\\) si dice esponenziale con parametro \\(\\lambda &gt; 0\\) se la sua funzione di densità di probabilità è:\n\\[\nf(x) = \\begin{cases}\n\\lambda e^{-\\lambda x} & \\text{se } x \\geq 0, \\\\\n0 & \\text{se } x &lt; 0.\n\\end{cases}\n\\]\nLa funzione di ripartizione è:\n\\[\nF(x) = P(X \\leq x) = \\begin{cases}\n1 - e^{-\\lambda x} & \\text{se } x \\geq 0, \\\\\n0 & \\text{se } x &lt; 0.\n\\end{cases}\n\\]\nLa distribuzione esponenziale modella tipicamente il tempo di attesa prima che si verifichi un evento casuale. Il suo valore atteso è:\n\\[\nE[X] = \\dfrac{1}{\\lambda},\n\\]\nil momento secondo è:\n\\[\nE[X^2] = \\dfrac{2}{\\lambda^2},\n\\]\ne la varianza è:\n\\[\n\\operatorname{Var}(X) = \\dfrac{1}{\\lambda^2}.\n\\]\nLa proprietà fondamentale è l’assenza di memoria:\n\\[\nP(X &gt; s + t \\mid X &gt; t) = P(X &gt; s), \\quad \\forall s, t \\geq 0.\n\\]\nEsempio. Se \\(X\\) rappresenta il tempo di vita di un oggetto, sapendo che ha già funzionato per un tempo \\(t\\), la probabilità che continui a funzionare per un ulteriore tempo \\(s\\) è la stessa che avrebbe avuto all’inizio.\nPer la distribuzione esponenziale, vale:\n\\[\nP(X &gt; s + t) = P(X &gt; s) P(X &gt; t).\n\\]\nQuesto riflette la proprietà di assenza di memoria, poiché la probabilità che l’evento non si sia verificato entro \\(s + t\\) è il prodotto delle probabilità di non verificarsi in \\(s\\) e \\(t\\).\n\n\n\n\n\nUna variabile aleatoria \\(Y\\) si dice di distribuzione lognormale se la variabile \\(\\ln(Y)\\) segue una distribuzione normale con parametri \\(\\mu\\) e \\(\\sigma^2\\). In altre parole, se \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), allora \\(Y = e^X\\) si dice lognormale. La funzione di densità di probabilità di \\(Y\\), per \\(y &gt; 0\\), è data da\n\\(f_Y(y) = \\dfrac{1}{y\\,\\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\dfrac{(\\ln(y) - \\mu)^2}{2\\sigma^2}\\right)\\)\n\nIl suo valore atteso si calcola come \\(E[Y] = e^{\\mu + \\frac{\\sigma^2}{2}}\\), mentre la sua varianza risulta \\(\\operatorname{Var}(Y) = \\left(e^{\\sigma^2} - 1\\right)\\,e^{2\\mu + \\sigma^2}\\)\n\n\n\nLa distribuzione Gamma caratterizza una variabile aleatoria definita per valori positivi ed è spesso parametrizzata con un parametro di forma \\(\\alpha\\) e un parametro di tasso \\(\\beta\\). In questo caso, la funzione di densità di probabilità, per \\(x &gt; 0\\), è\n\\[f_X(x) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\, x^{\\alpha - 1} e^{-\\beta x}\\] Il suo valore atteso è \\(E[X] = \\dfrac{\\alpha}{\\beta}\\), mentre la sua varianza è \\(\\operatorname{Var}(X) = \\dfrac{\\alpha}{\\beta^2}\\). Questa distribuzione è utile per descrivere tempi di attesa fino al verificarsi di un certo numero di eventi in processi di Poisson e, in generale, per modellare fenomeni positivi con variabilità asimmetrica.\nNota. Talvolta, la distribuzione Gamma è espressa in funzione dei parametri \\(k\\) e \\(\\theta\\), dove \\(\\displaystyle \\alpha =k\\) e \\(\\displaystyle \\beta =1/\\theta\\).\n\nLa distribuzione \\(\\chi^2\\) (chi-quadro) è un caso particolare della gamma in cui \\(\\alpha = \\dfrac{k}{2}\\) e \\(\\beta = \\dfrac{1}{2}\\), dove \\(k\\) rappresenta i gradi di libertà. In tale situazione, la funzione di densità risulta\n\\(\\displaystyle f_X(x) = \\frac{1}{2^{k/2}\\,\\Gamma(\\dfrac{k}{2})} \\, x^{({k}/{2}) - 1} e^{-{x}/{2}}\\)\nper \\(x&gt;0\\). Il suo valore atteso è \\(E[X] = k\\), la varianza è \\(\\operatorname{Var}(X) = 2k\\). Questa distribuzione è fondamentale in statistica per definire test di ipotesi e intervalli di confidenza relativi, ad esempio, a varianze e regressioni.\n\nLa distribuzione Erlang è anch’essa un caso speciale di gamma in cui il parametro di forma \\(\\alpha\\) è un intero positivo \\(k\\). Con un parametro di tasso \\(\\lambda\\), la funzione di densità si esprime come\n\\(\\displaystyle f_X(x) = \\dfrac{\\lambda^k}{(k-1)!}\\, x^{k-1}\\, e^{-\\lambda x}\\)\nper \\(x&gt;0\\). Il suo valore atteso è \\(E[X] = \\frac{k}{\\lambda}\\), mentre la varianza è \\(\\operatorname{Var}(X) = \\dfrac{k}{\\lambda^2}\\). Si ottiene come somma di \\(k\\) variabili esponenziali indipendenti, tutte con lo stesso parametro \\(\\lambda\\), ed è comunemente impiegata per analizzare tempi di attesa in processi di natura “conta-eventi” (processi di Poisson).\n\n\n\n\nLa distribuzione t di Student è una distribuzione di probabilità continua utilizzata principalmente nelle inferenze statistiche, in particolare nei test di ipotesi per campioni di dimensioni ridotte. È caratterizzata da un parametro detto gradi di libertà (\\(\\nu\\)), che influisce sulla forma della distribuzione.\nLa funzione di densità di probabilità (pdf) della distribuzione t di Student è\n\\(f_X(x) = \\dfrac{\\Gamma\\left(\\dfrac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi}\\ \\Gamma\\left(\\dfrac{\\nu}{2}\\right)} \\left(1 + \\dfrac{x^2}{\\nu} \\right)^{-\\dfrac{\\nu+1}{2}}, \\quad -\\infty &lt; x &lt; \\infty.\\)\nQui, \\(\\Gamma(\\cdot)\\) rappresenta la funzione Gamma.\n\nLa distribuzione t di Student ha media:\n\\(E[X] = 0, \\quad \\text{per } \\nu &gt; 1.\\)\nLa varianza è:\n\\(\\operatorname{Var}(X) = \\dfrac{\\nu}{\\nu - 2}, \\quad \\text{per } \\nu &gt; 2.\\)\nPer \\(\\nu \\leq 2\\), la varianza non è definita, mentre per \\(\\nu \\leq 1\\), la media non è definita.\nLa distribuzione t di Student assomiglia alla distribuzione normale standard, ma ha code più pesanti, il che significa che assegna una maggiore probabilità a valori estremi. Questo comportamento è particolarmente utile quando si lavora con campioni di piccole dimensioni, dove la distribuzione dei dati potrebbe non essere perfettamente normale.\nUn’applicazione fondamentale della distribuzione t di Student è nel test t di Student, che viene utilizzato per confrontare la media di un campione con un valore noto o per confrontare le medie di due campioni. In generale, quando la dimensione del campione è grande (\\(\\nu \\to \\infty\\)), la distribuzione t di Student si avvicina alla distribuzione normale standard.\nQuesta distribuzione è molto utilizzata in statistica inferenziale per stimare intervalli di confidenza e per test di significatività quando la varianza della popolazione non è nota.\n\n\n\nLa distribuzione F di Fisher è una distribuzione di probabilità continua utilizzata principalmente nei test di ipotesi statistici per confrontare le varianze di due popolazioni. È particolarmente importante nell’analisi della varianza (ANOVA) e nei test F, che servono a determinare se due campioni provengono da popolazioni con la stessa varianza.\nLa distribuzione F di Fisher dipende da due parametri, detti gradi di libertà: \\(d_1\\) per il numeratore e \\(d_2\\) per il denominatore. La funzione di densità di probabilità (pdf) è:\n\\(f_X(x) = \\dfrac{\\left(\\dfrac{d_1}{d_2}\\right)^{\\dfrac{d_1}{2}} x^{\\dfrac{d_1}{2} - 1}}{B\\left(\\dfrac{d_1}{2}, \\dfrac{d_2}{2}\\right)} \\left(1 + \\dfrac{d_1}{d_2} x\\right)^{-\\dfrac{d_1 + d_2}{2}}, \\quad x &gt; 0.\\)\ndove \\(B(a, b)\\) è la funzione Beta.\n\nLa distribuzione F è sempre definita per valori positivi \\(x &gt; 0\\), perché rappresenta il rapporto tra due varianze stimate da campioni casuali. Il valore atteso della distribuzione è dato da:\n\\(E[X] = \\dfrac{d_2}{d_2 - 2}, \\quad \\text{per } d_2 &gt; 2\\)\nLa varianza è\n\\(\\operatorname{Var}(X) = \\dfrac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}, \\quad \\text{per } d_2 &gt; 4\\)\nSe \\(d_2 \\leq 2\\), la media non è definita, mentre se \\(d_2 \\leq 4\\), la varianza non è definita.\nLa distribuzione F di Fisher è ottenuta come il rapporto tra due variabili aleatorie chi-quadrato indipendenti, normalizzate rispetto ai rispettivi gradi di libertà:\n\\(F = {\\dfrac{S_1^2}{d_1}}/{\\dfrac{S_2^2}{d_2}}.\\)\ndove \\(S_1^2\\) e \\(S_2^2\\) sono varianze campionarie.\nLa distribuzione F è asimmetrica e sempre positiva, con una coda più lunga a destra. Man mano che i gradi di libertà aumentano, la distribuzione si avvicina a una distribuzione normale.",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-di-bernoulli",
    "href": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-di-bernoulli",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria \\(X\\) si dice di Bernoulli se può assumere solo i valori \\(0\\) e \\(1\\), con probabilità rispettivamente \\(1 - p\\) e \\(p\\), dove \\(p \\in [0, 1]\\). La sua funzione di massa di probabilità è:\n\\[\nP(X = x) = \\begin{cases}\np & \\text{se } x = 1, \\\\\n1 - p & \\text{se } x = 0.\n\\end{cases}\n\\]\nIl suo valore atteso è \\(E[X] = p\\), mentre la sua varianza è \\(\\operatorname{Var}(X) = p(1 - p)\\).\nUn processo di Bernoulli è una successione di variabili aleatorie indipendenti \\(X_i\\) con uguale distribuzione di Bernoulli \\(\\mathcal{B}(p)\\).\nLa distribuzione binomiale descrive la probabilità del numero di successi in \\(n\\) prove di Bernoulli indipendenti, ovvero della variabile aleatoria:\n\\[\nS_n = X_1 + X_2 + \\dots + X_n.\n\\]",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-binomiali",
    "href": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-binomiali",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Quando si effettuano \\(n\\) ripetizioni indipendenti di un esperimento binario, ciascuna con probabilità di successo \\(p\\) e di fallimento \\(1 - p\\), il numero totale di successi \\(S_n\\) è una variabile aleatoria binomiale \\(\\mathcal{B}(n, p)\\).\nIl coefficiente binomiale è definito come:\n\\[\n\\binom{n}{i} = \\dfrac{n!}{i! (n - i)!}.\n\\]\nLa funzione di massa di probabilità di una variabile aleatoria binomiale è:\n\\[\nP(S_n = i) = \\binom{n}{i} p^i (1 - p)^{n - i}, \\quad \\text{per } i = 0, 1, \\dots, n.\n\\]\nIl suo valore atteso è:\n\\[\nE[S_n] = n p,\n\\]\nmentre la sua varianza è:\n\\[\n\\operatorname{Var}(S_n) = n p (1 - p).\n\\]\nLa figura rappresenta il grafico della funzione di massa di una variabile binomiale con parametri \\(n = 10\\) e \\(p = 0{,}5\\), che risulta simmetrica rispetto al valore medio.",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-di-poisson",
    "href": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-di-poisson",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria \\(X\\) che assume valori interi non negativi \\(i = 0, 1, 2, \\dots\\) si dice di Poisson di parametro \\(\\lambda &gt; 0\\) se la sua funzione di massa di probabilità è:\n\\[\nP(X = i) = \\dfrac{e^{-\\lambda} \\lambda^i}{i!}.\n\\]\nIl parametro \\(\\lambda\\) rappresenta sia il valore atteso \\(E[X] = \\lambda\\) sia la varianza \\(\\operatorname{Var}(X) = \\lambda\\).\nNota. Le variabili di Poisson vengono spesso utilizzate come approssimazione delle variabili binomiali \\(\\mathcal{B}(n, p)\\) quando \\(n\\) è molto grande e \\(p\\) è molto piccolo, mantenendo \\(\\lambda = n p\\) costante.\nLa figura illustra il grafico della funzione di massa di una variabile di Poisson con parametro \\(\\lambda = 4\\).\n\nNota. La distribuzione di Poisson viene spesso utilizzata per modellare il numero di eventi in un intervallo di tempo, dato un tasso medio di occorrenza \\(\\lambda\\)",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#processi-di-poisson",
    "href": "th/03. Modelli di variabili aleatorie.html#processi-di-poisson",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Un processo di Poisson con tasso \\(\\lambda &gt; 0\\) descrive il verificarsi di eventi indipendenti nel tempo con un ritmo medio costante. Se \\(N(t)\\) indica il numero di eventi avvenuti nell’intervallo di tempo \\([0,t]\\), allora \\(N(t)\\) segue una distribuzione di Poisson con parametro \\(\\lambda t\\), vale a dire:\n\\(P\\left(N(t) = k\\right) = \\dfrac{(\\lambda t)^k}{k!}\\,e^{-\\lambda t}, \\quad k = 0,1,2,\\ldots\\)\nIl suo valore atteso è \\(E[N(t)] = \\lambda t\\), mentre la varianza è \\(\\operatorname{Var}(N(t)) = \\lambda t\\). I tempi di attesa fra un evento e il successivo, invece, seguono una distribuzione esponenziale con lo stesso parametro \\(\\lambda\\). Uno dei tratti distintivi di questo modello è il suo carattere “senza memoria”, per cui la probabilità di un evento futuro non dipende da quanto tempo è già trascorso.",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-uniformi",
    "href": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-uniformi",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria continua \\(X\\) si dice uniformemente distribuita sull’intervallo \\([a,b]\\)(con \\(a &lt; b\\)) se la sua funzione di densità di probabilità (pdf) è costante su \\([a,b]\\) e nulla altrove. Formalmente,\n\\(f_X(x) = \\begin{cases} \\dfrac{1}{b - a}, & x \\in [a, b],\\\\ 0, & \\text{altrimenti}. \\end{cases}\\)\nLa distribuzione uniforme è così chiamata perché ogni valore all’interno dell’intervallo \\([a, b]\\) è ugualmente probabile.\n\nIl valore atteso della variabile è dato da \\(E[X] = \\dfrac{a + b}{2}\\), mentre la varianza è \\(\\operatorname{Var}(X) = \\dfrac{(b - a)^2}{12}\\).\nOltre alla versione continua, esiste anche una versione discreta della distribuzione uniforme. Se \\(X\\) può assumere \\(n\\) valori distinti \\(\\{1, 2, \\dots, n\\}\\) con la stessa probabilità, allora la funzione di massa di probabilità è\n\\(P(X = k) = \\frac{1}{n}, \\quad k = 1, 2, \\dots, n.\\)\nIn questo caso, il valore atteso è \\(E[X] = \\dfrac{n+1}{2}\\) e la varianza è \\(\\operatorname{Var}(X) = \\dfrac{n^2 - 1}{12}\\).\nQuesta distribuzione è spesso utilizzata quando si vuole assegnare la stessa probabilità a tutti i possibili valori di una variabile, senza favorirne nessuno.",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#distribuzione-geometrica",
    "href": "th/03. Modelli di variabili aleatorie.html#distribuzione-geometrica",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria \\(T\\) si dice di distribuzione geometrica, con parametro \\(p \\in (0,1]\\), se rappresenta il numero di prove necessarie fino al primo successo in un processo di Bernoulli, cioè una sequenza di prove indipendenti con probabilità di successo costante \\(p\\).\nFunzione di massa di probabilità (pmf)\nSe \\(T\\) rappresenta il numero della prima prova che ha successo, allora:\n\\[\nP(T = k) = (1 - p)^{k - 1} p, \\quad \\text{per } k = 1, 2, 3, \\dots\n\\]\nIn alternativa, se si considera \\(Y = T - 1\\), che rappresenta il numero di fallimenti prima del primo successo, allora:\n\\[\nP(Y = k) = (1 - p)^k p, \\quad \\text{per } k = 0, 1, 2, \\dots\n\\]\nValore atteso e varianza\nPer la variabile \\(T\\) (che inizia da 1):\n\\[\nE[T] = \\frac{1}{p}, \\quad \\operatorname{Var}(T) = \\frac{1 - p}{p^2}\n\\]\nPer la variabile \\(Y = T - 1\\) (che inizia da 0):\n\\[\nE[Y] = \\frac{1 - p}{p}, \\quad \\operatorname{Var}(Y) = \\frac{1 - p}{p^2}\n\\]\nLa distribuzione geometrica è l’unica distribuzione discreta che gode della proprietà di assenza di memoria:\n\\[\nP(T &gt; m + n \\mid T &gt; m) = P(T &gt; n), \\quad \\text{per ogni } m,n \\in \\mathbb{N}\n\\]\nInoltre, la distribuzione geometrica:\n\nÈ collegata al processo di Bernoulli, in cui si eseguono prove ripetute fino al primo successo.\nÈ un caso particolare della binomiale negativa con un solo successo.\nÈ l’analogo discreto della distribuzione esponenziale, che ha la stessa proprietà di memoria nulla.",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#distribuzione-ipergeometrica",
    "href": "th/03. Modelli di variabili aleatorie.html#distribuzione-ipergeometrica",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "La variabile aleatoria \\(X\\) segue una distribuzione ipergeometrica se rappresenta il numero di successi ottenuti estraendo senza reinserimento da una popolazione finita composta da successi e insuccessi.\nSupponiamo di avere: - una popolazione di dimensione \\(N\\), - che contiene \\(K\\) elementi di tipo “successo” (e quindi \\(N - K\\) di tipo “insuccesso”), - da cui si estraggono \\(n\\) elementi senza reinserimento,\nallora \\(X\\) è il numero di successi nelle \\(n\\) estrazioni.\nFunzione di massa di probabilità (pmf)\n\\[\nP(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n - k}}{\\binom{N}{n}}\n\\]\ndove\n\\[\n\\max(0,\\; n - (N - K)) \\le k \\le \\min(K,\\; n)\n\\]\nValore atteso e varianza\n\\[\nE[X] = n \\cdot \\frac{K}{N}\n\\]\n\\[\n\\operatorname{Var}(X) = n \\cdot \\frac{K}{N} \\cdot \\left(1 - \\frac{K}{N}\\right) \\cdot \\frac{N - n}{N - 1}\n\\]\nA differenza della distribuzione binomiale:\n\nNella binomiale le prove sono indipendenti (estrazione con reinserimento).\nNella ipergeometrica le estrazioni sono senza reinserimento, quindi dipendenti.\nSe \\(N\\) è molto grande rispetto a \\(n\\), l’ipergometrica si approssima alla binomiale con \\(p = \\frac{K}{N}\\).",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#distribuzione-binomiale-negativa",
    "href": "th/03. Modelli di variabili aleatorie.html#distribuzione-binomiale-negativa",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria \\(X\\) si dice di binomiale negativa se rappresenta il numero di fallimenti prima di ottenere \\(r\\) successi in una sequenza di prove di Bernoulli indipendenti, ciascuna con probabilità di successo \\(p \\in (0, 1]\\). Si indica con:\n\\[\nX \\sim \\operatorname{NB}(r, p)\n\\]\nIl supporto di \\(X\\) è l’insieme \\(\\{0, 1, 2, \\dots\\}\\).\nLa funzione di massa di probabilità è:\n\\[\nP(X = k) = \\binom{k + r - 1}{r - 1} p^r (1 - p)^k, \\quad k = 0, 1, 2, \\dots\n\\]\ndove: - \\(k\\) è il numero di fallimenti, - \\(r\\) è il numero di successi desiderati.\nIl valore atteso è:\n\\[\nE[X] = \\frac{r(1 - p)}{p}\n\\]\nLa varianza è:\n\\[\n\\operatorname{Var}(X) = \\frac{r(1 - p)}{p^2}\n\\]\nLa distribuzione geometrica è un caso particolare della binomiale negativa con \\(r = 1\\).\nInterpretazione alternativa\nIn alcuni contesti si considera una variabile \\(Y = X + r\\), che rappresenta il numero totale di prove necessarie per ottenere \\(r\\) successi. In tal caso, la funzione di massa è:\n\\[\nP(Y = n) = \\binom{n - 1}{r - 1} p^r (1 - p)^{n - r}, \\quad n = r, r + 1, r + 2, \\dots\n\\]\nIl valore atteso e la varianza di \\(Y\\) sono:\n\\[\nE[Y] = \\frac{r}{p}, \\qquad \\operatorname{Var}(Y) = \\frac{r(1 - p)}{p^2}\n\\]",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#distribuzione-gamma-poisson",
    "href": "th/03. Modelli di variabili aleatorie.html#distribuzione-gamma-poisson",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "La distribuzione Gamma-Poisson è una mistura tra una distribuzione di Poisson e una distribuzione Gamma. Si ottiene considerando che il parametro \\(\\lambda\\) della Poisson non è fisso, ma segue una distribuzione Gamma.\nFormalmente, si definisce una variabile aleatoria \\(X\\) come:\n\\[\nX \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda), \\quad \\lambda \\sim \\operatorname{Gamma}(r, \\theta)\n\\]\ndove: - \\(r &gt; 0\\) è il parametro di forma della Gamma, - \\(\\theta &gt; 0\\) è il parametro di scala della Gamma (oppure \\(\\beta = 1/\\theta\\) se si usa la parametrizzazione alternativa con il parametro di tasso).\nDistribuzione marginale di \\(X\\)\nIntegrando su \\(\\lambda\\), la distribuzione marginale di \\(X\\) è la binomiale negativa:\n\\[\nX \\sim \\operatorname{NB}(r, p), \\quad \\text{con } p = \\frac{\\theta}{1 + \\theta}\n\\]\noppure, invertendo la formula:\n\\[\n\\theta = \\frac{p}{1 - p}\n\\]\nLa funzione di massa di probabilità della distribuzione marginale è:\n\\[\nP(X = k) = \\binom{k + r - 1}{k} \\left( \\frac{\\theta}{1 + \\theta} \\right)^r \\left( \\frac{1}{1 + \\theta} \\right)^k, \\quad k = 0, 1, 2, \\dots\n\\]\noppure, in termini di \\(p\\):\n\\[\nP(X = k) = \\binom{k + r - 1}{k} (1 - p)^r p^k\n\\]\nIl valore atteso di \\(X\\) è:\n\\[\nE[X] = r \\cdot \\theta\n\\]\nLa varianza è:\n\\[\n\\operatorname{Var}(X) = r \\cdot \\theta (1 + \\theta)\n\\]\noppure, usando \\(p = \\theta / (1 + \\theta)\\):\n\\[\nE[X] = \\frac{r(1 - p)}{p}, \\qquad \\operatorname{Var}(X) = \\frac{r(1 - p)}{p^2}\n\\]\nLa Gamma-Poisson modella conteggi dove il tasso \\(\\lambda\\) non è fisso ma casuale, diverso da soggetto a soggetto:\n\\[\n\\lambda \\sim \\operatorname{Gamma}(r, \\theta), \\quad X \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda)\n\\]\nQuesta eterogeneità tra unità spiega perché:\n\nla media è \\(E[X] = r\\theta\\),\nma la varianza è più grande: \\(\\operatorname{Var}(X) = r\\theta(1 + \\theta)\\).\n\nLa Poisson ha \\(\\operatorname{Var}(X) = E[X]\\) (dispersione fissa), mentre qui si ha sovradispersione dovuta alla variabilità di \\(\\lambda\\).",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-gaussiane",
    "href": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-gaussiane",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) si dice gaussiana (o normale) di parametri \\(\\mu\\) e \\(\\sigma^2\\) se ha la seguente funzione di densità di probabilità:\n\\[\nf(x) = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\dfrac{(x - \\mu)^2}{2\\sigma^2} \\right).\n\\]\nLa funzione di densità è una curva a campana, detta curva di Gauss, simmetrica rispetto a \\(x = \\mu\\), con massimo in \\(x = \\mu\\) di altezza \\((\\sigma \\sqrt{2\\pi})^{-1} \\approx 0{,}399/\\sigma\\).\nIl valore atteso è \\(E[X] = \\mu\\), mentre la varianza è \\(\\operatorname{Var}(X) = \\sigma^2\\).\nNota. Il momento secondo è \\(E[X^2] = \\sigma^2 + \\mu^2\\).\nProposizione. Se \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) e \\(Y = \\alpha X + \\beta\\) con \\(\\alpha \\neq 0\\), allora \\(Y \\sim \\mathcal{N}(\\alpha \\mu + \\beta, \\alpha^2 \\sigma^2)\\).\nLa variabile standardizzata:\n\\[\nZ = \\dfrac{X - \\mu}{\\sigma}\n\\]\nsegue una distribuzione normale standard \\(\\mathcal{N}(0, 1)\\).\nIl grafico della funzione di densità di una normale standard mostra la classica forma a campana centrata in zero.\n\nLa funzione di ripartizione della normale standard è indicata con \\(\\Phi\\) ed è definita come:\n\\[\n\\Phi(x) = P(Z \\leq x) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} e^{-y^2/2} \\, dy.\n\\]\nPoiché \\(Z = \\dfrac{X - \\mu}{\\sigma}\\), possiamo esprimere le probabilità relative a \\(X\\) in termini di \\(\\Phi\\):\n\\[\nP(X &lt; b) = \\Phi\\left( \\dfrac{b - \\mu}{\\sigma} \\right).\n\\]\nPer \\(a &lt; b\\):\n\\[\nP(a &lt; X &lt; b) = \\Phi\\left( \\dfrac{b - \\mu}{\\sigma} \\right) - \\Phi\\left( \\dfrac{a - \\mu}{\\sigma} \\right).\n\\]\nL’integrale che definisce \\(\\Phi(x)\\) non ha una soluzione analitica esatta; si utilizzano tabelle o approssimazioni numeriche.\n\nPoiché la normale standard è simmetrica rispetto a zero:\n\\[\n\\Phi(-x) = 1 - \\Phi(x).\n\\]\nPer ogni \\(\\alpha \\in (0, 1)\\), definiamo \\(z_\\alpha\\) come:\n\\[\nP(Z &gt; z_\\alpha) = \\alpha \\quad \\Rightarrow \\quad z_\\alpha = \\Phi^{-1}(1 - \\alpha).\n\\]\nIl quantile \\(k\\)-esimo della normale standard è il valore \\(m\\) tale che:\n\\[\n\\Phi(m) = \\dfrac{k}{100}.\n\\]\nPonendo \\(k = 100(1 - \\alpha)\\), otteniamo \\(z_\\alpha\\), indicando che la normale standard è inferiore a \\(z_\\alpha\\) nel \\(k\\%\\) dei casi.",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-beta",
    "href": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-beta",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Le variabili aleatorie beta sono distribuite secondo la distribuzione beta, una distribuzione continua definita su un intervallo \\([0,1]\\) e caratterizzata da due parametri positivi \\(\\alpha\\) e \\(\\beta\\). La funzione di densità di probabilità (pdf) della distribuzione beta è data da:\n\\(f_X(x) = \\dfrac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}, \\quad 0 \\leq x \\leq 1,\\)\ndove il denominatore è la funzione beta, definita come:\n\\(B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha - 1} (1 - t)^{\\beta - 1} dt\\)\nQuesta funzione normalizza la densità affinché l’area totale sotto la curva sia uguale a \\(1\\).\nIl valore atteso della distribuzione Beta è dato da:\n\\(E[X] = \\dfrac{\\alpha}{\\alpha + \\beta}\\)\nLa varianza è:\n\\(\\operatorname{Var}(X) = \\dfrac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\)\nLa distribuzione Beta è molto flessibile perché, in base ai valori di \\(\\alpha\\) e \\(\\beta\\), può assumere forme molto diverse.\n\nPer esempio, se \\(\\alpha = \\beta = 1\\), la distribuzione diventa uniforme su \\([0,1]\\). Se \\(\\alpha &gt; 1\\) e \\(\\beta &gt; 1\\), la distribuzione è concentrata al centro. Se invece uno dei due parametri è minore di \\(1\\), la distribuzione diventa asimmetrica, con una maggiore probabilità vicino a \\(0\\) o \\(1\\).",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-esponenziali",
    "href": "th/03. Modelli di variabili aleatorie.html#variabili-aleatorie-esponenziali",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria continua \\(X\\) si dice esponenziale con parametro \\(\\lambda &gt; 0\\) se la sua funzione di densità di probabilità è:\n\\[\nf(x) = \\begin{cases}\n\\lambda e^{-\\lambda x} & \\text{se } x \\geq 0, \\\\\n0 & \\text{se } x &lt; 0.\n\\end{cases}\n\\]\nLa funzione di ripartizione è:\n\\[\nF(x) = P(X \\leq x) = \\begin{cases}\n1 - e^{-\\lambda x} & \\text{se } x \\geq 0, \\\\\n0 & \\text{se } x &lt; 0.\n\\end{cases}\n\\]\nLa distribuzione esponenziale modella tipicamente il tempo di attesa prima che si verifichi un evento casuale. Il suo valore atteso è:\n\\[\nE[X] = \\dfrac{1}{\\lambda},\n\\]\nil momento secondo è:\n\\[\nE[X^2] = \\dfrac{2}{\\lambda^2},\n\\]\ne la varianza è:\n\\[\n\\operatorname{Var}(X) = \\dfrac{1}{\\lambda^2}.\n\\]\nLa proprietà fondamentale è l’assenza di memoria:\n\\[\nP(X &gt; s + t \\mid X &gt; t) = P(X &gt; s), \\quad \\forall s, t \\geq 0.\n\\]\nEsempio. Se \\(X\\) rappresenta il tempo di vita di un oggetto, sapendo che ha già funzionato per un tempo \\(t\\), la probabilità che continui a funzionare per un ulteriore tempo \\(s\\) è la stessa che avrebbe avuto all’inizio.\nPer la distribuzione esponenziale, vale:\n\\[\nP(X &gt; s + t) = P(X &gt; s) P(X &gt; t).\n\\]\nQuesto riflette la proprietà di assenza di memoria, poiché la probabilità che l’evento non si sia verificato entro \\(s + t\\) è il prodotto delle probabilità di non verificarsi in \\(s\\) e \\(t\\).",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/03. Modelli di variabili aleatorie.html#distribuzioni-che-derivano-da-quella-normale",
    "href": "th/03. Modelli di variabili aleatorie.html#distribuzioni-che-derivano-da-quella-normale",
    "title": "Modelli di variabili aleatorie",
    "section": "",
    "text": "Una variabile aleatoria \\(Y\\) si dice di distribuzione lognormale se la variabile \\(\\ln(Y)\\) segue una distribuzione normale con parametri \\(\\mu\\) e \\(\\sigma^2\\). In altre parole, se \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), allora \\(Y = e^X\\) si dice lognormale. La funzione di densità di probabilità di \\(Y\\), per \\(y &gt; 0\\), è data da\n\\(f_Y(y) = \\dfrac{1}{y\\,\\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\dfrac{(\\ln(y) - \\mu)^2}{2\\sigma^2}\\right)\\)\n\nIl suo valore atteso si calcola come \\(E[Y] = e^{\\mu + \\frac{\\sigma^2}{2}}\\), mentre la sua varianza risulta \\(\\operatorname{Var}(Y) = \\left(e^{\\sigma^2} - 1\\right)\\,e^{2\\mu + \\sigma^2}\\)\n\n\n\nLa distribuzione Gamma caratterizza una variabile aleatoria definita per valori positivi ed è spesso parametrizzata con un parametro di forma \\(\\alpha\\) e un parametro di tasso \\(\\beta\\). In questo caso, la funzione di densità di probabilità, per \\(x &gt; 0\\), è\n\\[f_X(x) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\, x^{\\alpha - 1} e^{-\\beta x}\\] Il suo valore atteso è \\(E[X] = \\dfrac{\\alpha}{\\beta}\\), mentre la sua varianza è \\(\\operatorname{Var}(X) = \\dfrac{\\alpha}{\\beta^2}\\). Questa distribuzione è utile per descrivere tempi di attesa fino al verificarsi di un certo numero di eventi in processi di Poisson e, in generale, per modellare fenomeni positivi con variabilità asimmetrica.\nNota. Talvolta, la distribuzione Gamma è espressa in funzione dei parametri \\(k\\) e \\(\\theta\\), dove \\(\\displaystyle \\alpha =k\\) e \\(\\displaystyle \\beta =1/\\theta\\).\n\nLa distribuzione \\(\\chi^2\\) (chi-quadro) è un caso particolare della gamma in cui \\(\\alpha = \\dfrac{k}{2}\\) e \\(\\beta = \\dfrac{1}{2}\\), dove \\(k\\) rappresenta i gradi di libertà. In tale situazione, la funzione di densità risulta\n\\(\\displaystyle f_X(x) = \\frac{1}{2^{k/2}\\,\\Gamma(\\dfrac{k}{2})} \\, x^{({k}/{2}) - 1} e^{-{x}/{2}}\\)\nper \\(x&gt;0\\). Il suo valore atteso è \\(E[X] = k\\), la varianza è \\(\\operatorname{Var}(X) = 2k\\). Questa distribuzione è fondamentale in statistica per definire test di ipotesi e intervalli di confidenza relativi, ad esempio, a varianze e regressioni.\n\nLa distribuzione Erlang è anch’essa un caso speciale di gamma in cui il parametro di forma \\(\\alpha\\) è un intero positivo \\(k\\). Con un parametro di tasso \\(\\lambda\\), la funzione di densità si esprime come\n\\(\\displaystyle f_X(x) = \\dfrac{\\lambda^k}{(k-1)!}\\, x^{k-1}\\, e^{-\\lambda x}\\)\nper \\(x&gt;0\\). Il suo valore atteso è \\(E[X] = \\frac{k}{\\lambda}\\), mentre la varianza è \\(\\operatorname{Var}(X) = \\dfrac{k}{\\lambda^2}\\). Si ottiene come somma di \\(k\\) variabili esponenziali indipendenti, tutte con lo stesso parametro \\(\\lambda\\), ed è comunemente impiegata per analizzare tempi di attesa in processi di natura “conta-eventi” (processi di Poisson).\n\n\n\n\nLa distribuzione t di Student è una distribuzione di probabilità continua utilizzata principalmente nelle inferenze statistiche, in particolare nei test di ipotesi per campioni di dimensioni ridotte. È caratterizzata da un parametro detto gradi di libertà (\\(\\nu\\)), che influisce sulla forma della distribuzione.\nLa funzione di densità di probabilità (pdf) della distribuzione t di Student è\n\\(f_X(x) = \\dfrac{\\Gamma\\left(\\dfrac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi}\\ \\Gamma\\left(\\dfrac{\\nu}{2}\\right)} \\left(1 + \\dfrac{x^2}{\\nu} \\right)^{-\\dfrac{\\nu+1}{2}}, \\quad -\\infty &lt; x &lt; \\infty.\\)\nQui, \\(\\Gamma(\\cdot)\\) rappresenta la funzione Gamma.\n\nLa distribuzione t di Student ha media:\n\\(E[X] = 0, \\quad \\text{per } \\nu &gt; 1.\\)\nLa varianza è:\n\\(\\operatorname{Var}(X) = \\dfrac{\\nu}{\\nu - 2}, \\quad \\text{per } \\nu &gt; 2.\\)\nPer \\(\\nu \\leq 2\\), la varianza non è definita, mentre per \\(\\nu \\leq 1\\), la media non è definita.\nLa distribuzione t di Student assomiglia alla distribuzione normale standard, ma ha code più pesanti, il che significa che assegna una maggiore probabilità a valori estremi. Questo comportamento è particolarmente utile quando si lavora con campioni di piccole dimensioni, dove la distribuzione dei dati potrebbe non essere perfettamente normale.\nUn’applicazione fondamentale della distribuzione t di Student è nel test t di Student, che viene utilizzato per confrontare la media di un campione con un valore noto o per confrontare le medie di due campioni. In generale, quando la dimensione del campione è grande (\\(\\nu \\to \\infty\\)), la distribuzione t di Student si avvicina alla distribuzione normale standard.\nQuesta distribuzione è molto utilizzata in statistica inferenziale per stimare intervalli di confidenza e per test di significatività quando la varianza della popolazione non è nota.\n\n\n\nLa distribuzione F di Fisher è una distribuzione di probabilità continua utilizzata principalmente nei test di ipotesi statistici per confrontare le varianze di due popolazioni. È particolarmente importante nell’analisi della varianza (ANOVA) e nei test F, che servono a determinare se due campioni provengono da popolazioni con la stessa varianza.\nLa distribuzione F di Fisher dipende da due parametri, detti gradi di libertà: \\(d_1\\) per il numeratore e \\(d_2\\) per il denominatore. La funzione di densità di probabilità (pdf) è:\n\\(f_X(x) = \\dfrac{\\left(\\dfrac{d_1}{d_2}\\right)^{\\dfrac{d_1}{2}} x^{\\dfrac{d_1}{2} - 1}}{B\\left(\\dfrac{d_1}{2}, \\dfrac{d_2}{2}\\right)} \\left(1 + \\dfrac{d_1}{d_2} x\\right)^{-\\dfrac{d_1 + d_2}{2}}, \\quad x &gt; 0.\\)\ndove \\(B(a, b)\\) è la funzione Beta.\n\nLa distribuzione F è sempre definita per valori positivi \\(x &gt; 0\\), perché rappresenta il rapporto tra due varianze stimate da campioni casuali. Il valore atteso della distribuzione è dato da:\n\\(E[X] = \\dfrac{d_2}{d_2 - 2}, \\quad \\text{per } d_2 &gt; 2\\)\nLa varianza è\n\\(\\operatorname{Var}(X) = \\dfrac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}, \\quad \\text{per } d_2 &gt; 4\\)\nSe \\(d_2 \\leq 2\\), la media non è definita, mentre se \\(d_2 \\leq 4\\), la varianza non è definita.\nLa distribuzione F di Fisher è ottenuta come il rapporto tra due variabili aleatorie chi-quadrato indipendenti, normalizzate rispetto ai rispettivi gradi di libertà:\n\\(F = {\\dfrac{S_1^2}{d_1}}/{\\dfrac{S_2^2}{d_2}}.\\)\ndove \\(S_1^2\\) e \\(S_2^2\\) sono varianze campionarie.\nLa distribuzione F è asimmetrica e sempre positiva, con una coda più lunga a destra. Man mano che i gradi di libertà aumentano, la distribuzione si avvicina a una distribuzione normale.",
    "crumbs": [
      "Theory",
      "Modelli di variabili aleatorie"
    ]
  },
  {
    "objectID": "th/12_gestione_delle_variabili.html",
    "href": "th/12_gestione_delle_variabili.html",
    "title": "Gestione delle variabili",
    "section": "",
    "text": "Quando si costruisce un modello statistico, non solo di regressione, è essenziale trattare correttamente le variabili in base alla loro natura. In generale si distinguono variabili dicotomiche, categoriche e numeriche, e questa distinzione influisce direttamente sul tipo di modello utilizzabile e sull’interpretazione dei parametri.\nLe variabili dicotomiche hanno due soli livelli, come sì/no o vero/falso. Le variabili categoriche presentano più di due modalità, ad esempio la stagione o la regione di provenienza. Le variabili numeriche, infine, rappresentano quantità misurabili e possono essere continue o discrete.\n\n\n\n\nQuando una variabile dicotomica è usata come ingresso, viene quasi sempre codificata con i valori 0 e 1, anche se in teoria qualsiasi coppia di numeri distinti sarebbe possibile. Questa scelta rende immediata l’interpretazione dei coefficienti.\nSe, ad esempio, \\(x_i=1\\) indica “femmina” e \\(x_i=0\\) “maschio”, allora il coefficiente \\(\\beta_1\\) associato a \\(x_i\\) misura la differenza media nella risposta \\(Y\\) tra il gruppo femminile e il gruppo di riferimento (maschile), a parità delle altre variabili. Per questo motivo le dicotomiche sono molto comuni anche nei disegni sperimentali, dove permettono confronti diretti tra trattamento e controllo.\n\n\n\nUna variabile dicotomica non è adatta come risposta in un modello di regressione lineare classica, perché le ipotesi di normalità e omoschedasticità risultano violate. In questi casi si utilizzano modelli specifici per la classificazione, come la regressione logistica nel caso binario, l’analisi discriminante o, più in generale, modelli di machine learning per la classificazione.\n\n\n\n\nLe variabili categoriche si distinguono in nominali e ordinali. Le nominali non hanno un ordine naturale tra le categorie, come nel caso della regione di provenienza o del tipo di carburante. Le ordinali, invece, presentano un ordine intrinseco, ad esempio il livello di istruzione o il grado di gravità di un sintomo.\nTalvolta le variabili ordinali vengono codificate con numeri interi crescenti, ma questa scelta va fatta con cautela, perché introduce implicitamente una nozione di distanza tra le categorie che potrebbe non essere giustificata dal fenomeno osservato.\n\n\nQuando la variabile risposta è categorica, il modello di regressione lineare non è appropriato. A seconda del numero di categorie, si ricorre alla regressione logistica binaria o multinomiale, oppure a modelli di classificazione più generali come alberi decisionali, foreste casuali o reti neurali.\n\n\n\nIn un modello di regressione, una variabile categorica con \\(k\\) livelli non può essere inserita direttamente come numero. Deve essere trasformata in \\(k-1\\) variabili dicotomiche, dette dummy, scegliendo una categoria come riferimento. Questa scelta evita problemi di collinearità e consente di interpretare i coefficienti come differenze rispetto al livello di riferimento.\n\n\n\n\nNel caso di una variabile categorica con \\(k\\) livelli, la codifica one-hot richiede di scegliere una categoria come riferimento (o default) e di costruire solo \\(k-1\\) variabili binarie. La categoria di riferimento non compare esplicitamente nel modello, ma è assorbita nel termine noto.\nSe, ad esempio, la variabile stagione ha quattro livelli (inverno, primavera, estate, autunno) e si sceglie inverno come default, il modello lineare assume la forma \\[\nY = \\beta_0 + \\beta_{\\text{pri}} x_{\\text{pri}} + \\beta_{\\text{est}} x_{\\text{est}} + \\beta_{\\text{aut}} x_{\\text{aut}} + \\varepsilon.\n\\] In questa parametrizzazione, \\(\\beta_0\\) rappresenta il valore atteso di \\(Y\\) per la categoria di riferimento (inverno), mentre ciascun coefficiente associato alle dummy misura la differenza media rispetto al default.\nConsiderando, ad esempio, le seguenti stime:\n\n\n\nCoefficiente\nValore\n\n\n\n\n\\(\\beta_0\\) (inverno)\n16.4\n\n\n\\(\\beta_{\\text{pri}}\\) (primavera)\n5.4\n\n\n\\(\\beta_{\\text{est}}\\) (estate)\n6.7\n\n\n\\(\\beta_{\\text{aut}}\\) (autunno)\n1.8\n\n\n\ni valori attesi della risposta risultano:\n\n\n\nStagione\n\\(E[Y]\\)\n\n\n\n\nInverno\n16.4\n\n\nPrimavera\n21.8\n\n\nEstate\n23.1\n\n\nAutunno\n18.2\n\n\n\nL’interpretazione è immediata: ciascun coefficiente indica di quanto la media della risposta cambia rispetto all’inverno. Questa codifica consente anche di formulare test di ipotesi mirati, ad esempio verificare se l’autunno è indistinguibile dall’inverno tramite \\(H_0:\\beta_{\\text{aut}}=0\\), oppure confrontare direttamente primavera ed estate con \\(H_0:\\beta_{\\text{pri}}=\\beta_{\\text{est}}\\).\n\n\n\nQuando si applica una procedura di stepwise backward, può accadere che una delle variabili dummy venga eliminata dal modello. Questo va interpretato come evidenza del fatto che quella categoria non è statisticamente distinguibile dalla categoria di riferimento. Se, ad esempio, la dummy per l’autunno viene rimossa, il modello sta suggerendo che autunno e inverno possono essere considerati equivalenti rispetto alla risposta.\nIn termini di codifica, la situazione diventa:\n\n\n\nStagione\nprimavera\nestate\n\n\n\n\nPrimavera\n1\n0\n\n\nEstate\n0\n1\n\n\nAutunno\n0\n0\n\n\nInverno\n0\n0\n\n\n\nAutunno viene quindi trattato come inverno, poiché entrambe le categorie condividono la stessa combinazione di dummy.\nSe l’interesse è confrontare direttamente due categorie non di riferimento, come primavera ed estate, ci sono due possibilità concettualmente equivalenti. La prima consiste nel formulare un test lineare sui parametri, ad esempio \\(H_0:\\beta_{\\text{pri}}=\\beta_{\\text{est}}\\). La seconda è cambiare la categoria di riferimento, ad esempio ponendo estate come default, e ricodificare il modello.\nVa infine ricordato che ogni categoria aggiuntiva aumenta il numero di parametri \\(p\\). È quindi importante monitorare il rapporto \\(n/p\\): in presenza di categorie rare può essere opportuno accorparle in macro-categorie, oppure valutare l’esclusione di alcune osservazioni, per mantenere il modello stabile e interpretabile.\n\n\n\nLe variabili numeriche rappresentano quantità misurabili e, a differenza delle categoriche, possono entrare direttamente in un modello di regressione. Tuttavia, il loro significato statistico e la loro scala influenzano il modo in cui è opportuno utilizzarle.\n\n\nLe variabili di tipo differenza sono quelle per cui ha senso interpretare direttamente una variazione additiva. Esempi tipici sono statura, peso, quoziente intellettivo o consumi espressi in unità fisiche standard. In questi casi non sono richieste trasformazioni particolari: il coefficiente di regressione misura semplicemente la variazione media della risposta associata a un incremento unitario della variabile.\n\n\n\nLe variabili di tipo rapporto sono definite su valori positivi e spesso presentano una distribuzione fortemente asimmetrica o estesa su più ordini di grandezza, come reddito, popolazione o fatturato. In questi casi è frequente applicare una trasformazione logaritmica.\nIn un modello lineare del tipo \\[\nY = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p + \\varepsilon,\n\\] se \\(x_1\\) è una variabile di tipo rapporto, l’uso di \\(\\log x_1\\) può rendere la relazione più vicina alla linearità e ridurre l’asimmetria della distribuzione, migliorando il comportamento dei residui.\n\n\n\nLe trasformazioni lineari, come il cambio di unità di misura o una riscalatura affine, non modificano le conclusioni statistiche fondamentali del modello, come i test di ipotesi o il valore di \\(R^2\\), anche se cambiano il valore numerico dei coefficienti.\nLe trasformazioni non lineari, come il logaritmo o la radice quadrata, hanno invece un impatto più sostanziale: possono migliorare la normalità dei residui, la stabilità della varianza e la linearità della relazione tra risposta e predittori, rendendo il modello più adeguato alle ipotesi teoriche.\nUn caso frequente è la regressione logaritmica sulla risposta: \\[\n\\log Y = \\beta_0 + \\beta_1 x_1 + \\cdots,\n\\] che, riscritta sulla scala originale, diventa \\[\nY = e^{\\beta_0}\\cdot e^{\\beta_1 x_1}\\cdot \\cdots.\n\\] In questa formulazione i coefficienti hanno un’interpretazione moltiplicativa. Ad esempio, se \\(\\beta_1=\\log(1.4)\\), un aumento unitario di \\(x_1\\) è associato a un incremento medio di circa +40% di \\(Y\\), a parità delle altre variabili.",
    "crumbs": [
      "Theory",
      "Gestione delle variabili"
    ]
  },
  {
    "objectID": "th/12_gestione_delle_variabili.html#variabili-dicotomiche",
    "href": "th/12_gestione_delle_variabili.html#variabili-dicotomiche",
    "title": "Gestione delle variabili",
    "section": "",
    "text": "Quando una variabile dicotomica è usata come ingresso, viene quasi sempre codificata con i valori 0 e 1, anche se in teoria qualsiasi coppia di numeri distinti sarebbe possibile. Questa scelta rende immediata l’interpretazione dei coefficienti.\nSe, ad esempio, \\(x_i=1\\) indica “femmina” e \\(x_i=0\\) “maschio”, allora il coefficiente \\(\\beta_1\\) associato a \\(x_i\\) misura la differenza media nella risposta \\(Y\\) tra il gruppo femminile e il gruppo di riferimento (maschile), a parità delle altre variabili. Per questo motivo le dicotomiche sono molto comuni anche nei disegni sperimentali, dove permettono confronti diretti tra trattamento e controllo.\n\n\n\nUna variabile dicotomica non è adatta come risposta in un modello di regressione lineare classica, perché le ipotesi di normalità e omoschedasticità risultano violate. In questi casi si utilizzano modelli specifici per la classificazione, come la regressione logistica nel caso binario, l’analisi discriminante o, più in generale, modelli di machine learning per la classificazione.",
    "crumbs": [
      "Theory",
      "Gestione delle variabili"
    ]
  },
  {
    "objectID": "th/12_gestione_delle_variabili.html#variabili-categoriche",
    "href": "th/12_gestione_delle_variabili.html#variabili-categoriche",
    "title": "Gestione delle variabili",
    "section": "",
    "text": "Le variabili categoriche si distinguono in nominali e ordinali. Le nominali non hanno un ordine naturale tra le categorie, come nel caso della regione di provenienza o del tipo di carburante. Le ordinali, invece, presentano un ordine intrinseco, ad esempio il livello di istruzione o il grado di gravità di un sintomo.\nTalvolta le variabili ordinali vengono codificate con numeri interi crescenti, ma questa scelta va fatta con cautela, perché introduce implicitamente una nozione di distanza tra le categorie che potrebbe non essere giustificata dal fenomeno osservato.\n\n\nQuando la variabile risposta è categorica, il modello di regressione lineare non è appropriato. A seconda del numero di categorie, si ricorre alla regressione logistica binaria o multinomiale, oppure a modelli di classificazione più generali come alberi decisionali, foreste casuali o reti neurali.\n\n\n\nIn un modello di regressione, una variabile categorica con \\(k\\) livelli non può essere inserita direttamente come numero. Deve essere trasformata in \\(k-1\\) variabili dicotomiche, dette dummy, scegliendo una categoria come riferimento. Questa scelta evita problemi di collinearità e consente di interpretare i coefficienti come differenze rispetto al livello di riferimento.",
    "crumbs": [
      "Theory",
      "Gestione delle variabili"
    ]
  },
  {
    "objectID": "th/12_gestione_delle_variabili.html#codifica-one-hot-con-categoria-di-riferimento",
    "href": "th/12_gestione_delle_variabili.html#codifica-one-hot-con-categoria-di-riferimento",
    "title": "Gestione delle variabili",
    "section": "",
    "text": "Nel caso di una variabile categorica con \\(k\\) livelli, la codifica one-hot richiede di scegliere una categoria come riferimento (o default) e di costruire solo \\(k-1\\) variabili binarie. La categoria di riferimento non compare esplicitamente nel modello, ma è assorbita nel termine noto.\nSe, ad esempio, la variabile stagione ha quattro livelli (inverno, primavera, estate, autunno) e si sceglie inverno come default, il modello lineare assume la forma \\[\nY = \\beta_0 + \\beta_{\\text{pri}} x_{\\text{pri}} + \\beta_{\\text{est}} x_{\\text{est}} + \\beta_{\\text{aut}} x_{\\text{aut}} + \\varepsilon.\n\\] In questa parametrizzazione, \\(\\beta_0\\) rappresenta il valore atteso di \\(Y\\) per la categoria di riferimento (inverno), mentre ciascun coefficiente associato alle dummy misura la differenza media rispetto al default.\nConsiderando, ad esempio, le seguenti stime:\n\n\n\nCoefficiente\nValore\n\n\n\n\n\\(\\beta_0\\) (inverno)\n16.4\n\n\n\\(\\beta_{\\text{pri}}\\) (primavera)\n5.4\n\n\n\\(\\beta_{\\text{est}}\\) (estate)\n6.7\n\n\n\\(\\beta_{\\text{aut}}\\) (autunno)\n1.8\n\n\n\ni valori attesi della risposta risultano:\n\n\n\nStagione\n\\(E[Y]\\)\n\n\n\n\nInverno\n16.4\n\n\nPrimavera\n21.8\n\n\nEstate\n23.1\n\n\nAutunno\n18.2\n\n\n\nL’interpretazione è immediata: ciascun coefficiente indica di quanto la media della risposta cambia rispetto all’inverno. Questa codifica consente anche di formulare test di ipotesi mirati, ad esempio verificare se l’autunno è indistinguibile dall’inverno tramite \\(H_0:\\beta_{\\text{aut}}=0\\), oppure confrontare direttamente primavera ed estate con \\(H_0:\\beta_{\\text{pri}}=\\beta_{\\text{est}}\\).",
    "crumbs": [
      "Theory",
      "Gestione delle variabili"
    ]
  },
  {
    "objectID": "th/12_gestione_delle_variabili.html#effetto-della-selezione-stepwise",
    "href": "th/12_gestione_delle_variabili.html#effetto-della-selezione-stepwise",
    "title": "Gestione delle variabili",
    "section": "",
    "text": "Quando si applica una procedura di stepwise backward, può accadere che una delle variabili dummy venga eliminata dal modello. Questo va interpretato come evidenza del fatto che quella categoria non è statisticamente distinguibile dalla categoria di riferimento. Se, ad esempio, la dummy per l’autunno viene rimossa, il modello sta suggerendo che autunno e inverno possono essere considerati equivalenti rispetto alla risposta.\nIn termini di codifica, la situazione diventa:\n\n\n\nStagione\nprimavera\nestate\n\n\n\n\nPrimavera\n1\n0\n\n\nEstate\n0\n1\n\n\nAutunno\n0\n0\n\n\nInverno\n0\n0\n\n\n\nAutunno viene quindi trattato come inverno, poiché entrambe le categorie condividono la stessa combinazione di dummy.\nSe l’interesse è confrontare direttamente due categorie non di riferimento, come primavera ed estate, ci sono due possibilità concettualmente equivalenti. La prima consiste nel formulare un test lineare sui parametri, ad esempio \\(H_0:\\beta_{\\text{pri}}=\\beta_{\\text{est}}\\). La seconda è cambiare la categoria di riferimento, ad esempio ponendo estate come default, e ricodificare il modello.\nVa infine ricordato che ogni categoria aggiuntiva aumenta il numero di parametri \\(p\\). È quindi importante monitorare il rapporto \\(n/p\\): in presenza di categorie rare può essere opportuno accorparle in macro-categorie, oppure valutare l’esclusione di alcune osservazioni, per mantenere il modello stabile e interpretabile.",
    "crumbs": [
      "Theory",
      "Gestione delle variabili"
    ]
  },
  {
    "objectID": "th/12_gestione_delle_variabili.html#variabili-numeriche",
    "href": "th/12_gestione_delle_variabili.html#variabili-numeriche",
    "title": "Gestione delle variabili",
    "section": "",
    "text": "Le variabili numeriche rappresentano quantità misurabili e, a differenza delle categoriche, possono entrare direttamente in un modello di regressione. Tuttavia, il loro significato statistico e la loro scala influenzano il modo in cui è opportuno utilizzarle.\n\n\nLe variabili di tipo differenza sono quelle per cui ha senso interpretare direttamente una variazione additiva. Esempi tipici sono statura, peso, quoziente intellettivo o consumi espressi in unità fisiche standard. In questi casi non sono richieste trasformazioni particolari: il coefficiente di regressione misura semplicemente la variazione media della risposta associata a un incremento unitario della variabile.\n\n\n\nLe variabili di tipo rapporto sono definite su valori positivi e spesso presentano una distribuzione fortemente asimmetrica o estesa su più ordini di grandezza, come reddito, popolazione o fatturato. In questi casi è frequente applicare una trasformazione logaritmica.\nIn un modello lineare del tipo \\[\nY = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p + \\varepsilon,\n\\] se \\(x_1\\) è una variabile di tipo rapporto, l’uso di \\(\\log x_1\\) può rendere la relazione più vicina alla linearità e ridurre l’asimmetria della distribuzione, migliorando il comportamento dei residui.\n\n\n\nLe trasformazioni lineari, come il cambio di unità di misura o una riscalatura affine, non modificano le conclusioni statistiche fondamentali del modello, come i test di ipotesi o il valore di \\(R^2\\), anche se cambiano il valore numerico dei coefficienti.\nLe trasformazioni non lineari, come il logaritmo o la radice quadrata, hanno invece un impatto più sostanziale: possono migliorare la normalità dei residui, la stabilità della varianza e la linearità della relazione tra risposta e predittori, rendendo il modello più adeguato alle ipotesi teoriche.\nUn caso frequente è la regressione logaritmica sulla risposta: \\[\n\\log Y = \\beta_0 + \\beta_1 x_1 + \\cdots,\n\\] che, riscritta sulla scala originale, diventa \\[\nY = e^{\\beta_0}\\cdot e^{\\beta_1 x_1}\\cdot \\cdots.\n\\] In questa formulazione i coefficienti hanno un’interpretazione moltiplicativa. Ad esempio, se \\(\\beta_1=\\log(1.4)\\), un aumento unitario di \\(x_1\\) è associato a un incremento medio di circa +40% di \\(Y\\), a parità delle altre variabili.",
    "crumbs": [
      "Theory",
      "Gestione delle variabili"
    ]
  },
  {
    "objectID": "lab/01_02_Introduzione.html",
    "href": "lab/01_02_Introduzione.html",
    "title": "01-02: Introduzione",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ngen=np.random.randint(0,100, size=100)\nplt.hist(gen)\nplt.show()\n\n\n\n\n\n\n\n\n\nvals, freqs = np.unique(gen, return_counts=True)\nplt.bar(vals,freqs)\nplt.show()\n\n\n\n\n\n\n\n\n\nf_gen = np.random.uniform(0,1, size=100)\nf_gen\n\narray([1.52041499e-01, 6.30839436e-01, 5.21142994e-01, 3.75766743e-02,\n       1.90704818e-01, 3.53463772e-01, 6.11742370e-01, 9.64105044e-01,\n       4.74260010e-01, 8.95519640e-01, 4.96846248e-01, 4.33177875e-01,\n       1.99619133e-01, 9.47677944e-01, 4.18110121e-01, 3.13258267e-01,\n       5.33809036e-01, 8.59851163e-01, 3.08351785e-01, 5.26905725e-01,\n       3.66313509e-01, 7.55822360e-01, 4.91134848e-01, 5.32147528e-01,\n       1.80006287e-01, 5.23692600e-01, 4.81787764e-01, 9.69285912e-01,\n       9.48764984e-01, 2.02829210e-01, 4.28632372e-02, 3.95234790e-01,\n       7.90260561e-01, 7.27555605e-01, 6.60772549e-02, 1.46392366e-01,\n       1.16627891e-01, 5.53497779e-01, 7.19912320e-01, 4.78135812e-04,\n       7.59272001e-01, 6.14927985e-02, 6.65614446e-01, 6.24738656e-01,\n       7.44943917e-02, 9.90929470e-01, 5.83544027e-01, 7.72674795e-01,\n       2.99580385e-01, 3.25610762e-01, 1.37593906e-01, 2.81585675e-01,\n       5.08403543e-01, 7.80397136e-01, 4.36556748e-01, 8.13315364e-01,\n       8.08486157e-01, 6.91533666e-01, 5.11304412e-01, 7.34973605e-01,\n       2.53875507e-01, 7.72117122e-01, 2.84611460e-01, 1.73545515e-01,\n       4.18820514e-01, 3.66116311e-01, 3.94067614e-01, 4.40649868e-01,\n       3.74612927e-02, 5.26417613e-01, 2.81059280e-01, 7.86509150e-01,\n       2.63950309e-01, 6.71883489e-02, 4.92052219e-02, 6.23452740e-01,\n       6.74960849e-01, 9.13132543e-01, 5.92615914e-01, 1.92269643e-01,\n       6.01897800e-01, 8.45466129e-01, 9.58903528e-02, 5.18269881e-01,\n       6.42104800e-01, 8.34977741e-01, 5.92467957e-02, 1.48619077e-01,\n       6.77815939e-01, 9.36100636e-01, 2.89635814e-02, 2.27803891e-01,\n       9.89414240e-01, 9.05032133e-01, 3.49191121e-01, 9.62800008e-02,\n       3.24079944e-01, 9.51528519e-01, 1.83452855e-01, 2.02846415e-01])\n\n\n\nimport random\nmin_val = 1\nmax_val = 10\nvalues = [random.randint(min_val, max_val) for _ in range(100)]\nfrequencies = [values.count(i) for i in range(min_val, max_val+1)]\n\nvalues = np.array(values)\nfrequencies = np.array(frequencies)\nunique_vals = np.unique(values)\nprint(f\"vals: {values}\")\nprint(f\"freqs: {frequencies}\")\nprint(f\"uniques: {unique_vals}\")\nunique_vals, frequencies = np.unique(values, return_counts=True)\nprint(f\"freqs: {frequencies}\")\nprint(f\"uniques: {unique_vals}\")\n\nvals: [ 7 10  7  6  5  4  5  5 10 10 10  3  3  4  3  9  1  5  4  2  3  3  3  1\n 10  4  1  5  6  4  9 10  7  4  2  3  2  3 10  2  8  5  8  2  6  2  5  1\n  7  5 10  3  3  2  7  9  5  2  2  2  4  6  9  1  5  4 10  2  1  2  9  6\n  3  2  2 10  5  9  3  5  3  3  2  7  9 10  9  4  5  7  1  7  6  3  9  6\n  4  4  9 10]\nfreqs: [ 7 15 15 11 13  7  8  2 10 12]\nuniques: [ 1  2  3  4  5  6  7  8  9 10]\nfreqs: [ 7 15 15 11 13  7  8  2 10 12]\nuniques: [ 1  2  3  4  5  6  7  8  9 10]\n\n\nGeneriamo 100 valori da una distribuzione normale con media = 5 e std = 2, arrotondati a 1 decimale. Calcoliamo valori unici e frequenze.\n\nn=100\nmean=5\nstd=1\n\nvals = np.random.normal(loc=mean, scale=2, size=n)\nvals = np.round(vals,1)\n\nunqs, freqs = np.unique(vals, return_counts=True)\nplt.bar(unqs, freqs)\nplt.show()\n\n\n\n\n\n\n\n\n\nn_samples = 500\ngen = np.random.normal(loc=10, scale=2, size=n_samples)\ngen = np.round(gen, 1).astype(float)\n\nplt.hist(gen)\nplt.show()",
    "crumbs": [
      "Laboratory",
      "01-02: Introduzione"
    ]
  },
  {
    "objectID": "lab/01_02_Introduzione.html#generazione-di-valori-casuali-con-numpy",
    "href": "lab/01_02_Introduzione.html#generazione-di-valori-casuali-con-numpy",
    "title": "01-02: Introduzione",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ngen=np.random.randint(0,100, size=100)\nplt.hist(gen)\nplt.show()\n\n\n\n\n\n\n\n\n\nvals, freqs = np.unique(gen, return_counts=True)\nplt.bar(vals,freqs)\nplt.show()\n\n\n\n\n\n\n\n\n\nf_gen = np.random.uniform(0,1, size=100)\nf_gen\n\narray([1.52041499e-01, 6.30839436e-01, 5.21142994e-01, 3.75766743e-02,\n       1.90704818e-01, 3.53463772e-01, 6.11742370e-01, 9.64105044e-01,\n       4.74260010e-01, 8.95519640e-01, 4.96846248e-01, 4.33177875e-01,\n       1.99619133e-01, 9.47677944e-01, 4.18110121e-01, 3.13258267e-01,\n       5.33809036e-01, 8.59851163e-01, 3.08351785e-01, 5.26905725e-01,\n       3.66313509e-01, 7.55822360e-01, 4.91134848e-01, 5.32147528e-01,\n       1.80006287e-01, 5.23692600e-01, 4.81787764e-01, 9.69285912e-01,\n       9.48764984e-01, 2.02829210e-01, 4.28632372e-02, 3.95234790e-01,\n       7.90260561e-01, 7.27555605e-01, 6.60772549e-02, 1.46392366e-01,\n       1.16627891e-01, 5.53497779e-01, 7.19912320e-01, 4.78135812e-04,\n       7.59272001e-01, 6.14927985e-02, 6.65614446e-01, 6.24738656e-01,\n       7.44943917e-02, 9.90929470e-01, 5.83544027e-01, 7.72674795e-01,\n       2.99580385e-01, 3.25610762e-01, 1.37593906e-01, 2.81585675e-01,\n       5.08403543e-01, 7.80397136e-01, 4.36556748e-01, 8.13315364e-01,\n       8.08486157e-01, 6.91533666e-01, 5.11304412e-01, 7.34973605e-01,\n       2.53875507e-01, 7.72117122e-01, 2.84611460e-01, 1.73545515e-01,\n       4.18820514e-01, 3.66116311e-01, 3.94067614e-01, 4.40649868e-01,\n       3.74612927e-02, 5.26417613e-01, 2.81059280e-01, 7.86509150e-01,\n       2.63950309e-01, 6.71883489e-02, 4.92052219e-02, 6.23452740e-01,\n       6.74960849e-01, 9.13132543e-01, 5.92615914e-01, 1.92269643e-01,\n       6.01897800e-01, 8.45466129e-01, 9.58903528e-02, 5.18269881e-01,\n       6.42104800e-01, 8.34977741e-01, 5.92467957e-02, 1.48619077e-01,\n       6.77815939e-01, 9.36100636e-01, 2.89635814e-02, 2.27803891e-01,\n       9.89414240e-01, 9.05032133e-01, 3.49191121e-01, 9.62800008e-02,\n       3.24079944e-01, 9.51528519e-01, 1.83452855e-01, 2.02846415e-01])\n\n\n\nimport random\nmin_val = 1\nmax_val = 10\nvalues = [random.randint(min_val, max_val) for _ in range(100)]\nfrequencies = [values.count(i) for i in range(min_val, max_val+1)]\n\nvalues = np.array(values)\nfrequencies = np.array(frequencies)\nunique_vals = np.unique(values)\nprint(f\"vals: {values}\")\nprint(f\"freqs: {frequencies}\")\nprint(f\"uniques: {unique_vals}\")\nunique_vals, frequencies = np.unique(values, return_counts=True)\nprint(f\"freqs: {frequencies}\")\nprint(f\"uniques: {unique_vals}\")\n\nvals: [ 7 10  7  6  5  4  5  5 10 10 10  3  3  4  3  9  1  5  4  2  3  3  3  1\n 10  4  1  5  6  4  9 10  7  4  2  3  2  3 10  2  8  5  8  2  6  2  5  1\n  7  5 10  3  3  2  7  9  5  2  2  2  4  6  9  1  5  4 10  2  1  2  9  6\n  3  2  2 10  5  9  3  5  3  3  2  7  9 10  9  4  5  7  1  7  6  3  9  6\n  4  4  9 10]\nfreqs: [ 7 15 15 11 13  7  8  2 10 12]\nuniques: [ 1  2  3  4  5  6  7  8  9 10]\nfreqs: [ 7 15 15 11 13  7  8  2 10 12]\nuniques: [ 1  2  3  4  5  6  7  8  9 10]\n\n\nGeneriamo 100 valori da una distribuzione normale con media = 5 e std = 2, arrotondati a 1 decimale. Calcoliamo valori unici e frequenze.\n\nn=100\nmean=5\nstd=1\n\nvals = np.random.normal(loc=mean, scale=2, size=n)\nvals = np.round(vals,1)\n\nunqs, freqs = np.unique(vals, return_counts=True)\nplt.bar(unqs, freqs)\nplt.show()\n\n\n\n\n\n\n\n\n\nn_samples = 500\ngen = np.random.normal(loc=10, scale=2, size=n_samples)\ngen = np.round(gen, 1).astype(float)\n\nplt.hist(gen)\nplt.show()",
    "crumbs": [
      "Laboratory",
      "01-02: Introduzione"
    ]
  },
  {
    "objectID": "lab/01_02_Introduzione.html#funzioni-cdf-e-ecdf",
    "href": "lab/01_02_Introduzione.html#funzioni-cdf-e-ecdf",
    "title": "01-02: Introduzione",
    "section": "Funzioni CDF e ECDF",
    "text": "Funzioni CDF e ECDF\n\nx = [i/n_samples for i in range(n_samples)] # una sorta di linspace tra 0 e 1 di dimensione n_samples\ny_sorted = sorted(gen)\n\n#cdf(i) = p(X &lt;= i)\ndef cdf(i):\n    count = 0\n    for v in y_sorted:\n        if v &lt;= i:\n            count +=1\n\n    p = count/n_samples\n    return p\n\n\nplt.plot(y_sorted,x, label=\"prova\")\nplt.plot(10, cdf(10), \"r.\")\nplt.legend()\nplt.show()\n\nprint(cdf(10))\n\n\n\n\n\n\n\n\n0.48\n\n\n\nfrom scipy.stats import norm \nx = np.linspace(-2,2, 100)\ny = norm.cdf(x)\n\nplt.plot(y, x)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\nx = np.linspace(6, 14, 100)\ny = norm.pdf(x, loc=10, scale=2)\n\nplt.plot(x, y, \"orange\")\nplt.plot(10, norm.pdf(10, loc=10, scale=2), \"r.\")\nplt.plot(x, [0.2 for _ in range(100)], \"g--\")\nplt.show()",
    "crumbs": [
      "Laboratory",
      "01-02: Introduzione"
    ]
  },
  {
    "objectID": "lab/15_16_OLS_Abalone.html",
    "href": "lab/15_16_OLS_Abalone.html",
    "title": "15-16: OLS (Abalone)",
    "section": "",
    "text": "from ucimlrepo import fetch_ucirepo\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Laboratory",
      "15-16: OLS (Abalone)"
    ]
  },
  {
    "objectID": "lab/15_16_OLS_Abalone.html#caricamento-dati-one-hot-encoding",
    "href": "lab/15_16_OLS_Abalone.html#caricamento-dati-one-hot-encoding",
    "title": "15-16: OLS (Abalone)",
    "section": "Caricamento dati + one-hot encoding",
    "text": "Caricamento dati + one-hot encoding\nIl dataset ha una feature categoriale Sex (F/M/I). La trasformiamo in 3 colonne dummy (0/1).\n\nabalone = fetch_ucirepo(id=1)\nX: pd.DataFrame = abalone.data.features.copy()\ny = abalone.data.targets.values.reshape(-1)  # 1D\n\n# one-hot su Sex\nX = pd.get_dummies(X, columns=[\"Sex\"], drop_first=False)\nfor c in [\"Sex_F\", \"Sex_M\", \"Sex_I\"]:\n    X[c] = X[c].astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX.head()\n\n\n\n\n\n\n\n\nLength\nDiameter\nHeight\nWhole_weight\nShucked_weight\nViscera_weight\nShell_weight\nSex_F\nSex_I\nSex_M\n\n\n\n\n0\n0.455\n0.365\n0.095\n0.5140\n0.2245\n0.1010\n0.150\n0\n0\n1\n\n\n1\n0.350\n0.265\n0.090\n0.2255\n0.0995\n0.0485\n0.070\n0\n0\n1\n\n\n2\n0.530\n0.420\n0.135\n0.6770\n0.2565\n0.1415\n0.210\n1\n0\n0\n\n\n3\n0.440\n0.365\n0.125\n0.5160\n0.2155\n0.1140\n0.155\n0\n0\n1\n\n\n4\n0.330\n0.255\n0.080\n0.2050\n0.0895\n0.0395\n0.055\n0\n1\n0",
    "crumbs": [
      "Laboratory",
      "15-16: OLS (Abalone)"
    ]
  },
  {
    "objectID": "lab/15_16_OLS_Abalone.html#ols-con-intercetta-valutazione-su-test",
    "href": "lab/15_16_OLS_Abalone.html#ols-con-intercetta-valutazione-su-test",
    "title": "15-16: OLS (Abalone)",
    "section": "OLS con intercetta + valutazione su test",
    "text": "OLS con intercetta + valutazione su test\nIn statsmodels, l’intercetta non è aggiunta automaticamente: la mettiamo con add_constant. Poi stimiamo su train e valutiamo su test con RMSE/MAE/R².\n\nX_train_c = sm.add_constant(X_train)\nX_test_c  = sm.add_constant(X_test, has_constant=\"add\")\n\nols = sm.OLS(y_train, X_train_c).fit()\nprint(ols.summary())\n\ny_pred = ols.predict(X_test_c)\n\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmae  = mean_absolute_error(y_test, y_pred)\nr2   = r2_score(y_test, y_pred)\n\nprint(f\"Test RMSE: {rmse:.3f} | MAE: {mae:.3f} | R^2: {r2:.3f}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.535\nModel:                            OLS   Adj. R-squared:                  0.534\nMethod:                 Least Squares   F-statistic:                     425.5\nDate:                Wed, 04 Feb 2026   Prob (F-statistic):               0.00\nTime:                        15:00:31   Log-Likelihood:                -7355.4\nNo. Observations:                3341   AIC:                         1.473e+04\nDf Residuals:                    3331   BIC:                         1.479e+04\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst              2.6537      0.235     11.299      0.000       2.193       3.114\nLength            -0.2016      2.080     -0.097      0.923      -4.281       3.877\nDiameter          11.1234      2.547      4.367      0.000       6.130      16.117\nHeight            10.4453      1.615      6.467      0.000       7.278      13.612\nWhole_weight       8.9322      0.863     10.346      0.000       7.239      10.625\nShucked_weight   -20.2565      0.967    -20.943      0.000     -22.153     -18.360\nViscera_weight    -9.5589      1.470     -6.504      0.000     -12.441      -6.677\nShell_weight       8.7924      1.308      6.724      0.000       6.228      11.356\nSex_F              1.0898      0.108     10.136      0.000       0.879       1.301\nSex_I              0.3708      0.090      4.140      0.000       0.195       0.546\nSex_M              1.1931      0.099     12.068      0.000       0.999       1.387\n==============================================================================\nOmnibus:                      750.514   Durbin-Watson:                   1.980\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2218.560\nSkew:                           1.155   Prob(JB):                         0.00\nKurtosis:                       6.256   Cond. No.                     1.88e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.72e-27. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\nTest RMSE: 2.212 | MAE: 1.593 | R^2: 0.548\n\n\n\nCome leggere summary() (in modo semplice)\n\nR-squared / Adj. R-squared: quanto il modello spiega sul training (utile, ma non basta).\nF-statistic e relativo Prob (F-statistic): test globale “c’è almeno una feature utile?”.\nTabella coefficienti:\n\ncoef: effetto stimato (a parità delle altre variabili).\nstd err: incertezza della stima.\nP&gt;|t|: se è piccola (es. &lt; 0.05), il coefficiente è “diverso da 0” nel modello.\n[0.025, 0.975]: intervallo di confidenza al 95% del coefficiente (se include 0, effetto poco certo).\n\n\nIn pratica: guarda sempre anche le metriche su test (RMSE/MAE/R²) e un controllo dei residui.",
    "crumbs": [
      "Laboratory",
      "15-16: OLS (Abalone)"
    ]
  },
  {
    "objectID": "lab/15_16_OLS_Abalone.html#formula-interface-più-leggibile",
    "href": "lab/15_16_OLS_Abalone.html#formula-interface-più-leggibile",
    "title": "15-16: OLS (Abalone)",
    "section": "Formula interface (più leggibile)",
    "text": "Formula interface (più leggibile)\nCon smf.ols(\"y ~ x1 + x2 + ...\", data=df) i nomi restano chiari e la costante è inclusa di default. Qui usiamo le colonne già preprocessate.\n\ndf_all = X.copy()\ndf_all[\"Rings\"] = y\n\ndf_train = df_all.loc[X_train.index].copy()\ndf_test  = df_all.loc[X_test.index].copy()\n\nformula = \"Rings ~ \" + \" + \".join([c for c in df_train.columns if c != \"Rings\"])\n\nols_f = smf.ols(formula, data=df_train).fit()\nprint(ols_f.summary())\n\ny_pred_f = ols_f.predict(df_test)\nrmse_f = np.sqrt(mean_squared_error(df_test[\"Rings\"], y_pred_f))\nprint(f\"Test RMSE (formula): {rmse_f:.3f}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Rings   R-squared:                       0.535\nModel:                            OLS   Adj. R-squared:                  0.534\nMethod:                 Least Squares   F-statistic:                     425.5\nDate:                Wed, 04 Feb 2026   Prob (F-statistic):               0.00\nTime:                        15:00:31   Log-Likelihood:                -7355.4\nNo. Observations:                3341   AIC:                         1.473e+04\nDf Residuals:                    3331   BIC:                         1.479e+04\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          2.6537      0.235     11.299      0.000       2.193       3.114\nLength            -0.2016      2.080     -0.097      0.923      -4.281       3.877\nDiameter          11.1234      2.547      4.367      0.000       6.130      16.117\nHeight            10.4453      1.615      6.467      0.000       7.278      13.612\nWhole_weight       8.9322      0.863     10.346      0.000       7.239      10.625\nShucked_weight   -20.2565      0.967    -20.943      0.000     -22.153     -18.360\nViscera_weight    -9.5589      1.470     -6.504      0.000     -12.441      -6.677\nShell_weight       8.7924      1.308      6.724      0.000       6.228      11.356\nSex_F              1.0898      0.108     10.136      0.000       0.879       1.301\nSex_I              0.3708      0.090      4.140      0.000       0.195       0.546\nSex_M              1.1931      0.099     12.068      0.000       0.999       1.387\n==============================================================================\nOmnibus:                      750.514   Durbin-Watson:                   1.980\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2218.560\nSkew:                           1.155   Prob(JB):                         0.00\nKurtosis:                       6.256   Cond. No.                     1.88e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.72e-27. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\nTest RMSE (formula): 2.212",
    "crumbs": [
      "Laboratory",
      "15-16: OLS (Abalone)"
    ]
  },
  {
    "objectID": "lab/15_16_OLS_Abalone.html#intervalli-di-confidenza-dei-coefficienti",
    "href": "lab/15_16_OLS_Abalone.html#intervalli-di-confidenza-dei-coefficienti",
    "title": "15-16: OLS (Abalone)",
    "section": "Intervalli di confidenza dei coefficienti",
    "text": "Intervalli di confidenza dei coefficienti\nUtile per vedere quali coefficienti sono stimati con più precisione.\n\nci = ols.conf_int(alpha=0.05)\nci.columns = [\"CI_low\", \"CI_high\"]\ncoefs = ols.params.rename(\"coef\")\n\ncoef_table = pd.concat([coefs, ci], axis=1).sort_values(\"coef\", ascending=False)\ncoef_table.head(12)\n\n\n\n\n\n\n\n\ncoef\nCI_low\nCI_high\n\n\n\n\nDiameter\n11.123391\n6.129592\n16.117190\n\n\nHeight\n10.445325\n7.278322\n13.612329\n\n\nWhole_weight\n8.932176\n7.239409\n10.624942\n\n\nShell_weight\n8.792378\n6.228441\n11.356315\n\n\nconst\n2.653678\n2.193204\n3.114152\n\n\nSex_M\n1.193068\n0.999236\n1.386901\n\n\nSex_F\n1.089792\n0.878976\n1.300608\n\n\nSex_I\n0.370818\n0.195211\n0.546425\n\n\nLength\n-0.201554\n-4.280565\n3.877458\n\n\nViscera_weight\n-9.558916\n-12.440667\n-6.677166\n\n\nShucked_weight\n-20.256545\n-22.152940\n-18.360150",
    "crumbs": [
      "Laboratory",
      "15-16: OLS (Abalone)"
    ]
  },
  {
    "objectID": "lab/15_16_OLS_Abalone.html#predizione-intervalli",
    "href": "lab/15_16_OLS_Abalone.html#predizione-intervalli",
    "title": "15-16: OLS (Abalone)",
    "section": "Predizione + intervalli",
    "text": "Predizione + intervalli\n\nget_prediction(...).summary_frame() produce anche intervalli.\nmean_ci_*: intervallo di confidenza della media E[Y|X].\nobs_ci_*: intervallo di predizione per una nuova osservazione (più largo).\n\n\npred = ols.get_prediction(X_test_c.iloc[:5])\npred.summary_frame(alpha=0.05)\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n866\n11.761361\n0.123218\n11.519770\n12.002953\n7.459743\n16.062979\n\n\n1483\n10.241926\n0.095967\n10.053766\n10.430087\n5.942978\n14.540875\n\n\n599\n14.001036\n0.119701\n13.766340\n14.235731\n9.699799\n18.302272\n\n\n1702\n11.995093\n0.079694\n11.838839\n12.151347\n7.697423\n16.292763\n\n\n670\n11.161415\n0.094532\n10.976069\n11.346761\n6.862589\n15.460241",
    "crumbs": [
      "Laboratory",
      "15-16: OLS (Abalone)"
    ]
  },
  {
    "objectID": "lab/15_16_OLS_Abalone.html#residui",
    "href": "lab/15_16_OLS_Abalone.html#residui",
    "title": "15-16: OLS (Abalone)",
    "section": "Residui",
    "text": "Residui\n\nResidui vs fitted: se vedi pattern → possibile non linearità / varianza non costante.\nQ-Q plot: se molto lontano dalla diagonale → residui non “gaussiani” (non sempre un disastro, ma è un segnale).\n\n\nfitted = ols.fittedvalues\nresid  = ols.resid\n\nplt.figure(figsize=(6,4))\nplt.plot(fitted, resid, \".\", ms=3)\nplt.axhline(0)\nplt.xlabel(\"Valori stimati (fitted)\")\nplt.ylabel(\"Residui\")\nplt.title(\"Residuals vs Fitted\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(6,4))\nsm.qqplot(resid, line=\"45\", fit=True)\nplt.title(\"Q-Q plot residui\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n&lt;Figure size 600x400 with 0 Axes&gt;",
    "crumbs": [
      "Laboratory",
      "15-16: OLS (Abalone)"
    ]
  },
  {
    "objectID": "lab/03_04_Gaussiane.html",
    "href": "lab/03_04_Gaussiane.html",
    "title": "03-04: Gaussiane",
    "section": "",
    "text": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Laboratory",
      "03-04: Gaussiane"
    ]
  },
  {
    "objectID": "lab/03_04_Gaussiane.html#normale-teorica",
    "href": "lab/03_04_Gaussiane.html#normale-teorica",
    "title": "03-04: Gaussiane",
    "section": "Normale teorica",
    "text": "Normale teorica\nDefiniamo una distribuzione normale con media mu=10 e deviazione standard sigma=4. Calcoliamo la funzione di densità (PDF) su un intervallo di valori.\n\nmean = 10\nstd = 4\nx = np.linspace(mean-3*std, mean+3*std, 100)\ny = norm.pdf(x, loc=mean, scale=std)\n\nplt.plot(x,y, label=\"pdf\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Laboratory",
      "03-04: Gaussiane"
    ]
  },
  {
    "objectID": "lab/03_04_Gaussiane.html#somma-di-variabili-uniformi",
    "href": "lab/03_04_Gaussiane.html#somma-di-variabili-uniformi",
    "title": "03-04: Gaussiane",
    "section": "Somma di variabili uniformi",
    "text": "Somma di variabili uniformi\nGeneriamo una matrice samples di dimensione (1000, 1000) dove ogni colonna è una sequenza di 1000 estrazioni da Uniforme(0,1). Calcoliamo la somma per colonna.\n\nn = 1000\nsamples = np.random.uniform(0,1, size=(n,n))\nprint(samples.shape)\n\n(1000, 1000)\n\n\n\nsumm = np.sum(samples, axis=0)\nprint(summ.shape)\nmean = np.round(np.mean(summ),2)\nstd = np.round(np.std(summ),2)\nprint(mean)\nprint(std)\n\n(1000,)\n500.43\n9.2",
    "crumbs": [
      "Laboratory",
      "03-04: Gaussiane"
    ]
  },
  {
    "objectID": "lab/03_04_Gaussiane.html#confronto-con-una-normale-stimata",
    "href": "lab/03_04_Gaussiane.html#confronto-con-una-normale-stimata",
    "title": "03-04: Gaussiane",
    "section": "Confronto con una normale stimata",
    "text": "Confronto con una normale stimata\nCostruiamo una normale N(mean_emp, std_emp) e confrontiamo graficamente PDF teorica e istogramma delle somme.\n\nx = np.linspace(mean-3*std, mean+3*std, 100)\ny = norm.pdf(x, loc=mean, scale=std)\n\nplt.hist(summ, density=True)\nplt.plot(x,y, \"r--\",label=\"theorical\")\nplt.show()",
    "crumbs": [
      "Laboratory",
      "03-04: Gaussiane"
    ]
  },
  {
    "objectID": "lab/09_10_Generazione_campioni.html",
    "href": "lab/09_10_Generazione_campioni.html",
    "title": "09-10: Generazione campioni da distribuzione scelta",
    "section": "",
    "text": "from scipy.stats import multinomial, dirichlet\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Laboratory",
      "09-10: Generazione campioni da distribuzione scelta"
    ]
  },
  {
    "objectID": "lab/09_10_Generazione_campioni.html#multinomiale",
    "href": "lab/09_10_Generazione_campioni.html#multinomiale",
    "title": "09-10: Generazione campioni da distribuzione scelta",
    "section": "Multinomiale",
    "text": "Multinomiale\n\nn_cat = 4\nsupp = np.random.randint(0,n_cat, size=1000)\nuniques, p = np.unique(supp,return_counts=True)\np = p / supp.size\np = np.round(p, 3)\nprint(p)\n\n[0.248 0.234 0.263 0.255]\n\n\n\nn_trials = 20\ngen = multinomial.rvs(n_trials, p, size=1000)\ngen\n\narray([[3, 3, 7, 7],\n       [9, 4, 2, 5],\n       [5, 4, 6, 5],\n       ...,\n       [5, 5, 2, 8],\n       [6, 6, 5, 3],\n       [3, 7, 2, 8]], shape=(1000, 4))\n\n\n\nestimated_p = gen.mean(axis=0) / n_trials\nestimated_p = np.round(estimated_p, 3)\nprint(estimated_p)\n\n[0.25  0.233 0.266 0.251]",
    "crumbs": [
      "Laboratory",
      "09-10: Generazione campioni da distribuzione scelta"
    ]
  },
  {
    "objectID": "lab/09_10_Generazione_campioni.html#dirichlet",
    "href": "lab/09_10_Generazione_campioni.html#dirichlet",
    "title": "09-10: Generazione campioni da distribuzione scelta",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nsamples = np.random.randint(1,6, size=3)\nsamples\n\narray([1, 3, 4])\n\n\n\ngen = dirichlet.rvs(samples, size=4)\ngen\n\narray([[0.17879651, 0.3029195 , 0.518284  ],\n       [0.02026829, 0.26864503, 0.71108669],\n       [0.12360091, 0.54761409, 0.328785  ],\n       [0.05088434, 0.40458376, 0.54453189]])",
    "crumbs": [
      "Laboratory",
      "09-10: Generazione campioni da distribuzione scelta"
    ]
  },
  {
    "objectID": "exams/esame_1_1.html",
    "href": "exams/esame_1_1.html",
    "title": "01-01: Analisi distribuzione Dirichlet",
    "section": "",
    "text": "1.1 Generare un vettore \\((X_1, ..., X_n)\\) di dimensione \\(n=10\\) con legge di Dirichlet di parametro α=0.5 (uguale per tutte le componenti). Sia Y il valore massimo delle componenti \\(Y=max_i X_i\\). Studiare la distribuzione di Y con una simulazione MC. È richiesto di stimarne media e varianza, con intervalli di confidenza, di visualizzare la distribuzione con istogramma, kde, cdf empirica.",
    "crumbs": [
      "Exams",
      "01-01: Analisi distribuzione Dirichlet"
    ]
  },
  {
    "objectID": "exams/esame_1_1.html#generazione-campioni-da-dirichlet",
    "href": "exams/esame_1_1.html#generazione-campioni-da-dirichlet",
    "title": "01-01: Analisi distribuzione Dirichlet",
    "section": "Generazione campioni da dirichlet",
    "text": "Generazione campioni da dirichlet\n\nfrom scipy.stats import dirichlet\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nn_samples = 1000\ndim = 10\nalpha = [0.5] * dim\n\n\nsamples = dirichlet.rvs(alpha=alpha, size=n_samples)\nsamples.shape\n\n(1000, 10)\n\n\n\ny = np.max(samples, axis=1)\ny.shape\n\n(1000,)",
    "crumbs": [
      "Exams",
      "01-01: Analisi distribuzione Dirichlet"
    ]
  },
  {
    "objectID": "exams/esame_1_1.html#stima-della-media-con-ic",
    "href": "exams/esame_1_1.html#stima-della-media-con-ic",
    "title": "01-01: Analisi distribuzione Dirichlet",
    "section": "Stima della media con IC",
    "text": "Stima della media con IC\n\nfrom scipy.stats import t\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\nmean = np.mean(y)\ns = np.std(y, ddof=1)          # deviazione standard campionaria\ndf = n - 1\n\nq = t.ppf(1 - alpha/2, df)     # quantile critico t\n\nmargin = q * s / np.sqrt(n)\n\nci_low  = mean - margin\nci_high = mean + margin\n\nprint(f\"IC {conf:.0%} per la media: [{ci_low:.6f}, {ci_high:.6f}]\")\n\nIC 95% per la media: [0.378259, 0.392966]",
    "crumbs": [
      "Exams",
      "01-01: Analisi distribuzione Dirichlet"
    ]
  },
  {
    "objectID": "exams/esame_1_1.html#stima-della-varianza-con-ic",
    "href": "exams/esame_1_1.html#stima-della-varianza-con-ic",
    "title": "01-01: Analisi distribuzione Dirichlet",
    "section": "Stima della varianza con IC",
    "text": "Stima della varianza con IC\n\nfrom scipy.stats import chi2\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\ns2 = np.var(y, ddof=1)          # varianza campionaria\ndf = n - 1\n\nq_low  = chi2.ppf(alpha/2, df)        # quantile basso\nq_high = chi2.ppf(1 - alpha/2, df)    # quantile alto\n\nci_low  = df * s2 / q_high\nci_high = df * s2 / q_low\n\nprint(f\"IC {conf:.0%} per la varianza: [{ci_low:.6f}, {ci_high:.6f}]\")\nprint(f\"IC {conf:.0%} per la std: [{np.sqrt(ci_low):.6f}, {np.sqrt(ci_high):.6f}]\")\n\nIC 95% per la varianza: [0.012888, 0.015360]\nIC 95% per la std: [0.113525, 0.123936]",
    "crumbs": [
      "Exams",
      "01-01: Analisi distribuzione Dirichlet"
    ]
  },
  {
    "objectID": "exams/esame_1_1.html#confronto-grafico-con-gaussiana",
    "href": "exams/esame_1_1.html#confronto-grafico-con-gaussiana",
    "title": "01-01: Analisi distribuzione Dirichlet",
    "section": "Confronto grafico con Gaussiana",
    "text": "Confronto grafico con Gaussiana\n1.2 Testare se la legge di Y può essere considerata approssimativamente Gaussiana sia graficamente, sia con un test di adattamento.\n\nfrom scipy.stats import norm\n\nmean = y.mean()\nstd = y.std(ddof=1)\n\nx_norm = np.linspace(mean + (4*std), mean * (-4*std),n_samples)\ny_norm = norm.pdf(x_norm, loc=mean, scale=std)\n\nplt.hist(y, density=True)\nplt.plot(x_norm, y_norm, \"--\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(y)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef ecdf(val, Y_sorted):\n  return np.sum(Y_sorted &lt;= val) / len(Y_sorted)\n\ny_sorted = sorted(y)\necdf_x = np.linspace(min(y_sorted), max(y_sorted), 100)\necdf_y = []\n\nfor val in ecdf_x:\n  ecdf_y.append(ecdf(val, y_sorted))\n\nplt.plot(ecdf_x, ecdf_y)\nplt.title(\"CdF Empirica\")\nplt.show()\n\n\n\n\n\n\n\n\nQuesto codice costruisce un Q-Q plot (quantile-quantile plot) per confrontare la distribuzione empirica dei dati x con una Normale stimata sugli stessi dati.\nAll’inizio ordini i dati con np.sort(x): così ottieni i quantili campionari, cioè i valori osservati messi in ordine crescente. Ogni posizione nell’array ordinato corrisponde a un quantile empirico.\nPoi costruisci p, che sono le plotting positions. L’espressione (i - 0.5) / n assegna a ciascun punto una probabilità compresa tra 0 e 1, evitando esattamente 0 e 1. Questo serve perché i quantili teorici della normale in 0 e 1 sarebbero ±∞.\nCon norm.ppf(p, loc=mean, scale=std) calcoli i quantili teorici della distribuzione Normale con media mean e deviazione standard std, stimati dal campione. In pratica stai chiedendo: “se i dati fossero davvero Normali(mean, std), quali valori mi aspetterei a queste probabilità?”\nNel grafico metti sull’asse x i quantili teorici e sull’asse y quelli campionari. Ogni punto confronta un quantile osservato con il quantile che la normale predirebbe.\nLa retta y = x è una linea di riferimento: se i punti stanno circa su questa retta, significa che i quantili empirici coincidono con quelli teorici e quindi la normalità è plausibile. Deviazioni sistematiche dalla retta indicano scostamenti dalla normalità (curvatura → code diverse, asimmetria → skewness).\nIn sintesi: il Q-Q plot è un controllo grafico molto diretto per valutare se i dati possono essere considerati approssimativamente Gaussiani.\n\ny_sorted = np.sort(y)                               # quantili campionari\np = (np.arange(1, n+1) - 0.5) / n                   # plotting positions (evita 0 e 1)\nq_theory = norm.ppf(p, loc=mean, scale=std)         # quantili teorici Normale(mean, std)\n\nmn = min(q_theory[0], y_sorted[0])\nmx = max(q_theory[-1], y_sorted[-1])\n\nplt.plot(q_theory, y_sorted, '.', label=\"Quantili campione vs teorici\")\nplt.plot([mn, mx], [mn, mx], 'r--', label=\"y = x (riferimento)\")\nplt.title(\"Q-Q plot\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Exams",
      "01-01: Analisi distribuzione Dirichlet"
    ]
  },
  {
    "objectID": "exams/esame_1_1.html#confronto-con-gaussiana-con-test-di-adattamento",
    "href": "exams/esame_1_1.html#confronto-con-gaussiana-con-test-di-adattamento",
    "title": "01-01: Analisi distribuzione Dirichlet",
    "section": "Confronto con Gaussiana con Test di Adattamento",
    "text": "Confronto con Gaussiana con Test di Adattamento\nQui stiamo applicando un test di adattamento chi-quadro per verificare se la variabile \\(Y\\) può essere considerata approssimativamente Gaussiana.\nStimiamo media \\(\\mu\\) e deviazione standard \\(\\sigma\\) dal campione e costruiamo una distribuzione normale \\(N(\\mu, \\sigma^2)\\). Usando questa normale, dividiamo l’asse reale in \\(k = 5\\) intervalli equiprobabili sotto l’ipotesi di normalità, calcolando i quantili ai livelli \\(1/k, 2/k, \\dots\\)\nContiamo quante osservazioni cadono in ciascun intervallo: queste sono le frequenze osservate. Se i dati fossero davvero normali, ci aspetteremmo circa \\(n/k\\) osservazioni in ogni classe: queste sono le frequenze attese.\nLa statistica chi-quadro misura quanto le frequenze osservate si discostano da quelle attese. Poiché media e varianza sono stimate dai dati, il test usa una correzione dei gradi di libertà (\\(ddof = 1\\)).\nSe il p-value del test è piccolo, rifiutiamo l’ipotesi che \\(Y\\) segua una distribuzione normale. Se invece è grande, non abbiamo evidenza contro la normalità, in accordo (o meno) con quanto osservato nei grafici.\n\nimport numpy as np\nfrom scipy.stats import norm, chisquare\n\nmu = y.mean()\nsigma = y.std(ddof=1)  # meglio ddof=1 come stima campionaria\nk = 5\n\ndi = norm(loc=mu, scale=sigma)\n\n# bordi ai quantili 1/k, 2/k, ..., (k-1)/k, con estremi infiniti\nedges = di.ppf(np.arange(1, k) / k)\nedges = np.r_[-np.inf, edges, np.inf]\n\nobsg, _ = np.histogram(y, bins=edges)\n\n# frequenze attese: n/k per ogni classe (equiprobabili sotto H0 \"normale stimata\")\nn = len(y)\nexpg = np.full(k, n / k)\n\nstatistic, pvalue = chisquare(obsg, f_exp=expg, ddof=2)\nprint(f\"Statistic: {statistic}\")\nprint(f\"P-value: {pvalue}\")\n\nStatistic: 29.149999999999995\nP-value: 4.679052587923901e-07",
    "crumbs": [
      "Exams",
      "01-01: Analisi distribuzione Dirichlet"
    ]
  },
  {
    "objectID": "exams/esame_2_1.html",
    "href": "exams/esame_2_1.html",
    "title": "02-01: Analisi distribuzione custom",
    "section": "",
    "text": "1. Consideriamo dei componenti meccanici delicati che hanno tempi di vita casuali con distribuzione a due parametri \\(a&gt;0\\), \\(b&gt;0\\), che segue questa formula:\n\\[F(t)=P(T \\le t) = 1 - e^{(-at^2-bt)}\\]\n1.1 Generare un campione con questa legge, con \\(a=0.01\\) e \\(b=0.01\\), tracciarne la densità e stimare media e deviazione standard al \\(95\\%\\) di confidenza. Stimare anche la mediana puntualmente.\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import rv_continuous\nimport pandas as pd\na = 0.01\nb = 0.01\nn_samples = 1000\n\nf = lambda t: 1 - np.exp(-a * (t**2) -b * t)\nx = np.linspace(1,20, n_samples)\nPrendi un valore \\(u \\in (0,1)\\), lo interpreti come probabilità cumulativa, e poi fai \\(t = F^{-1}(u)\\). Questo si chiama inverse transform sampling.",
    "crumbs": [
      "Exams",
      "02-01: Analisi distribuzione custom"
    ]
  },
  {
    "objectID": "exams/esame_2_1.html#stima-della-media-con-ic",
    "href": "exams/esame_2_1.html#stima-della-media-con-ic",
    "title": "02-01: Analisi distribuzione custom",
    "section": "Stima della media con IC",
    "text": "Stima della media con IC\n\nfrom scipy.stats import t\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(samples)\nmean = np.mean(samples)\ns = np.std(samples, ddof=1)          # deviazione standard campionaria\ndf = n - 1\n\nq = t.ppf(1 - alpha/2, df)     # quantile critico t\n\nmargin = q * s / np.sqrt(n)\n\nci_low  = mean - margin\nci_high = mean + margin\n\nprint(f\"IC {conf:.0%} per la media: [{ci_low:.6f}, {ci_high:.6f}]\")\n\nIC 95% per la media: [8.396207, 8.986630]\n\n\n\nfrom scipy.stats import chi2\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(samples)\ns2 = np.var(samples, ddof=1)          # varianza campionaria\ndf = n - 1\n\nq_low  = chi2.ppf(alpha/2, df)        # quantile basso\nq_high = chi2.ppf(1 - alpha/2, df)    # quantile alto\n\nci_low  = df * s2 / q_high\nci_high = df * s2 / q_low\n\nprint(f\"IC {conf:.0%} per la varianza: [{ci_low:.6f}, {ci_high:.6f}]\")\nprint(f\"IC {conf:.0%} per la std: [{np.sqrt(ci_low):.6f}, {np.sqrt(ci_high):.6f}]\")\n\nIC 95% per la varianza: [20.771081, 24.755314]\nIC 95% per la std: [4.557530, 4.975471]",
    "crumbs": [
      "Exams",
      "02-01: Analisi distribuzione custom"
    ]
  },
  {
    "objectID": "exams/esame_2_1.html#calcolo-della-mediana",
    "href": "exams/esame_2_1.html#calcolo-della-mediana",
    "title": "02-01: Analisi distribuzione custom",
    "section": "Calcolo della mediana",
    "text": "Calcolo della mediana\n\nmedian = np.median(samples)\nprint(f\"Median: {median}\")\n\nMedian: 8.06003334147718",
    "crumbs": [
      "Exams",
      "02-01: Analisi distribuzione custom"
    ]
  },
  {
    "objectID": "exams/esame_2_1.html#mle-dei-parametri-custom",
    "href": "exams/esame_2_1.html#mle-dei-parametri-custom",
    "title": "02-01: Analisi distribuzione custom",
    "section": "MLE dei parametri custom",
    "text": "MLE dei parametri custom\n1.2 Calcolare gli stimatori MLE di a e b per questa distribuzione, a partire dal campione qui a fianco.\n\nbase = pd.read_excel(\"inml25tst02.xlsx\", sheet_name=\"Es 1\", header=None)\ndf = base.drop(columns=base.columns[:-1])\ndf.columns = [\"Values\"]\n\n\ndf.head()\n\n\n\n\n\n\n\n\nValues\n\n\n\n\n0\n272\n\n\n1\n49\n\n\n2\n189\n\n\n3\n101\n\n\n4\n131\n\n\n\n\n\n\n\n\nT = df[\"Values\"].values\n\n\na_range = np.linspace(1e-3, 1e-5, 100)\nb_range = np.linspace(1e-3,1e-5, 100)\n\nlkl = []\n\n##########################\n# STIMA DI A_0 - B_0\n##########################\n\nm1 = T.mean()\nm2 = (T**2).mean()\n\na0 = 0.5 / m2\nb0 = 0.5 / m1\n\n##########################\n\nfor a_val in a_range:\n    for b_val in b_range:\n        lkl.append((a_val, b_val, dist.logpdf(T, a=a_val, b=b_val).mean()))\n\n\nlkl = np.array(lkl)\n\n\nidx = np.argsort(lkl[:, 2])[::-1]\nlkl_sorted = lkl[idx]\nbest_a, best_b, best_lkl = lkl_sorted[0]\n\n\nplt.plot(range(lkl.shape[0]), lkl[:,2])\nplt.hlines(best_lkl, 0, lkl.shape[0], 'r')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Best a: {best_a:.5f}\")\nprint(f\"Best b: {best_b:.5f}\")\nprint(f\"Best likelihood: {best_lkl:.5f}\")\n\nBest a: 0.00008\nBest b: 0.00100\nBest likelihood: -5.36027",
    "crumbs": [
      "Exams",
      "02-01: Analisi distribuzione custom"
    ]
  },
  {
    "objectID": "exams/esame_2_1.html#test-di-adattamento-per-la-distribuzione-gamma",
    "href": "exams/esame_2_1.html#test-di-adattamento-per-la-distribuzione-gamma",
    "title": "02-01: Analisi distribuzione custom",
    "section": "Test di adattamento per la distribuzione Gamma",
    "text": "Test di adattamento per la distribuzione Gamma\nSempre per il campione qui a fianco, verificare con un test di adattamento se può appartenere invece alla distribuzione gamma.\n\nimport numpy as np\nfrom scipy.stats import gamma, chisquare\n\nk = 5\nn = len(samples)\n\n# --- FIT GAMMA ---\n# Versione A (consigliata se y &gt; 0): Gamma con supporto (0, +inf) -&gt; loc fissato a 0\na, loc, scale = gamma.fit(samples, floc=0)\nddof = 2  # parametri stimati: a (shape), scale\n\n# # Versione B (più flessibile): stima anche loc (3 parametri)\n# a, loc, scale = gamma.fit(y)\n# ddof = 3  # parametri stimati: a, loc, scale\n\ndi = gamma(a=a, loc=loc, scale=scale)\n\n# bordi ai quantili 1/k, 2/k, ..., (k-1)/k, con estremi infiniti\nedges = di.ppf(np.arange(1, k) / k)\nedges = np.r_[-np.inf, edges, np.inf]\n\nobsg, _ = np.histogram(samples, bins=edges)\n\n# sotto H0 (Gamma stimata), ogni classe ha probabilità 1/k -&gt; attese tutte n/k\nexpg = np.full(k, n / k)\n\nstatistic, pvalue = chisquare(obsg, f_exp=expg, ddof=ddof)\nprint(f\"Gamma fit: a={a:.6g}, loc={loc:.6g}, scale={scale:.6g}\")\nprint(f\"Statistic: {statistic}\")\nprint(f\"P-value: {pvalue}\")\n\nGamma fit: a=2.59545, loc=0, scale=3.34871\nStatistic: 10.67\nP-value: 0.004819910112270773",
    "crumbs": [
      "Exams",
      "02-01: Analisi distribuzione custom"
    ]
  },
  {
    "objectID": "exams/esame_1_4.html",
    "href": "exams/esame_1_4.html",
    "title": "01-04: Regressione con OLS",
    "section": "",
    "text": "4.1 Si usino i dati dell’Es 2 per prevedere la variabile age, usando tutte le altre tranne group. È richiesto di considerare nonlinearità e interazioni, di fare la selezione delle variabili e di studiare i residui graficamente.",
    "crumbs": [
      "Exams",
      "01-04: Regressione con OLS"
    ]
  },
  {
    "objectID": "exams/esame_1_4.html#eda-e-pca",
    "href": "exams/esame_1_4.html#eda-e-pca",
    "title": "01-04: Regressione con OLS",
    "section": "EDA e PCA",
    "text": "EDA e PCA\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\n\ndf = base.drop(columns=base.columns[0:9])\n\ndf = df.set_index(\"id\")\ndf = df.sort_index()          # ordina per id\n\ndf_test  = df.tail(200)\ndf_train = df.iloc[:-200]\n\n# ALTERNATIVA\n# df_train = df[df[\"age\"].isna() != True]\n# df_test = df[df[\"age\"].isna()]\n\n\nX_train = df_train.drop(columns=[\"age\", \"group\"])\ny_train = df_train[\"age\"].values\n\nprint(df_train.shape)\nprint(X_train.columns)\n\nX_train = X_train.values\n\nprint(X_train.shape)\nprint(y_train.shape)\n\n(2078, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(2078, 7)\n(2078,)\n\n\n\nX_test = df_test.drop(columns=[\"age\", \"group\"])\n\nprint(df_test.shape)\nprint(X_test.columns)\n\nX_test = X_test.values\n\nprint(X_test.shape)\n\n(200, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(200, 7)\n\n\n\nmean = np.mean(X_train, axis=0)\n\nX_train = X_train - mean\nX_test = X_test - mean\n\npca = PCA(n_components=2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0, ddof=1)\n\nX_train = ( X_train - mean) / std\nX_test = ( X_test - mean) / std\n\n\nprint(np.unique(y_train, return_counts=True))\n\n(array([12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.,\n       25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37.,\n       38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50.,\n       51., 52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63.,\n       64., 65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76.,\n       77., 78., 79., 80.]), array([55, 43, 56, 45, 64, 42, 65, 39, 29, 31, 27, 30, 28, 34, 35, 32, 25,\n       23, 29, 33, 33, 28, 33, 22, 42, 24, 27, 23, 38, 34, 23, 43, 38, 33,\n       29, 32, 30, 18, 39, 36, 20, 25, 26, 39, 29, 26, 20, 28, 33, 25, 25,\n       44, 24, 22, 26, 14, 18, 14, 20, 21, 16, 20, 12, 12, 11,  9,  8,  8,\n       93]))",
    "crumbs": [
      "Exams",
      "01-04: Regressione con OLS"
    ]
  },
  {
    "objectID": "exams/esame_1_4.html#introduzione-di-termini-non-lineari",
    "href": "exams/esame_1_4.html#introduzione-di-termini-non-lineari",
    "title": "01-04: Regressione con OLS",
    "section": "Introduzione di termini non lineari",
    "text": "Introduzione di termini non lineari\n\nX_train_nl = np.column_stack(\n    [\n        X_train,  \n        X_train[:,0]**2,\n        X_train[:,0]**3,\n        X_train[:,0]**4, \n        X_train[:,1]**2,\n    ]\n)\n\nprint(X_train_nl.shape)\n\n(2078, 6)\n\n\nRistardandizziamo\n\nmean = np.mean(X_train_nl, axis=0)\nstd = np.std(X_train_nl, axis=0, ddof=1)\n\nX_train_nl = ( X_train_nl - mean) / std\n\n4.2 Si forniscano le previsioni di age per gli ultimi 200 id. Per ciascuno, si dia anche l’intervallo di predizione al 95%.\n\nimport statsmodels.api as sm\n\nAggiungiamo l’intercetta\n\nX_train_nl = sm.add_constant(X_train_nl)\nprint(X_train_nl.shape)\n\n(2078, 7)",
    "crumbs": [
      "Exams",
      "01-04: Regressione con OLS"
    ]
  },
  {
    "objectID": "exams/esame_1_4.html#analisi-delle-feature-più-rilevanti-con-ols",
    "href": "exams/esame_1_4.html#analisi-delle-feature-più-rilevanti-con-ols",
    "title": "01-04: Regressione con OLS",
    "section": "Analisi delle feature più rilevanti con OLS",
    "text": "Analisi delle feature più rilevanti con OLS\n\nmodel = sm.OLS(y_train, X_train_nl).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.158\nModel:                            OLS   Adj. R-squared:                  0.156\nMethod:                 Least Squares   F-statistic:                     64.79\nDate:                Tue, 03 Feb 2026   Prob (F-statistic):           5.86e-74\nTime:                        20:43:11   Log-Likelihood:                -9008.4\nNo. Observations:                2078   AIC:                         1.803e+04\nDf Residuals:                    2071   BIC:                         1.807e+04\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         41.5833      0.406    102.453      0.000      40.787      42.379\nx1            12.0100      0.678     17.718      0.000      10.681      13.339\nx2             2.6719      0.505      5.292      0.000       1.682       3.662\nx3             4.5892      2.766      1.659      0.097      -0.835      10.014\nx4           -36.4654      6.596     -5.529      0.000     -49.400     -23.531\nx5            25.1888      4.328      5.820      0.000      16.701      33.676\nx6             0.2163      0.893      0.242      0.809      -1.535       1.968\n==============================================================================\nOmnibus:                      191.047   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               72.993\nSkew:                           0.224   Prob(JB):                     1.41e-16\nKurtosis:                       2.198   Cond. No.                         42.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuesto summary riassume i risultati della regressione OLS, cioè come ciascuna variabile esplicativa contribuisce a spiegare la variabile risposta age, a parità delle altre.\nLa colonna coef contiene le stime dei coefficienti del modello. Ogni valore indica di quanto varia age in media per un aumento unitario della corrispondente variabile, mantenendo fisse le altre.\nIl termine const è l’intercetta e rappresenta il valore medio atteso di age quando tutte le covariate valgono zero (qui zero è da intendersi nello spazio trasformato e standardizzato).\nLa colonna std err riporta l’errore standard associato a ciascun coefficiente. Misura l’incertezza della stima. A parità di coefficiente, errori standard più piccoli indicano stime più precise.\nLa colonna t è la statistica t di Student, calcolata come rapporto tra il coefficiente stimato e il suo errore standard. Serve per testare l’ipotesi nulla che il coefficiente sia uguale a zero.\nLa colonna P&gt;|t| è il p-value del test t. Valori piccoli (tipicamente &lt; 0.05) indicano che la variabile è statisticamente significativa, cioè contribuisce in modo rilevante alla spiegazione di age una volta tenute fisse le altre covariate. Valori grandi suggeriscono invece che il termine può essere rimosso dal modello.\nLe colonne [0.025, 0.975] rappresentano l’intervallo di confidenza al 95% per ciascun coefficiente. Se l’intervallo include lo zero, la variabile non è significativa a quel livello; se non lo include, l’effetto è compatibile con un contributo diverso da zero.\nIn questo modello, tutti i termini tranne x2, x3, x6 risultano significativi. Il termine x3 e x6 hanno un p-value più elevato e un intervallo di confidenza che include lo zero, quindi non forniscono un contributo informativo aggiuntivo e possono essere eliminati per ottenere un modello più parsimonioso.",
    "crumbs": [
      "Exams",
      "01-04: Regressione con OLS"
    ]
  },
  {
    "objectID": "exams/esame_1_4.html#regressione-con-ols",
    "href": "exams/esame_1_4.html#regressione-con-ols",
    "title": "01-04: Regressione con OLS",
    "section": "Regressione con OLS",
    "text": "Regressione con OLS\n\nX_train_nl = np.column_stack(\n    [\n        X_train,  \n        X_train[:,0]**3, \n        X_train[:,0]**4, \n    ]\n)\n\nX_test_nl = np.column_stack(\n    [\n        X_test,  \n        X_test[:,0]**3, \n        X_test[:,0]**4, \n    ]\n)\n\nprint(X_train_nl.shape)\nprint(X_test_nl.shape)\n\n(2078, 4)\n(200, 4)\n\n\n\nmean = np.mean(X_train_nl, axis=0)\nstd = np.std(X_train_nl, axis=0, ddof=1)\n\nX_train_nl = ( X_train_nl - mean) / std\nX_test_nl = ( X_test_nl - mean) / std\n\n\nX_train_nl = sm.add_constant(X_train_nl)\nX_test_nl = sm.add_constant(X_test_nl)\nprint(X_train_nl.shape)\nprint(X_test_nl.shape)\n\n(2078, 5)\n(200, 5)\n\n\n\nmodel = sm.OLS(y_train, X_train_nl).fit()\n\nLa stima puntuale della risposta è data dai valori predetti \\(\\hat y_i = x_i^\\top \\hat\\beta,\\) che in statsmodels ottieni come model.fittedvalues per i dati di training, oppure tramite model.predict per nuovi dati\n\npredictions = model.predict(X_test_nl)\nresults = pd.DataFrame(predictions, index=df_test.index, columns=[\"Age\"])\nresults.head(10)\n\n\n\n\n\n\n\n\nAge\n\n\nid\n\n\n\n\n\n2079\n51.548385\n\n\n2080\n39.304501\n\n\n2081\n31.595369\n\n\n2082\n28.019782\n\n\n2083\n46.204170\n\n\n2084\n47.074833\n\n\n2085\n57.160863\n\n\n2086\n58.383387\n\n\n2087\n41.608662\n\n\n2088\n43.367784",
    "crumbs": [
      "Exams",
      "01-04: Regressione con OLS"
    ]
  },
  {
    "objectID": "exams/esame_1_4.html#analisi-dei-residui",
    "href": "exams/esame_1_4.html#analisi-dei-residui",
    "title": "01-04: Regressione con OLS",
    "section": "Analisi dei Residui",
    "text": "Analisi dei Residui\nI residui sono \\[\ne_i = y_i - \\hat y_i,\n\\] e corrispondono a model.resid.\n\nresid  = model.resid\nfitted = model.fittedvalues\nplt.figure()\nplt.scatter(fitted, resid, alpha=0.6)\nplt.axhline(0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Valori stimati\")\nplt.ylabel(\"Residui\")\nplt.title(\"Residui vs fitted\")\nplt.show()\n\n\n\n\n\n\n\n\nUn buon modello mostra una nube casuale centrata attorno a zero. Pattern a U o a ventaglio indicano rispettivamente nonlinearità non catturate o eteroschedasticità.\n\nCalcolo di \\(SSR\\), \\(S_e^2\\) e \\(S_e\\)\nLa somma dei quadrati dei residui (SSR) è \\(SSR = \\sum_{i=1}^n e_i^2,\\) ed è direttamente accessibile come model.ssr.\n\nmodel.ssr\n\nnp.float64(709900.7908922035)\n\n\nLa stima della varianza dell’errore è $ S_e^2 = , $ dove \\(n\\) è il numero di osservazioni e \\(p\\) il numero di parametri stimati (inclusa l’intercetta). In statsmodels questo valore è model.mse_resid.\n\nmodel.mse_resid\n\nnp.float64(342.4509362721676)\n\n\nLa deviazione standard dell’errore è semplicemente $ S_e = , $ che corrisponde a np.sqrt(model.mse_resid).\n\nnp.sqrt(model.mse_resid)\n\nnp.float64(18.505429913194874)\n\n\n\n\nDistribuzione dei residui\nServe a valutare simmetria e presenza di code anomale.\n\nplt.figure()\nsns.histplot(resid, kde=True)\nplt.xlabel(\"Residui\")\nplt.title(\"Distribuzione dei residui\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIntervallo di confidenza per la predizione\nPer un nuovo punto \\(x_0\\), la media attesa della predizione è \\[\n\\mathbb{E}[Y \\mid x_0] = x_0^\\top \\hat\\beta.\n\\] L’intervallo di confidenza al livello \\(1-\\alpha\\) per la media è \\[\n\\hat y_0 \\;\\pm\\; t_{n-p,\\,1-\\alpha/2}\n\\sqrt{x_0^\\top \\widehat{\\mathrm{Var}}(\\hat\\beta)\\, x_0}.\n\\]\nIn pratica, statsmodels lo calcola direttamente tramite:\n\npred = model.get_prediction(X_test_nl[0])\npred.conf_int()\n\narray([[50.22657123, 52.87019812]])\n\n\nNota. L’intervallo di confidenza è di default al 95%",
    "crumbs": [
      "Exams",
      "01-04: Regressione con OLS"
    ]
  },
  {
    "objectID": "exams/esame_1_3.html",
    "href": "exams/esame_1_3.html",
    "title": "01-03: Classificazione Logistic Regression e PyTorch",
    "section": "",
    "text": "3.1 Si usino i dati dell’Es 2 per prevedere la variabile group, usando tutte le altre tranne age. È richiesto di fornire la previsione per ciascuno degli ultimi 200 id, sia “hard” (Adult/Senior), sia “soft” con le probabilità delle due classi.",
    "crumbs": [
      "Exams",
      "01-03: Classificazione Logistic Regression e PyTorch"
    ]
  },
  {
    "objectID": "exams/esame_1_3.html#eda-e-pca",
    "href": "exams/esame_1_3.html#eda-e-pca",
    "title": "01-03: Classificazione Logistic Regression e PyTorch",
    "section": "EDA e PCA",
    "text": "EDA e PCA\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\ndf = base.drop(columns=base.columns[0:9])\n\ndf = df.set_index(\"id\")\ndf = df.sort_index()          # ordina per id\n\ndf_test  = df.tail(200)\ndf_train = df.iloc[:-200]\n\n# ALTERNATIVA\n# df_train = df[df[\"age\"].isna() != True]\n# df_test = df[df[\"age\"].isna()]\n\n\nX_train = df_train.drop(columns=[\"age\", \"group\"])\ny_train = df_train[\"group\"].values\n\nprint(df_train.shape)\nprint(X_train.columns)\n\nX_train = X_train.values\n\nprint(X_train.shape)\nprint(y_train.shape)\n\n(2078, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(2078, 7)\n(2078,)\n\n\n\nX_test = df_test.drop(columns=[\"age\", \"group\"])\n\nprint(df_test.shape)\nprint(X_test.columns)\n\nX_test = X_test.values\n\nprint(X_test.shape)\n\n(200, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(200, 7)\n\n\nATTENZIONE. PRIMA LA CALCOLI E POI LA USI, PERCHÈ X_TRAIN CAMBIA, E X_TEST HA I VALORI SBALLATI\n\nmean = np.mean(X_train, axis=0)\n\n\nX_train = X_train - mean\nX_test = X_test - mean\n\n\npca = PCA(n_components=2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\n\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0, ddof=1)\n\n\nX_train = ( X_train - mean) / std\nX_test = ( X_test - mean) / std\n\n\nprint(np.unique(y_train, return_counts=True))\ny_train = (y_train == \"Senior\").astype(int)\nprint(np.unique(y_train, return_counts=True))\n\n(array(['Adult', 'Senior'], dtype=object), array([1754,  324]))\n(array([0, 1]), array([1754,  324]))",
    "crumbs": [
      "Exams",
      "01-03: Classificazione Logistic Regression e PyTorch"
    ]
  },
  {
    "objectID": "exams/esame_1_3.html#logistic-regression-classificazione-binaria-con-sklearn",
    "href": "exams/esame_1_3.html#logistic-regression-classificazione-binaria-con-sklearn",
    "title": "01-03: Classificazione Logistic Regression e PyTorch",
    "section": "Logistic Regression (Classificazione Binaria) con sklearn",
    "text": "Logistic Regression (Classificazione Binaria) con sklearn\n\nfrom sklearn.linear_model import LogisticRegression\n\nIMPORTANTE. Le due classi sono molto sbilanciate!!! quindi mettiamo class_weight = \"balanced\"\n\nmodel = LogisticRegression(class_weight=\"balanced\")\nmodel.fit(X_train, y_train)\nhard_pred = model.predict(X_test)\nsoft_pred = model.predict_proba(X_test)\n\n\nprint(soft_pred[0])\nprint(np.unique(hard_pred, return_counts=True))\n\n[0.37712635 0.62287365]\n(array([0, 1]), array([142,  58]))\n\n\n\nresult = pd.DataFrame(soft_pred, columns=[\"Adult\", \"Senior\"])\nresult[\"Hard-Class\"] = hard_pred\nresult.head(10)\n\n\n\n\n\n\n\n\nAdult\nSenior\nHard-Class\n\n\n\n\n0\n0.377126\n0.622874\n1\n\n\n1\n0.484201\n0.515799\n1\n\n\n2\n0.580394\n0.419606\n0\n\n\n3\n0.773640\n0.226360\n0\n\n\n4\n0.466580\n0.533420\n1\n\n\n5\n0.497061\n0.502939\n1\n\n\n6\n0.344787\n0.655213\n1\n\n\n7\n0.433641\n0.566359\n1\n\n\n8\n0.532328\n0.467672\n0\n\n\n9\n0.552802\n0.447198\n0",
    "crumbs": [
      "Exams",
      "01-03: Classificazione Logistic Regression e PyTorch"
    ]
  },
  {
    "objectID": "exams/esame_1_3.html#classificazione-binaria-con-pytorch",
    "href": "exams/esame_1_3.html#classificazione-binaria-con-pytorch",
    "title": "01-03: Classificazione Logistic Regression e PyTorch",
    "section": "Classificazione Binaria con PyTorch",
    "text": "Classificazione Binaria con PyTorch\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nX_train_tensor = torch.from_numpy(X_train).to(torch.float)\ny_train_tensor = torch.from_numpy(y_train).to(torch.float)\nX_test_tensor = torch.from_numpy(X_test).to(torch.float)\n\n\nepochs = 20\nlr = 1e-4\nbatch = 16\n\n\nmodel = nn.Linear(X_train_tensor.shape[1], 1)\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n\nn_pos = (y_train_tensor == 1).sum()\nn_neg = (y_train_tensor == 0).sum()\n\npos_weight = n_neg / n_pos   # &gt;1 se i positivi sono rari\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n\n\nmodel.train()\nfor epoch in range(epochs):\n    \n    epoch_loss = 0\n    n_batches = 0\n    for data, target in train_loader:\n\n        out = model(data).squeeze()\n        loss = loss_fn(out, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n\n    print(f\"Epoch {epoch} - Loss {round(epoch_loss/n_batches, 4)}\")\n\nEpoch 0 - Loss 1.0951\nEpoch 1 - Loss 1.0943\nEpoch 2 - Loss 1.0942\nEpoch 3 - Loss 1.0934\nEpoch 4 - Loss 1.0925\nEpoch 5 - Loss 1.092\nEpoch 6 - Loss 1.0914\nEpoch 7 - Loss 1.0922\nEpoch 8 - Loss 1.0916\nEpoch 9 - Loss 1.0901\nEpoch 10 - Loss 1.0898\nEpoch 11 - Loss 1.0898\nEpoch 12 - Loss 1.0887\nEpoch 13 - Loss 1.0891\nEpoch 14 - Loss 1.0881\nEpoch 15 - Loss 1.0877\nEpoch 16 - Loss 1.0873\nEpoch 17 - Loss 1.0871\nEpoch 18 - Loss 1.0864\nEpoch 19 - Loss 1.0857\n\n\n\nhard_pred = []\nsoft_pred = []\nmodel.eval()\nwith torch.no_grad():\n\n    for sample in X_test_tensor:\n\n        logit = model(sample)\n\n        logit = torch.sigmoid(logit)\n\n        label = (logit &gt;= 0.5)\n        \n        logit = logit.detach().tolist()[0]\n        label = label.detach().to(int).tolist()[0]\n\n        soft_pred.append((1-logit, logit))\n        hard_pred.append(label)\n        \nprint(soft_pred[0])\nprint(np.unique(hard_pred, return_counts=True))\n\n(0.4343456029891968, 0.5656543970108032)\n(array([0, 1]), array([165,  35]))\n\n\n\nresult = pd.DataFrame(soft_pred, columns=[\"Adult\", \"Senior\"])\nresult[\"Hard-Class\"] = hard_pred\nresult.head(10)\n\n\n\n\n\n\n\n\nAdult\nSenior\nHard-Class\n\n\n\n\n0\n0.434346\n0.565654\n1\n\n\n1\n0.551826\n0.448174\n0\n\n\n2\n0.646597\n0.353403\n0\n\n\n3\n0.809278\n0.190722\n0\n\n\n4\n0.524993\n0.475007\n0\n\n\n5\n0.550599\n0.449401\n0\n\n\n6\n0.394198\n0.605802\n1\n\n\n7\n0.474728\n0.525272\n1\n\n\n8\n0.590127\n0.409873\n0\n\n\n9\n0.605022\n0.394978\n0",
    "crumbs": [
      "Exams",
      "01-03: Classificazione Logistic Regression e PyTorch"
    ]
  }
]