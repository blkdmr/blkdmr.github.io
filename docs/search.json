[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IML",
    "section": "",
    "text": "Esame di introduzione al machine learning"
  },
  {
    "objectID": "exams/esame1-3.html",
    "href": "exams/esame1-3.html",
    "title": "Esame 1 - Esercizio 3",
    "section": "",
    "text": "3.1 Si usino i dati dell’Es 2 per prevedere la variabile group, usando tutte le altre tranne age. È richiesto di fornire la previsione per ciascuno degli ultimi 200 id, sia “hard” (Adult/Senior), sia “soft” con le probabilità delle due classi.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\ndf = base.drop(columns=base.columns[0:9])\n\ndf = df.set_index(\"id\")\ndf = df.sort_index()          # ordina per id\n\ndf_test  = df.tail(200)\ndf_train = df.iloc[:-200]\n\n# ALTERNATIVA\n# df_train = df[df[\"age\"].isna() != True]\n# df_test = df[df[\"age\"].isna()]\nX_train = df_train.drop(columns=[\"age\", \"group\"])\ny_train = df_train[\"group\"].values\n\nprint(df_train.shape)\nprint(X_train.columns)\n\nX_train = X_train.values\n\nprint(X_train.shape)\nprint(y_train.shape)\n\n(2078, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(2078, 7)\n(2078,)\nX_test = df_test.drop(columns=[\"age\", \"group\"])\n\nprint(df_test.shape)\nprint(X_test.columns)\n\nX_test = X_test.values\n\nprint(X_test.shape)\n\n(200, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(200, 7)\nATTENZIONE. PRIMA LA CALCOLI E POI LA USI, PERCHÈ X_TRAIN CAMBIA, E X_TEST HA I VALORI SBALLATI\nmean = np.mean(X_train, axis=0)\nX_train = X_train - mean\nX_test = X_test - mean\npca = PCA(n_components=2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0, ddof=1)\nX_train = ( X_train - mean) / std\nX_test = ( X_test - mean) / std\nprint(np.unique(y_train, return_counts=True))\ny_train = (y_train == \"Senior\").astype(int)\nprint(np.unique(y_train, return_counts=True))\n\n(array(['Adult', 'Senior'], dtype=object), array([1754,  324]))\n(array([0, 1]), array([1754,  324]))",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 3"
    ]
  },
  {
    "objectID": "exams/esame1-3.html#predizione-sklearn",
    "href": "exams/esame1-3.html#predizione-sklearn",
    "title": "Esame 1 - Esercizio 3",
    "section": "Predizione (SKLEARN)",
    "text": "Predizione (SKLEARN)\n\nfrom sklearn.linear_model import LogisticRegression\n\nIMPORTANTE. Le due classi sono molto sbilanciate!!! quindi mettiamo class_weight = \"balanced\"\n\nmodel = LogisticRegression(class_weight=\"balanced\")\nmodel.fit(X_train, y_train)\nhard_pred = model.predict(X_test)\nsoft_pred = model.predict_proba(X_test)\n\n\nprint(soft_pred[0])\nprint(np.unique(hard_pred, return_counts=True))\n\n[0.37712635 0.62287365]\n(array([0, 1]), array([142,  58]))\n\n\n\nresult = pd.DataFrame(soft_pred, columns=[\"Adult\", \"Senior\"])\nresult[\"Hard-Class\"] = hard_pred\nresult.head(10)\n\n\n\n\n\n\n\n\nAdult\nSenior\nHard-Class\n\n\n\n\n0\n0.468150\n0.531850\n1\n\n\n1\n0.337974\n0.662026\n1\n\n\n2\n0.269127\n0.730873\n1\n\n\n3\n0.239190\n0.760810\n1\n\n\n4\n0.412775\n0.587225\n1\n\n\n5\n0.426228\n0.573772\n1\n\n\n6\n0.537999\n0.462001\n0\n\n\n7\n0.558185\n0.441815\n0\n\n\n8\n0.368082\n0.631918\n1\n\n\n9\n0.389986\n0.610014\n1",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 3"
    ]
  },
  {
    "objectID": "exams/esame1-3.html#predizione-pytorch",
    "href": "exams/esame1-3.html#predizione-pytorch",
    "title": "Esame 1 - Esercizio 3",
    "section": "Predizione (PyTorch)",
    "text": "Predizione (PyTorch)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nX_train_tensor = torch.from_numpy(X_train).to(torch.float)\ny_train_tensor = torch.from_numpy(y_train).to(torch.float)\nX_test_tensor = torch.from_numpy(X_test).to(torch.float)\n\n\nepochs = 20\nlr = 1e-4\nbatch = 16\n\n\nmodel = nn.Linear(X_train_tensor.shape[1], 1)\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n\nn_pos = (y_train_tensor == 1).sum()\nn_neg = (y_train_tensor == 0).sum()\n\npos_weight = n_neg / n_pos   # &gt;1 se i positivi sono rari\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n\n\nmodel.train()\nfor epoch in range(epochs):\n    \n    epoch_loss = 0\n    n_batches = 0\n    for data, target in train_loader:\n\n        out = model(data).squeeze()\n        loss = loss_fn(out, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n\n    print(f\"Epoch {epoch} - Loss {round(epoch_loss/n_batches, 4)}\")\n\nEpoch 0 - Loss 1.491\nEpoch 1 - Loss 1.4842\nEpoch 2 - Loss 1.4775\nEpoch 3 - Loss 1.4709\nEpoch 4 - Loss 1.4643\nEpoch 5 - Loss 1.4579\nEpoch 6 - Loss 1.4515\nEpoch 7 - Loss 1.4452\nEpoch 8 - Loss 1.439\nEpoch 9 - Loss 1.4329\nEpoch 10 - Loss 1.4268\nEpoch 11 - Loss 1.4209\nEpoch 12 - Loss 1.415\nEpoch 13 - Loss 1.4092\nEpoch 14 - Loss 1.4034\nEpoch 15 - Loss 1.3978\nEpoch 16 - Loss 1.3922\nEpoch 17 - Loss 1.3867\nEpoch 18 - Loss 1.3812\nEpoch 19 - Loss 1.3759\n\n\n\nhard_pred = []\nsoft_pred = []\nmodel.eval()\nwith torch.no_grad():\n\n    for sample in X_test_tensor:\n\n        logit = model(sample)\n\n        logit = torch.sigmoid(logit)\n\n        label = (logit &gt;= 0.5)\n        \n        logit = logit.detach().tolist()[0]\n        label = label.detach().to(int).tolist()[0]\n\n        soft_pred.append((1-logit, logit))\n        hard_pred.append(label)\n        \nprint(soft_pred[0])\nprint(np.unique(hard_pred, return_counts=True))\n\n(0.46814966201782227, 0.5318503379821777)\n(array([0, 1]), array([ 19, 181]))\n\n\n\nresult = pd.DataFrame(soft_pred, columns=[\"Adult\", \"Senior\"])\nresult[\"Hard-Class\"] = hard_pred\nresult.head(10)\n\n\n\n\n\n\n\n\nAdult\nSenior\nHard-Class\n\n\n\n\n0\n0.468150\n0.531850\n1\n\n\n1\n0.337974\n0.662026\n1\n\n\n2\n0.269127\n0.730873\n1\n\n\n3\n0.239190\n0.760810\n1\n\n\n4\n0.412775\n0.587225\n1\n\n\n5\n0.426228\n0.573772\n1\n\n\n6\n0.537999\n0.462001\n0\n\n\n7\n0.558185\n0.441815\n0\n\n\n8\n0.368082\n0.631918\n1\n\n\n9\n0.389986\n0.610014\n1",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 3"
    ]
  },
  {
    "objectID": "exams/esame1-1.html",
    "href": "exams/esame1-1.html",
    "title": "Esame 1 - Esercizio 1",
    "section": "",
    "text": "1.1 Generare un vettore \\((X_1, ..., X_n)\\) di dimensione \\(n=10\\) con legge di Dirichlet di parametro α=0.5 (uguale per tutte le componenti). Sia Y il valore massimo delle componenti \\(Y=max_i X_i\\). Studiare la distribuzione di Y con una simulazione MC. È richiesto di stimarne media e varianza, con intervalli di confidenza, di visualizzare la distribuzione con istogramma, kde, cdf empirica.\n\nfrom scipy.stats import dirichlet\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nn_samples = 1000\ndim = 10\nalpha = [0.5] * dim\n\n\nsamples = dirichlet.rvs(alpha=alpha, size=n_samples)\nsamples.shape\n\n(1000, 10)\n\n\n\ny = np.max(samples, axis=1)\ny.shape\n\n(1000,)\n\n\n\nfrom scipy.stats import t\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\nmean = np.mean(y)\ns = np.std(y, ddof=1)          # deviazione standard campionaria\ndf = n - 1\n\nq = t.ppf(1 - alpha/2, df)     # quantile critico t\n\nmargin = q * s / np.sqrt(n)\n\nci_low  = mean - margin\nci_high = mean + margin\n\nprint(f\"IC {conf:.0%} per la media: [{ci_low:.6f}, {ci_high:.6f}]\")\n\nIC 95% per la media: [0.372681, 0.386833]\n\n\n\nfrom scipy.stats import chi2\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\ns2 = np.var(y, ddof=1)          # varianza campionaria\ndf = n - 1\n\nq_low  = chi2.ppf(alpha/2, df)        # quantile basso\nq_high = chi2.ppf(1 - alpha/2, df)    # quantile alto\n\nci_low  = df * s2 / q_high\nci_high = df * s2 / q_low\n\nprint(f\"IC {conf:.0%} per la varianza: [{ci_low:.6f}, {ci_high:.6f}]\")\nprint(f\"IC {conf:.0%} per la std: [{np.sqrt(ci_low):.6f}, {np.sqrt(ci_high):.6f}]\")\n\nIC 95% per la varianza: [0.011934, 0.014223]\nIC 95% per la std: [0.109243, 0.119260]\n\n\n\nplt.hist(y, density=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(y)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef ecdf(val, Y_sorted):\n  return np.sum(Y_sorted &lt;= val) / len(Y_sorted)\n\ny_sorted = sorted(y)\necdf_x = np.linspace(min(y_sorted), max(y_sorted), 100)\necdf_y = []\n\nfor val in ecdf_x:\n  ecdf_y.append(ecdf(val, y_sorted))\n\nplt.plot(ecdf_x, ecdf_y)\nplt.title(\"CdF Empirica\")\nplt.show()\n\n\n\n\n\n\n\n\n1.2 Testare se la legge di Y può essere considerata approssimativamente Gaussiana sia graficamente, sia con un test di adattamento.\nQui stiamo applicando un test di adattamento chi-quadro per verificare se la variabile \\(Y\\)può essere considerata approssimativamente Gaussiana.\nPrima facciamo un controllo grafico\n\nfrom scipy.stats import norm\n\nmean = y.mean()\nstd = y.std(ddof=1)\n\nx_norm = np.linspace(mean + (4*std), mean * (-4*std),n_samples)\ny_norm = norm.pdf(x_norm, loc=mean, scale=std)\n\nplt.hist(y, density=True)\nplt.plot(x_norm, y_norm, \"--\")\nplt.show()\n\n\n\n\n\n\n\n\nQuesto codice costruisce un Q-Q plot (quantile-quantile plot) per confrontare la distribuzione empirica dei dati x con una Normale stimata sugli stessi dati.\nAll’inizio ordini i dati con np.sort(x): così ottieni i quantili campionari, cioè i valori osservati messi in ordine crescente. Ogni posizione nell’array ordinato corrisponde a un quantile empirico.\nPoi costruisci p, che sono le plotting positions. L’espressione (i - 0.5) / n assegna a ciascun punto una probabilità compresa tra 0 e 1, evitando esattamente 0 e 1. Questo serve perché i quantili teorici della normale in 0 e 1 sarebbero ±∞.\nCon norm.ppf(p, loc=mean, scale=std) calcoli i quantili teorici della distribuzione Normale con media mean e deviazione standard std, stimati dal campione. In pratica stai chiedendo: “se i dati fossero davvero Normali(mean, std), quali valori mi aspetterei a queste probabilità?”\nNel grafico metti sull’asse x i quantili teorici e sull’asse y quelli campionari. Ogni punto confronta un quantile osservato con il quantile che la normale predirebbe.\nLa retta y = x è una linea di riferimento: se i punti stanno circa su questa retta, significa che i quantili empirici coincidono con quelli teorici e quindi la normalità è plausibile. Deviazioni sistematiche dalla retta indicano scostamenti dalla normalità (curvatura → code diverse, asimmetria → skewness).\nIn sintesi: il Q-Q plot è un controllo grafico molto diretto per valutare se i dati possono essere considerati approssimativamente Gaussiani.\n\ny_sorted = np.sort(y)                               # quantili campionari\np = (np.arange(1, n+1) - 0.5) / n                   # plotting positions (evita 0 e 1)\nq_theory = norm.ppf(p, loc=mean, scale=std)         # quantili teorici Normale(mean, std)\n\nmn = min(q_theory[0], y_sorted[0])\nmx = max(q_theory[-1], y_sorted[-1])\n\nplt.plot(q_theory, y_sorted, '.', label=\"Quantili campione vs teorici\")\nplt.plot([mn, mx], [mn, mx], 'r--', label=\"y = x (riferimento)\")\nplt.title(\"Q-Q plot\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nPoi passiamo al test formale. Stimiamo media \\(\\mu\\) e deviazione standard \\(\\sigma\\) dal campione e costruiamo una distribuzione normale \\(N(\\mu, \\sigma^2)\\). Usando questa normale, dividiamo l’asse reale in \\(k = 5\\) intervalli equiprobabili sotto l’ipotesi di normalità, calcolando i quantili ai livelli \\(1/k, 2/k, \\dots\\)\nContiamo quante osservazioni cadono in ciascun intervallo: queste sono le frequenze osservate. Se i dati fossero davvero normali, ci aspetteremmo circa \\(n/k\\) osservazioni in ogni classe: queste sono le frequenze attese.\nLa statistica chi-quadro misura quanto le frequenze osservate si discostano da quelle attese. Poiché media e varianza sono stimate dai dati, il test usa una correzione dei gradi di libertà (\\(ddof = 1\\)).\nSe il p-value del test è piccolo, rifiutiamo l’ipotesi che \\(Y\\) segua una distribuzione normale. Se invece è grande, non abbiamo evidenza contro la normalità, in accordo (o meno) con quanto osservato nei grafici.\n\nimport numpy as np\nfrom scipy.stats import norm, chisquare\n\nmu = y.mean()\nsigma = y.std(ddof=1)  # meglio ddof=1 come stima campionaria\nk = 5\n\ndi = norm(loc=mu, scale=sigma)\n\n# bordi ai quantili 1/k, 2/k, ..., (k-1)/k, con estremi infiniti\nedges = di.ppf(np.arange(1, k) / k)\nedges = np.r_[-np.inf, edges, np.inf]\n\nobsg, _ = np.histogram(y, bins=edges)\n\n# frequenze attese: n/k per ogni classe (equiprobabili sotto H0 \"normale stimata\")\nn = len(y)\nexpg = np.full(k, n / k)\n\nstatistic, pvalue = chisquare(obsg, f_exp=expg, ddof=2)\nprint(f\"Statistic: {statistic}\")\nprint(f\"P-value: {pvalue}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 4\n      1 import numpy as np\n      2 from scipy.stats import norm, chisquare\n----&gt; 4 mu = y.mean()\n      5 sigma = y.std(ddof=1)  # meglio ddof=1 come stima campionaria\n      6 k = 5\n\nNameError: name 'y' is not defined",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 1"
    ]
  },
  {
    "objectID": "th/esame1-1.html",
    "href": "th/esame1-1.html",
    "title": "Esame 1 - Esercizio 1",
    "section": "",
    "text": "1.1 Generare un vettore \\((X_1, ..., X_n)\\) di dimensione \\(n=10\\) con legge di Dirichlet di parametro α=0.5 (uguale per tutte le componenti). Sia Y il valore massimo delle componenti \\(Y=max_i X_i\\). Studiare la distribuzione di Y con una simulazione MC. È richiesto di stimarne media e varianza, con intervalli di confidenza, di visualizzare la distribuzione con istogramma, kde, cdf empirica.\n\nfrom scipy.stats import dirichlet\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nn_samples = 1000\ndim = 10\nalpha = [0.5] * dim\n\n\nsamples = dirichlet.rvs(alpha=alpha, size=n_samples)\nsamples.shape\n\n(1000, 10)\n\n\n\ny = np.max(samples, axis=1)\ny.shape\n\n(1000,)\n\n\n\nfrom scipy.stats import t\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\nmean = np.mean(y)\ns = np.std(y, ddof=1)          # deviazione standard campionaria\ndf = n - 1\n\nq = t.ppf(1 - alpha/2, df)     # quantile critico t\n\nmargin = q * s / np.sqrt(n)\n\nci_low  = mean - margin\nci_high = mean + margin\n\nprint(f\"IC {conf:.0%} per la media: [{ci_low:.6f}, {ci_high:.6f}]\")\n\nIC 95% per la media: [0.372681, 0.386833]\n\n\n\nfrom scipy.stats import chi2\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\ns2 = np.var(y, ddof=1)          # varianza campionaria\ndf = n - 1\n\nq_low  = chi2.ppf(alpha/2, df)        # quantile basso\nq_high = chi2.ppf(1 - alpha/2, df)    # quantile alto\n\nci_low  = df * s2 / q_high\nci_high = df * s2 / q_low\n\nprint(f\"IC {conf:.0%} per la varianza: [{ci_low:.6f}, {ci_high:.6f}]\")\nprint(f\"IC {conf:.0%} per la std: [{np.sqrt(ci_low):.6f}, {np.sqrt(ci_high):.6f}]\")\n\nIC 95% per la varianza: [0.011934, 0.014223]\nIC 95% per la std: [0.109243, 0.119260]\n\n\n\nplt.hist(y, density=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(y)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef ecdf(val, Y_sorted):\n  return np.sum(Y_sorted &lt;= val) / len(Y_sorted)\n\ny_sorted = sorted(y)\necdf_x = np.linspace(min(y_sorted), max(y_sorted), 100)\necdf_y = []\n\nfor val in ecdf_x:\n  ecdf_y.append(ecdf(val, y_sorted))\n\nplt.plot(ecdf_x, ecdf_y)\nplt.title(\"CdF Empirica\")\nplt.show()\n\n\n\n\n\n\n\n\n1.2 Testare se la legge di Y può essere considerata approssimativamente Gaussiana sia graficamente, sia con un test di adattamento.\nQui stiamo applicando un test di adattamento chi-quadro per verificare se la variabile \\(Y\\)può essere considerata approssimativamente Gaussiana.\nPrima facciamo un controllo grafico\n\nfrom scipy.stats import norm\n\nmean = y.mean()\nstd = y.std(ddof=1)\n\nx_norm = np.linspace(mean + (4*std), mean * (-4*std),n_samples)\ny_norm = norm.pdf(x_norm, loc=mean, scale=std)\n\nplt.hist(y, density=True)\nplt.plot(x_norm, y_norm, \"--\")\nplt.show()\n\n\n\n\n\n\n\n\nQuesto codice costruisce un Q-Q plot (quantile-quantile plot) per confrontare la distribuzione empirica dei dati x con una Normale stimata sugli stessi dati.\nAll’inizio ordini i dati con np.sort(x): così ottieni i quantili campionari, cioè i valori osservati messi in ordine crescente. Ogni posizione nell’array ordinato corrisponde a un quantile empirico.\nPoi costruisci p, che sono le plotting positions. L’espressione (i - 0.5) / n assegna a ciascun punto una probabilità compresa tra 0 e 1, evitando esattamente 0 e 1. Questo serve perché i quantili teorici della normale in 0 e 1 sarebbero ±∞.\nCon norm.ppf(p, loc=mean, scale=std) calcoli i quantili teorici della distribuzione Normale con media mean e deviazione standard std, stimati dal campione. In pratica stai chiedendo: “se i dati fossero davvero Normali(mean, std), quali valori mi aspetterei a queste probabilità?”\nNel grafico metti sull’asse x i quantili teorici e sull’asse y quelli campionari. Ogni punto confronta un quantile osservato con il quantile che la normale predirebbe.\nLa retta y = x è una linea di riferimento: se i punti stanno circa su questa retta, significa che i quantili empirici coincidono con quelli teorici e quindi la normalità è plausibile. Deviazioni sistematiche dalla retta indicano scostamenti dalla normalità (curvatura → code diverse, asimmetria → skewness).\nIn sintesi: il Q-Q plot è un controllo grafico molto diretto per valutare se i dati possono essere considerati approssimativamente Gaussiani.\n\ny_sorted = np.sort(y)                               # quantili campionari\np = (np.arange(1, n+1) - 0.5) / n                   # plotting positions (evita 0 e 1)\nq_theory = norm.ppf(p, loc=mean, scale=std)         # quantili teorici Normale(mean, std)\n\nmn = min(q_theory[0], y_sorted[0])\nmx = max(q_theory[-1], y_sorted[-1])\n\nplt.plot(q_theory, y_sorted, '.', label=\"Quantili campione vs teorici\")\nplt.plot([mn, mx], [mn, mx], 'r--', label=\"y = x (riferimento)\")\nplt.title(\"Q-Q plot\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nPoi passiamo al test formale. Stimiamo media \\(\\mu\\) e deviazione standard \\(\\sigma\\) dal campione e costruiamo una distribuzione normale \\(N(\\mu, \\sigma^2)\\). Usando questa normale, dividiamo l’asse reale in \\(k = 5\\) intervalli equiprobabili sotto l’ipotesi di normalità, calcolando i quantili ai livelli \\(1/k, 2/k, \\dots\\)\nContiamo quante osservazioni cadono in ciascun intervallo: queste sono le frequenze osservate. Se i dati fossero davvero normali, ci aspetteremmo circa \\(n/k\\) osservazioni in ogni classe: queste sono le frequenze attese.\nLa statistica chi-quadro misura quanto le frequenze osservate si discostano da quelle attese. Poiché media e varianza sono stimate dai dati, il test usa una correzione dei gradi di libertà (\\(ddof = 1\\)).\nSe il p-value del test è piccolo, rifiutiamo l’ipotesi che \\(Y\\) segua una distribuzione normale. Se invece è grande, non abbiamo evidenza contro la normalità, in accordo (o meno) con quanto osservato nei grafici.\n\nimport numpy as np\nfrom scipy.stats import norm, chisquare\n\nmu = y.mean()\nsigma = y.std(ddof=1)  # meglio ddof=1 come stima campionaria\nk = 5\n\ndi = norm(loc=mu, scale=sigma)\n\n# bordi ai quantili 1/k, 2/k, ..., (k-1)/k, con estremi infiniti\nedges = di.ppf(np.arange(1, k) / k)\nedges = np.r_[-np.inf, edges, np.inf]\n\nobsg, _ = np.histogram(y, bins=edges)\n\n# frequenze attese: n/k per ogni classe (equiprobabili sotto H0 \"normale stimata\")\nn = len(y)\nexpg = np.full(k, n / k)\n\nstatistic, pvalue = chisquare(obsg, f_exp=expg, ddof=2)\nprint(statistic)  # statistic, pvalue\nprint(pvalue)\n\n29.67\n3.607790917976994e-07",
    "crumbs": [
      "Theory",
      "Esame 1 - Esercizio 1"
    ]
  },
  {
    "objectID": "exams/esame1-4.html",
    "href": "exams/esame1-4.html",
    "title": "Esame 1 - Esercizio 4",
    "section": "",
    "text": "4.1 Si usino i dati dell’Es 2 per prevedere la variabile age, usando tutte le altre tranne group. È richiesto di considerare nonlinearità e interazioni, di fare la selezione delle variabili e di studiare i residui graficamente.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\n\ndf = base.drop(columns=base.columns[0:9])\n\ndf = df.set_index(\"id\")\ndf = df.sort_index()          # ordina per id\n\ndf_test  = df.tail(200)\ndf_train = df.iloc[:-200]\n\n# ALTERNATIVA\n# df_train = df[df[\"age\"].isna() != True]\n# df_test = df[df[\"age\"].isna()]\nX_train = df_train.drop(columns=[\"age\", \"group\"])\ny_train = df_train[\"age\"].values\n\nprint(df_train.shape)\nprint(X_train.columns)\n\nX_train = X_train.values\n\nprint(X_train.shape)\nprint(y_train.shape)\n\n(2078, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(2078, 7)\n(2078,)\nX_test = df_test.drop(columns=[\"age\", \"group\"])\n\nprint(df_test.shape)\nprint(X_test.columns)\n\nX_test = X_test.values\n\nprint(X_test.shape)\n\n(200, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(200, 7)\nmean = np.mean(X_train, axis=0)\n\nX_train = X_train - mean\nX_test = X_test - mean\n\npca = PCA(n_components=2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0, ddof=1)\n\nX_train = ( X_train - mean) / std\nX_test = ( X_test - mean) / std\nprint(np.unique(y_train, return_counts=True))\n\n(array([12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.,\n       25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37.,\n       38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50.,\n       51., 52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63.,\n       64., 65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76.,\n       77., 78., 79., 80.]), array([55, 43, 56, 45, 64, 42, 65, 39, 29, 31, 27, 30, 28, 34, 35, 32, 25,\n       23, 29, 33, 33, 28, 33, 22, 42, 24, 27, 23, 38, 34, 23, 43, 38, 33,\n       29, 32, 30, 18, 39, 36, 20, 25, 26, 39, 29, 26, 20, 28, 33, 25, 25,\n       44, 24, 22, 26, 14, 18, 14, 20, 21, 16, 20, 12, 12, 11,  9,  8,  8,\n       93]))",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 4"
    ]
  },
  {
    "objectID": "exams/esame1-4.html#introduzione-di-termini-non-lineari",
    "href": "exams/esame1-4.html#introduzione-di-termini-non-lineari",
    "title": "Esame 1 - Esercizio 4",
    "section": "Introduzione di termini non lineari",
    "text": "Introduzione di termini non lineari\n\nX_train_nl = np.column_stack(\n    [\n        X_train,  \n        X_train[:,0]**2,\n        X_train[:,0]**3,\n        X_train[:,0]**4, \n        X_train[:,1]**2,\n    ]\n)\n\nprint(X_train_nl.shape)\n\n(2078, 6)\n\n\nRistardandizziamo\n\nmean = np.mean(X_train_nl, axis=0)\nstd = np.std(X_train_nl, axis=0, ddof=1)\n\nX_train_nl = ( X_train_nl - mean) / std\n\n4.2 Si forniscano le previsioni di age per gli ultimi 200 id. Per ciascuno, si dia anche l’intervallo di predizione al 95%.\n\nimport statsmodels.api as sm\n\nAggiungiamo l’intercetta\n\nX_train_nl = sm.add_constant(X_train_nl)\nprint(X_train_nl.shape)\n\n(2078, 7)\n\n\n\nmodel = sm.OLS(y_train, X_train_nl).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.158\nModel:                            OLS   Adj. R-squared:                  0.156\nMethod:                 Least Squares   F-statistic:                     64.79\nDate:                Tue, 03 Feb 2026   Prob (F-statistic):           5.86e-74\nTime:                        15:48:35   Log-Likelihood:                -9008.4\nNo. Observations:                2078   AIC:                         1.803e+04\nDf Residuals:                    2071   BIC:                         1.807e+04\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         41.5833      0.406    102.453      0.000      40.787      42.379\nx1            12.0100      0.678     17.718      0.000      10.681      13.339\nx2             2.6719      0.505      5.292      0.000       1.682       3.662\nx3             4.5892      2.766      1.659      0.097      -0.835      10.014\nx4           -36.4654      6.596     -5.529      0.000     -49.400     -23.531\nx5            25.1888      4.328      5.820      0.000      16.701      33.676\nx6             0.2163      0.893      0.242      0.809      -1.535       1.968\n==============================================================================\nOmnibus:                      191.047   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               72.993\nSkew:                           0.224   Prob(JB):                     1.41e-16\nKurtosis:                       2.198   Cond. No.                         42.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuesto summary riassume i risultati della regressione OLS, cioè come ciascuna variabile esplicativa contribuisce a spiegare la variabile risposta age, a parità delle altre.\nLa colonna coef contiene le stime dei coefficienti del modello. Ogni valore indica di quanto varia age in media per un aumento unitario della corrispondente variabile, mantenendo fisse le altre.\nIl termine const è l’intercetta e rappresenta il valore medio atteso di age quando tutte le covariate valgono zero (qui zero è da intendersi nello spazio trasformato e standardizzato).\nLa colonna std err riporta l’errore standard associato a ciascun coefficiente. Misura l’incertezza della stima. A parità di coefficiente, errori standard più piccoli indicano stime più precise.\nLa colonna t è la statistica t di Student, calcolata come rapporto tra il coefficiente stimato e il suo errore standard. Serve per testare l’ipotesi nulla che il coefficiente sia uguale a zero.\nLa colonna P&gt;|t| è il p-value del test t. Valori piccoli (tipicamente &lt; 0.05) indicano che la variabile è statisticamente significativa, cioè contribuisce in modo rilevante alla spiegazione di age una volta tenute fisse le altre covariate. Valori grandi suggeriscono invece che il termine può essere rimosso dal modello.\nLe colonne [0.025, 0.975] rappresentano l’intervallo di confidenza al 95% per ciascun coefficiente. Se l’intervallo include lo zero, la variabile non è significativa a quel livello; se non lo include, l’effetto è compatibile con un contributo diverso da zero.\nIn questo modello, tutti i termini tranne x2, x3, x6 risultano significativi. Il termine x3 e x6 hanno un p-value più elevato e un intervallo di confidenza che include lo zero, quindi non forniscono un contributo informativo aggiuntivo e possono essere eliminati per ottenere un modello più parsimonioso.",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 4"
    ]
  },
  {
    "objectID": "exams/esame1-4.html#residui",
    "href": "exams/esame1-4.html#residui",
    "title": "Esame 1 - Esercizio 4",
    "section": "Residui",
    "text": "Residui\nI residui sono \\[\ne_i = y_i - \\hat y_i,\n\\] e corrispondono a model.resid.\n\nresid  = model.resid\nfitted = model.fittedvalues\nplt.figure()\nplt.scatter(fitted, resid, alpha=0.6)\nplt.axhline(0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Valori stimati\")\nplt.ylabel(\"Residui\")\nplt.title(\"Residui vs fitted\")\nplt.show()\n\n\n\n\n\n\n\n\nUn buon modello mostra una nube casuale centrata attorno a zero. Pattern a U o a ventaglio indicano rispettivamente nonlinearità non catturate o eteroschedasticità.\n\nCalcolo di SSR, \\(S_e^2\\) e \\(S_e\\)\nLa somma dei quadrati dei residui (SSR) è \\(SSR = \\sum_{i=1}^n e_i^2,\\) ed è direttamente accessibile come model.ssr.\n\nmodel.ssr\n\nnp.float64(709900.7908922035)\n\n\nLa stima della varianza dell’errore è $ S_e^2 = , $ dove \\(n\\) è il numero di osservazioni e \\(p\\) il numero di parametri stimati (inclusa l’intercetta). In statsmodels questo valore è model.mse_resid.\n\nmodel.mse_resid\n\nnp.float64(342.4509362721676)\n\n\nLa deviazione standard dell’errore è semplicemente $ S_e = , $ che corrisponde a np.sqrt(model.mse_resid).\n\nnp.sqrt(model.mse_resid)\n\nnp.float64(18.505429913194874)\n\n\n\n\nDistribuzione dei residui\nServe a valutare simmetria e presenza di code anomale.\n\nplt.figure()\nsns.histplot(resid, kde=True)\nplt.xlabel(\"Residui\")\nplt.title(\"Distribuzione dei residui\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIntervallo di confidenza per la media della predizione\nPer un nuovo punto \\(x_0\\), la media attesa della predizione è \\[\n\\mathbb{E}[Y \\mid x_0] = x_0^\\top \\hat\\beta.\n\\] L’intervallo di confidenza al livello \\(1-\\alpha\\) per la media è \\[\n\\hat y_0 \\;\\pm\\; t_{n-p,\\,1-\\alpha/2}\n\\sqrt{x_0^\\top \\widehat{\\mathrm{Var}}(\\hat\\beta)\\, x_0}.\n\\]\nIn pratica, statsmodels lo calcola direttamente tramite:\n\npred = model.get_prediction(X_test_nl[0])\npred.conf_int()\n\narray([[50.22657123, 52.87019812]])\n\n\nNota. L’intervallo di confidenza è di default al 95%",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 4"
    ]
  },
  {
    "objectID": "exams/esame1-2.html",
    "href": "exams/esame1-2.html",
    "title": "Esame 1 - Esercizio 2",
    "section": "",
    "text": "2. I dati qui a fianco riportano misurazioni fisiologiche, scelte di stile di vita e marcatori biochimici, che si ipotizza siano fortemente correlati con l’età. Questo dataset viene usato in tutti gli esercizi restanti del compito.\nDescrizione delle variabili: group gruppo di età (senior/non-senior) age età gender sesso PA Se il soggetto svolge, nella settimana tipica, attività sportive, di fitness o ricreative di intensità moderata o vigorosa BMI Body Mass Index GLU Glucosio nel sangue da digiuno diabetic Diabetico (1 sì, 2 no, 3 incerto) GLT Glucosio orale insulin Livello di insulina\n2.1 Studiare la distribuzione delle diverse variabili, sia univariata, sia bivariata, alla ricerca di outliers e altri difetti. Si scelgano opportune trasformazioni per facilitare questo compito e per ottenere migliori risultati negli esercizi seguenti.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\n\n\nprint(base.columns)\ndf = base.drop(columns=base.columns[0:9])\ndf = df.set_index([\"id\"])\nprint(df.columns)\n\nIndex(['2. I dati qui a fianco riportano misurazioni fisiologiche, scelte di stile di vita e marcatori biochimici, che si ipotizza siano fortemente correlati con l’età.\\nQuesto dataset viene usato in tutti gli esercizi restanti del compito.\\nDescrizione delle variabili:\\ngroup gruppo di età (senior/non-senior)\\nage età\\ngender sesso\\nPA Se il soggetto svolge, nella settimana tipica, attività sportive, di fitness o ricreative di intensità moderata o vigorosa\\nBMI Body Mass Index\\nGLU Glucosio nel sangue da digiuno\\ndiabetic Diabetico (1 sì, 2 no, 3 incerto)\\nGLT Glucosio orale\\ninsulin Livello di insulina',\n       'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5',\n       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'id', 'group', 'age',\n       'gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'],\n      dtype='object')\nIndex(['group', 'age', 'gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT',\n       'insulin'],\n      dtype='object')\n\n\n\nprint(df.info())\nprint(df[\"group\"].unique())\ndf = pd.get_dummies(df, \"group\", dtype=int)\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2278 entries, 1 to 2278\nData columns (total 9 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   group     2078 non-null   object \n 1   age       2078 non-null   float64\n 2   gender    2278 non-null   int64  \n 3   PA        2278 non-null   int64  \n 4   BMI       2278 non-null   float64\n 5   GLU       2278 non-null   int64  \n 6   diabetic  2278 non-null   int64  \n 7   GLT       2278 non-null   int64  \n 8   insulin   2278 non-null   float64\ndtypes: float64(3), int64(5), object(1)\nmemory usage: 178.0+ KB\nNone\n['Adult' 'Senior' nan]\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2278 entries, 1 to 2278\nData columns (total 10 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   age           2078 non-null   float64\n 1   gender        2278 non-null   int64  \n 2   PA            2278 non-null   int64  \n 3   BMI           2278 non-null   float64\n 4   GLU           2278 non-null   int64  \n 5   diabetic      2278 non-null   int64  \n 6   GLT           2278 non-null   int64  \n 7   insulin       2278 non-null   float64\n 8   group_Adult   2278 non-null   int64  \n 9   group_Senior  2278 non-null   int64  \ndtypes: float64(3), int64(7)\nmemory usage: 195.8 KB\nNone\n\n\n\nfor col in df.columns:\n    plt.hist(df[col])\n    plt.title(col)\n    plt.show()\n    break # debug only\n\n\n\n\n\n\n\n\n\nsns.pairplot(df)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEsempio di anomalia trovata\n\nplt.plot(df[\"age\"], df[\"PA\"], \".\")\nplt.xlabel(\"age\")\nplt.ylabel(\"PA\")\nplt.show()\n\n\n\n\n\n\n\n\n\noutlier = df[df[\"PA\"] == 7]\ndf.drop(index=outlier.index, inplace=True)\n\n\n2.2 Eseguire una PCA delle variabili da L a S. Quante componenti conviene tenere? Descrivere qualitativamente il significato delle prime componenti.\n\nfrom sklearn.decomposition import PCA\n\n\nprint(df.shape)\n\ndf_train = df[df[\"age\"].isna() != True]\ndf_test = df[df[\"age\"].isna()]\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n(2277, 10)\n(2077, 10)\n(200, 10)\n\n\n\ndf_train = df_train - df_train.mean()\ndf_test = df_test - df_test.mean()\n\n\npca = PCA()\npca.fit(df_train)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \nNone\n\n\n\ncopy \nTrue\n\n\n\nwhiten \nFalse\n\n\n\nsvd_solver \n'auto'\n\n\n\ntol \n0.0\n\n\n\niterated_power \n'auto'\n\n\n\nn_oversamples \n10\n\n\n\npower_iteration_normalizer \n'auto'\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\n\nx = np.linspace(0,1, df_train.shape[1])\ny = pca.explained_variance_ratio_\n\nplt.plot(x,y)\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nthr = 0.9\nsumm = 0\nk = 0\n\nfor v_ratio in pca.explained_variance_ratio_:\n    summ += v_ratio\n    k += 1\n\n    if summ &gt;= thr: break\n\nprint(summ)\nprint(k)\n\n0.9050591901248333\n2\n\n\n\nloadings = pca.components_.T\nloadings_df = pd.DataFrame(loadings, index=df_train.columns)\nloadings_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nage\n0.149467\n0.982362\n0.030367\n0.071332\n-0.079321\n0.000377\n-0.012669\n0.012654\n-0.000773\n-8.189253e-20\n\n\ngender\n0.000087\n0.000353\n-0.007604\n0.002312\n0.010518\n0.959367\n-0.206288\n-0.191365\n0.016317\n-2.973482e-19\n\n\nPA\n0.000531\n-0.000115\n-0.002723\n0.000352\n-0.004322\n0.281361\n0.691982\n0.664798\n0.003776\n4.255316e-17\n\n\nBMI\n0.030699\n0.022993\n0.113314\n0.516372\n0.847839\n-0.007767\n0.012016\n-0.003536\n-0.000418\n-2.738812e-18\n\n\nGLU\n0.272446\n-0.057854\n0.940743\n-0.192546\n-0.016719\n0.008135\n0.002002\n-0.001913\n0.000806\n9.080420e-18\n\n\ndiabetic\n0.000029\n0.000531\n-0.000385\n0.001406\n-0.000392\n-0.016760\n-0.004472\n0.006064\n0.999830\n1.237927e-16\n\n\nGLT\n0.949014\n-0.133745\n-0.284926\n-0.010398\n0.013671\n-0.002579\n-0.001096\n0.000378\n-0.000097\n-2.785639e-18\n\n\ninsulin\n0.043079\n-0.113648\n0.141360\n0.831313\n-0.523560\n0.003886\n-0.005553\n0.000824\n-0.001226\n-1.056133e-18\n\n\ngroup_Adult\n-0.001952\n-0.011924\n0.002121\n0.000072\n0.009093\n-0.003307\n-0.489002\n0.510495\n-0.005328\n7.071068e-01\n\n\ngroup_Senior\n0.001952\n0.011924\n-0.002121\n-0.000072\n-0.009093\n0.003307\n0.489002\n-0.510495\n0.005328\n7.071068e-01\n\n\n\n\n\n\n\n\nloadings_df = loadings_df &gt;= loadings_df.mean()\nloadings_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nage\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ngender\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\nPA\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\nBMI\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\nGLU\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ndiabetic\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\nGLT\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ninsulin\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ngroup_Adult\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\ngroup_Senior\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 2"
    ]
  }
]