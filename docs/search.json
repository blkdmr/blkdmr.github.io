[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IML",
    "section": "",
    "text": "Esame di introduzione al machine learning"
  },
  {
    "objectID": "exams/esame1-3.html",
    "href": "exams/esame1-3.html",
    "title": "Esame 1 - Esercizio 3",
    "section": "",
    "text": "3.1 Si usino i dati dell’Es 2 per prevedere la variabile group, usando tutte le altre tranne age. È richiesto di fornire la previsione per ciascuno degli ultimi 200 id, sia “hard” (Adult/Senior), sia “soft” con le probabilità delle due classi.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\ndf = base.drop(columns=base.columns[0:9])\ndf = df.set_index([\"id\"])\n\ndf_test  = df.tail(200)\ndf_train = df.iloc[:-200]\n\n# ALTERNATIVA\n# df_train = df[df[\"age\"].isna() != True]\n# df_test = df[df[\"age\"].isna()]\nX_train = df_train.drop(columns=[\"age\", \"group\"])\ny_train = df_train[\"group\"].values\n\nprint(df_train.shape)\nprint(X_train.columns)\n\nX_train = X_train.values\n\nprint(X_train.shape)\nprint(y_train.shape)\n\n(2078, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(2078, 7)\n(2078,)\nX_test = df_test.drop(columns=[\"age\", \"group\"])\n\nprint(df_test.shape)\nprint(X_test.columns)\n\nX_test = X_test.values\n\nprint(X_test.shape)\n\n(200, 9)\nIndex(['gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'], dtype='object')\n(200, 7)\nATTENZIONE. PRIMA LA CALCOLI E POI LA USI, PERCHÈ X_TRAIN CAMBIA, E X_TEST HA I VALORI SBALLATI\nmean = np.mean(X_train, axis=0)\nX_train = X_train - mean\nX_test = X_test - mean\npca = PCA(n_components=2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0, ddof=1)\nX_train = ( X_train - mean) / std\nX_test = ( X_test - mean) / std\nprint(np.unique(y_train, return_counts=True))\ny_train = (y_train == \"Senior\").astype(int)\nprint(np.unique(y_train, return_counts=True))\n\n(array(['Adult', 'Senior'], dtype=object), array([1754,  324]))\n(array([0, 1]), array([1754,  324]))",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 3"
    ]
  },
  {
    "objectID": "exams/esame1-3.html#predizione-sklearn",
    "href": "exams/esame1-3.html#predizione-sklearn",
    "title": "Esame 1 - Esercizio 3",
    "section": "Predizione (SKLEARN)",
    "text": "Predizione (SKLEARN)\n\nfrom sklearn.linear_model import LogisticRegression\n\nIMPORTANTE. Le due classi sono molto sbilanciate!!! quindi mettiamo class_weight = \"balanced\"\n\nmodel = LogisticRegression(class_weight=\"balanced\")\nmodel.fit(X_train, y_train)\nhard_pred = model.predict(X_test)\nsoft_pred = model.predict_proba(X_test)\n\n\nprint(soft_pred[0])\nprint(np.unique(hard_pred, return_counts=True))\n\n[0.37712635 0.62287365]\n(array([0, 1]), array([142,  58]))\n\n\n\nresult = pd.DataFrame(soft_pred, columns=[\"Adult\", \"Senior\"])\nresult[\"Hard-Class\"] = hard_pred\nresult.head(10)\n\n\n\n\n\n\n\n\nAdult\nSenior\nHard-Class\n\n\n\n\n0\n0.468150\n0.531850\n1\n\n\n1\n0.337974\n0.662026\n1\n\n\n2\n0.269127\n0.730873\n1\n\n\n3\n0.239190\n0.760810\n1\n\n\n4\n0.412775\n0.587225\n1\n\n\n5\n0.426228\n0.573772\n1\n\n\n6\n0.537999\n0.462001\n0\n\n\n7\n0.558185\n0.441815\n0\n\n\n8\n0.368082\n0.631918\n1\n\n\n9\n0.389986\n0.610014\n1",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 3"
    ]
  },
  {
    "objectID": "exams/esame1-3.html#predizione-pytorch",
    "href": "exams/esame1-3.html#predizione-pytorch",
    "title": "Esame 1 - Esercizio 3",
    "section": "Predizione (PyTorch)",
    "text": "Predizione (PyTorch)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nX_train_tensor = torch.from_numpy(X_train).to(torch.float)\ny_train_tensor = torch.from_numpy(y_train).to(torch.float)\nX_test_tensor = torch.from_numpy(X_test).to(torch.float)\n\n\nepochs = 20\nlr = 1e-4\nbatch = 16\n\n\nmodel = nn.Linear(X_train_tensor.shape[1], 1)\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n\nn_pos = (y_train_tensor == 1).sum()\nn_neg = (y_train_tensor == 0).sum()\n\npos_weight = n_neg / n_pos   # &gt;1 se i positivi sono rari\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n\n\nmodel.train()\nfor epoch in range(epochs):\n    \n    epoch_loss = 0\n    n_batches = 0\n    for data, target in train_loader:\n\n        out = model(data).squeeze()\n        loss = loss_fn(out, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n\n    print(f\"Epoch {epoch} - Loss {round(epoch_loss/n_batches, 4)}\")\n\nEpoch 0 - Loss 1.491\nEpoch 1 - Loss 1.4842\nEpoch 2 - Loss 1.4775\nEpoch 3 - Loss 1.4709\nEpoch 4 - Loss 1.4643\nEpoch 5 - Loss 1.4579\nEpoch 6 - Loss 1.4515\nEpoch 7 - Loss 1.4452\nEpoch 8 - Loss 1.439\nEpoch 9 - Loss 1.4329\nEpoch 10 - Loss 1.4268\nEpoch 11 - Loss 1.4209\nEpoch 12 - Loss 1.415\nEpoch 13 - Loss 1.4092\nEpoch 14 - Loss 1.4034\nEpoch 15 - Loss 1.3978\nEpoch 16 - Loss 1.3922\nEpoch 17 - Loss 1.3867\nEpoch 18 - Loss 1.3812\nEpoch 19 - Loss 1.3759\n\n\n\nhard_pred = []\nsoft_pred = []\nmodel.eval()\nwith torch.no_grad():\n\n    for sample in X_test_tensor:\n\n        logit = model(sample)\n\n        logit = torch.sigmoid(logit)\n\n        label = (logit &gt;= 0.5)\n        \n        logit = logit.detach().tolist()[0]\n        label = label.detach().to(int).tolist()[0]\n\n        soft_pred.append((1-logit, logit))\n        hard_pred.append(label)\n        \nprint(soft_pred[0])\nprint(np.unique(hard_pred, return_counts=True))\n\n(0.46814966201782227, 0.5318503379821777)\n(array([0, 1]), array([ 19, 181]))\n\n\n\nresult = pd.DataFrame(soft_pred, columns=[\"Adult\", \"Senior\"])\nresult[\"Hard-Class\"] = hard_pred\nresult.head(10)\n\n\n\n\n\n\n\n\nAdult\nSenior\nHard-Class\n\n\n\n\n0\n0.468150\n0.531850\n1\n\n\n1\n0.337974\n0.662026\n1\n\n\n2\n0.269127\n0.730873\n1\n\n\n3\n0.239190\n0.760810\n1\n\n\n4\n0.412775\n0.587225\n1\n\n\n5\n0.426228\n0.573772\n1\n\n\n6\n0.537999\n0.462001\n0\n\n\n7\n0.558185\n0.441815\n0\n\n\n8\n0.368082\n0.631918\n1\n\n\n9\n0.389986\n0.610014\n1",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 3"
    ]
  },
  {
    "objectID": "th/esame1-1.html",
    "href": "th/esame1-1.html",
    "title": "Esame 1 - Esercizio 1",
    "section": "",
    "text": "1.1 Generare un vettore \\((X_1, ..., X_n)\\) di dimensione \\(n=10\\) con legge di Dirichlet di parametro α=0.5 (uguale per tutte le componenti). Sia Y il valore massimo delle componenti \\(Y=max_i X_i\\). Studiare la distribuzione di Y con una simulazione MC. È richiesto di stimarne media e varianza, con intervalli di confidenza, di visualizzare la distribuzione con istogramma, kde, cdf empirica.\n\nfrom scipy.stats import dirichlet\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nn_samples = 1000\ndim = 10\nalpha = [0.5] * dim\n\n\nsamples = dirichlet.rvs(alpha=alpha, size=n_samples)\nsamples.shape\n\n(1000, 10)\n\n\n\ny = np.max(samples, axis=1)\ny.shape\n\n(1000,)\n\n\n\nfrom scipy.stats import t\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\nmean = np.mean(y)\ns = np.std(y, ddof=1)          # deviazione standard campionaria\ndf = n - 1\n\nq = t.ppf(1 - alpha/2, df)     # quantile critico t\n\nmargin = q * s / np.sqrt(n)\n\nci_low  = mean - margin\nci_high = mean + margin\n\nprint(f\"IC {conf:.0%} per la media: [{ci_low:.6f}, {ci_high:.6f}]\")\n\nIC 95% per la media: [0.372681, 0.386833]\n\n\n\nfrom scipy.stats import chi2\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\ns2 = np.var(y, ddof=1)          # varianza campionaria\ndf = n - 1\n\nq_low  = chi2.ppf(alpha/2, df)        # quantile basso\nq_high = chi2.ppf(1 - alpha/2, df)    # quantile alto\n\nci_low  = df * s2 / q_high\nci_high = df * s2 / q_low\n\nprint(f\"IC {conf:.0%} per la varianza: [{ci_low:.6f}, {ci_high:.6f}]\")\nprint(f\"IC {conf:.0%} per la std: [{np.sqrt(ci_low):.6f}, {np.sqrt(ci_high):.6f}]\")\n\nIC 95% per la varianza: [0.011934, 0.014223]\nIC 95% per la std: [0.109243, 0.119260]\n\n\n\nplt.hist(y, density=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(y)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef ecdf(val, Y_sorted):\n  return np.sum(Y_sorted &lt;= val) / len(Y_sorted)\n\ny_sorted = sorted(y)\necdf_x = np.linspace(min(y_sorted), max(y_sorted), 100)\necdf_y = []\n\nfor val in ecdf_x:\n  ecdf_y.append(ecdf(val, y_sorted))\n\nplt.plot(ecdf_x, ecdf_y)\nplt.title(\"CdF Empirica\")\nplt.show()\n\n\n\n\n\n\n\n\n1.2 Testare se la legge di Y può essere considerata approssimativamente Gaussiana sia graficamente, sia con un test di adattamento.\nQui stiamo applicando un test di adattamento chi-quadro per verificare se la variabile \\(Y\\)può essere considerata approssimativamente Gaussiana.\nPrima facciamo un controllo grafico\n\nfrom scipy.stats import norm\n\nmean = y.mean()\nstd = y.std(ddof=1)\n\nx_norm = np.linspace(mean + (4*std), mean * (-4*std),n_samples)\ny_norm = norm.pdf(x_norm, loc=mean, scale=std)\n\nplt.hist(y, density=True)\nplt.plot(x_norm, y_norm, \"--\")\nplt.show()\n\n\n\n\n\n\n\n\nQuesto codice costruisce un Q-Q plot (quantile-quantile plot) per confrontare la distribuzione empirica dei dati x con una Normale stimata sugli stessi dati.\nAll’inizio ordini i dati con np.sort(x): così ottieni i quantili campionari, cioè i valori osservati messi in ordine crescente. Ogni posizione nell’array ordinato corrisponde a un quantile empirico.\nPoi costruisci p, che sono le plotting positions. L’espressione (i - 0.5) / n assegna a ciascun punto una probabilità compresa tra 0 e 1, evitando esattamente 0 e 1. Questo serve perché i quantili teorici della normale in 0 e 1 sarebbero ±∞.\nCon norm.ppf(p, loc=mean, scale=std) calcoli i quantili teorici della distribuzione Normale con media mean e deviazione standard std, stimati dal campione. In pratica stai chiedendo: “se i dati fossero davvero Normali(mean, std), quali valori mi aspetterei a queste probabilità?”\nNel grafico metti sull’asse x i quantili teorici e sull’asse y quelli campionari. Ogni punto confronta un quantile osservato con il quantile che la normale predirebbe.\nLa retta y = x è una linea di riferimento: se i punti stanno circa su questa retta, significa che i quantili empirici coincidono con quelli teorici e quindi la normalità è plausibile. Deviazioni sistematiche dalla retta indicano scostamenti dalla normalità (curvatura → code diverse, asimmetria → skewness).\nIn sintesi: il Q-Q plot è un controllo grafico molto diretto per valutare se i dati possono essere considerati approssimativamente Gaussiani.\n\ny_sorted = np.sort(y)                               # quantili campionari\np = (np.arange(1, n+1) - 0.5) / n                   # plotting positions (evita 0 e 1)\nq_theory = norm.ppf(p, loc=mean, scale=std)         # quantili teorici Normale(mean, std)\n\nmn = min(q_theory[0], y_sorted[0])\nmx = max(q_theory[-1], y_sorted[-1])\n\nplt.plot(q_theory, y_sorted, '.', label=\"Quantili campione vs teorici\")\nplt.plot([mn, mx], [mn, mx], 'r--', label=\"y = x (riferimento)\")\nplt.title(\"Q-Q plot\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nPoi passiamo al test formale. Stimiamo media \\(\\mu\\) e deviazione standard \\(\\sigma\\) dal campione e costruiamo una distribuzione normale \\(N(\\mu, \\sigma^2)\\). Usando questa normale, dividiamo l’asse reale in \\(k = 5\\) intervalli equiprobabili sotto l’ipotesi di normalità, calcolando i quantili ai livelli \\(1/k, 2/k, \\dots\\)\nContiamo quante osservazioni cadono in ciascun intervallo: queste sono le frequenze osservate. Se i dati fossero davvero normali, ci aspetteremmo circa \\(n/k\\) osservazioni in ogni classe: queste sono le frequenze attese.\nLa statistica chi-quadro misura quanto le frequenze osservate si discostano da quelle attese. Poiché media e varianza sono stimate dai dati, il test usa una correzione dei gradi di libertà (\\(ddof = 1\\)).\nSe il p-value del test è piccolo, rifiutiamo l’ipotesi che \\(Y\\) segua una distribuzione normale. Se invece è grande, non abbiamo evidenza contro la normalità, in accordo (o meno) con quanto osservato nei grafici.\n\nimport numpy as np\nfrom scipy.stats import norm, chisquare\n\nmu = y.mean()\nsigma = y.std(ddof=1)  # meglio ddof=1 come stima campionaria\nk = 5\n\ndi = norm(loc=mu, scale=sigma)\n\n# bordi ai quantili 1/k, 2/k, ..., (k-1)/k, con estremi infiniti\nedges = di.ppf(np.arange(1, k) / k)\nedges = np.r_[-np.inf, edges, np.inf]\n\nobsg, _ = np.histogram(y, bins=edges)\n\n# frequenze attese: n/k per ogni classe (equiprobabili sotto H0 \"normale stimata\")\nn = len(y)\nexpg = np.full(k, n / k)\n\nstatistic, pvalue = chisquare(obsg, f_exp=expg, ddof=2)\nprint(statistic)  # statistic, pvalue\nprint(pvalue)\n\n29.67\n3.607790917976994e-07",
    "crumbs": [
      "Theory",
      "Esame 1 - Esercizio 1"
    ]
  },
  {
    "objectID": "exams/esame1-1.html",
    "href": "exams/esame1-1.html",
    "title": "Esame 1 - Esercizio 1",
    "section": "",
    "text": "1.1 Generare un vettore \\((X_1, ..., X_n)\\) di dimensione \\(n=10\\) con legge di Dirichlet di parametro α=0.5 (uguale per tutte le componenti). Sia Y il valore massimo delle componenti \\(Y=max_i X_i\\). Studiare la distribuzione di Y con una simulazione MC. È richiesto di stimarne media e varianza, con intervalli di confidenza, di visualizzare la distribuzione con istogramma, kde, cdf empirica.\n\nfrom scipy.stats import dirichlet\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nn_samples = 1000\ndim = 10\nalpha = [0.5] * dim\n\n\nsamples = dirichlet.rvs(alpha=alpha, size=n_samples)\nsamples.shape\n\n(1000, 10)\n\n\n\ny = np.max(samples, axis=1)\ny.shape\n\n(1000,)\n\n\n\nfrom scipy.stats import t\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\nmean = np.mean(y)\ns = np.std(y, ddof=1)          # deviazione standard campionaria\ndf = n - 1\n\nq = t.ppf(1 - alpha/2, df)     # quantile critico t\n\nmargin = q * s / np.sqrt(n)\n\nci_low  = mean - margin\nci_high = mean + margin\n\nprint(f\"IC {conf:.0%} per la media: [{ci_low:.6f}, {ci_high:.6f}]\")\n\nIC 95% per la media: [0.372681, 0.386833]\n\n\n\nfrom scipy.stats import chi2\nimport numpy as np\n\nconf = 0.95\nalpha = 1 - conf\n\nn = len(y)\ns2 = np.var(y, ddof=1)          # varianza campionaria\ndf = n - 1\n\nq_low  = chi2.ppf(alpha/2, df)        # quantile basso\nq_high = chi2.ppf(1 - alpha/2, df)    # quantile alto\n\nci_low  = df * s2 / q_high\nci_high = df * s2 / q_low\n\nprint(f\"IC {conf:.0%} per la varianza: [{ci_low:.6f}, {ci_high:.6f}]\")\nprint(f\"IC {conf:.0%} per la std: [{np.sqrt(ci_low):.6f}, {np.sqrt(ci_high):.6f}]\")\n\nIC 95% per la varianza: [0.011934, 0.014223]\nIC 95% per la std: [0.109243, 0.119260]\n\n\n\nplt.hist(y, density=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(y)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef ecdf(val, Y_sorted):\n  return np.sum(Y_sorted &lt;= val) / len(Y_sorted)\n\ny_sorted = sorted(y)\necdf_x = np.linspace(min(y_sorted), max(y_sorted), 100)\necdf_y = []\n\nfor val in ecdf_x:\n  ecdf_y.append(ecdf(val, y_sorted))\n\nplt.plot(ecdf_x, ecdf_y)\nplt.title(\"CdF Empirica\")\nplt.show()\n\n\n\n\n\n\n\n\n1.2 Testare se la legge di Y può essere considerata approssimativamente Gaussiana sia graficamente, sia con un test di adattamento.\nQui stiamo applicando un test di adattamento chi-quadro per verificare se la variabile \\(Y\\)può essere considerata approssimativamente Gaussiana.\nPrima facciamo un controllo grafico\n\nfrom scipy.stats import norm\n\nmean = y.mean()\nstd = y.std(ddof=1)\n\nx_norm = np.linspace(mean + (4*std), mean * (-4*std),n_samples)\ny_norm = norm.pdf(x_norm, loc=mean, scale=std)\n\nplt.hist(y, density=True)\nplt.plot(x_norm, y_norm, \"--\")\nplt.show()\n\n\n\n\n\n\n\n\nQuesto codice costruisce un Q-Q plot (quantile-quantile plot) per confrontare la distribuzione empirica dei dati x con una Normale stimata sugli stessi dati.\nAll’inizio ordini i dati con np.sort(x): così ottieni i quantili campionari, cioè i valori osservati messi in ordine crescente. Ogni posizione nell’array ordinato corrisponde a un quantile empirico.\nPoi costruisci p, che sono le plotting positions. L’espressione (i - 0.5) / n assegna a ciascun punto una probabilità compresa tra 0 e 1, evitando esattamente 0 e 1. Questo serve perché i quantili teorici della normale in 0 e 1 sarebbero ±∞.\nCon norm.ppf(p, loc=mean, scale=std) calcoli i quantili teorici della distribuzione Normale con media mean e deviazione standard std, stimati dal campione. In pratica stai chiedendo: “se i dati fossero davvero Normali(mean, std), quali valori mi aspetterei a queste probabilità?”\nNel grafico metti sull’asse x i quantili teorici e sull’asse y quelli campionari. Ogni punto confronta un quantile osservato con il quantile che la normale predirebbe.\nLa retta y = x è una linea di riferimento: se i punti stanno circa su questa retta, significa che i quantili empirici coincidono con quelli teorici e quindi la normalità è plausibile. Deviazioni sistematiche dalla retta indicano scostamenti dalla normalità (curvatura → code diverse, asimmetria → skewness).\nIn sintesi: il Q-Q plot è un controllo grafico molto diretto per valutare se i dati possono essere considerati approssimativamente Gaussiani.\n\ny_sorted = np.sort(y)                               # quantili campionari\np = (np.arange(1, n+1) - 0.5) / n                   # plotting positions (evita 0 e 1)\nq_theory = norm.ppf(p, loc=mean, scale=std)         # quantili teorici Normale(mean, std)\n\nmn = min(q_theory[0], y_sorted[0])\nmx = max(q_theory[-1], y_sorted[-1])\n\nplt.plot(q_theory, y_sorted, '.', label=\"Quantili campione vs teorici\")\nplt.plot([mn, mx], [mn, mx], 'r--', label=\"y = x (riferimento)\")\nplt.title(\"Q-Q plot\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nPoi passiamo al test formale. Stimiamo media \\(\\mu\\) e deviazione standard \\(\\sigma\\) dal campione e costruiamo una distribuzione normale \\(N(\\mu, \\sigma^2)\\). Usando questa normale, dividiamo l’asse reale in \\(k = 5\\) intervalli equiprobabili sotto l’ipotesi di normalità, calcolando i quantili ai livelli \\(1/k, 2/k, \\dots\\)\nContiamo quante osservazioni cadono in ciascun intervallo: queste sono le frequenze osservate. Se i dati fossero davvero normali, ci aspetteremmo circa \\(n/k\\) osservazioni in ogni classe: queste sono le frequenze attese.\nLa statistica chi-quadro misura quanto le frequenze osservate si discostano da quelle attese. Poiché media e varianza sono stimate dai dati, il test usa una correzione dei gradi di libertà (\\(ddof = 1\\)).\nSe il p-value del test è piccolo, rifiutiamo l’ipotesi che \\(Y\\) segua una distribuzione normale. Se invece è grande, non abbiamo evidenza contro la normalità, in accordo (o meno) con quanto osservato nei grafici.\n\nimport numpy as np\nfrom scipy.stats import norm, chisquare\n\nmu = y.mean()\nsigma = y.std(ddof=1)  # meglio ddof=1 come stima campionaria\nk = 5\n\ndi = norm(loc=mu, scale=sigma)\n\n# bordi ai quantili 1/k, 2/k, ..., (k-1)/k, con estremi infiniti\nedges = di.ppf(np.arange(1, k) / k)\nedges = np.r_[-np.inf, edges, np.inf]\n\nobsg, _ = np.histogram(y, bins=edges)\n\n# frequenze attese: n/k per ogni classe (equiprobabili sotto H0 \"normale stimata\")\nn = len(y)\nexpg = np.full(k, n / k)\n\nstatistic, pvalue = chisquare(obsg, f_exp=expg, ddof=2)\nprint(statistic)  # statistic, pvalue\nprint(pvalue)\n\n29.67\n3.607790917976994e-07",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 1"
    ]
  },
  {
    "objectID": "exams/esame1-2.html",
    "href": "exams/esame1-2.html",
    "title": "Esame 1 - Esercizio 2",
    "section": "",
    "text": "2. I dati qui a fianco riportano misurazioni fisiologiche, scelte di stile di vita e marcatori biochimici, che si ipotizza siano fortemente correlati con l’età. Questo dataset viene usato in tutti gli esercizi restanti del compito.\nDescrizione delle variabili: group gruppo di età (senior/non-senior) age età gender sesso PA Se il soggetto svolge, nella settimana tipica, attività sportive, di fitness o ricreative di intensità moderata o vigorosa BMI Body Mass Index GLU Glucosio nel sangue da digiuno diabetic Diabetico (1 sì, 2 no, 3 incerto) GLT Glucosio orale insulin Livello di insulina\n2.1 Studiare la distribuzione delle diverse variabili, sia univariata, sia bivariata, alla ricerca di outliers e altri difetti. Si scelgano opportune trasformazioni per facilitare questo compito e per ottenere migliori risultati negli esercizi seguenti.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nbase = pd.read_excel(\"inml25tst01.xlsx\", sheet_name=\"Es 2\")\n\n\nprint(base.columns)\ndf = base.drop(columns=base.columns[0:9])\ndf = df.set_index([\"id\"])\nprint(df.columns)\n\nIndex(['2. I dati qui a fianco riportano misurazioni fisiologiche, scelte di stile di vita e marcatori biochimici, che si ipotizza siano fortemente correlati con l’età.\\nQuesto dataset viene usato in tutti gli esercizi restanti del compito.\\nDescrizione delle variabili:\\ngroup gruppo di età (senior/non-senior)\\nage età\\ngender sesso\\nPA Se il soggetto svolge, nella settimana tipica, attività sportive, di fitness o ricreative di intensità moderata o vigorosa\\nBMI Body Mass Index\\nGLU Glucosio nel sangue da digiuno\\ndiabetic Diabetico (1 sì, 2 no, 3 incerto)\\nGLT Glucosio orale\\ninsulin Livello di insulina',\n       'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5',\n       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'id', 'group', 'age',\n       'gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT', 'insulin'],\n      dtype='object')\nIndex(['group', 'age', 'gender', 'PA', 'BMI', 'GLU', 'diabetic', 'GLT',\n       'insulin'],\n      dtype='object')\n\n\n\nprint(df.info())\nprint(df[\"group\"].unique())\ndf = pd.get_dummies(df, \"group\", dtype=int)\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2278 entries, 1 to 2278\nData columns (total 9 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   group     2078 non-null   object \n 1   age       2078 non-null   float64\n 2   gender    2278 non-null   int64  \n 3   PA        2278 non-null   int64  \n 4   BMI       2278 non-null   float64\n 5   GLU       2278 non-null   int64  \n 6   diabetic  2278 non-null   int64  \n 7   GLT       2278 non-null   int64  \n 8   insulin   2278 non-null   float64\ndtypes: float64(3), int64(5), object(1)\nmemory usage: 178.0+ KB\nNone\n['Adult' 'Senior' nan]\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2278 entries, 1 to 2278\nData columns (total 10 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   age           2078 non-null   float64\n 1   gender        2278 non-null   int64  \n 2   PA            2278 non-null   int64  \n 3   BMI           2278 non-null   float64\n 4   GLU           2278 non-null   int64  \n 5   diabetic      2278 non-null   int64  \n 6   GLT           2278 non-null   int64  \n 7   insulin       2278 non-null   float64\n 8   group_Adult   2278 non-null   int64  \n 9   group_Senior  2278 non-null   int64  \ndtypes: float64(3), int64(7)\nmemory usage: 195.8 KB\nNone\n\n\n\nfor col in df.columns:\n    plt.hist(df[col])\n    plt.title(col)\n    plt.show()\n    break # debug only\n\n\n\n\n\n\n\n\n\nsns.pairplot(df)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEsempio di anomalia trovata\n\nplt.plot(df[\"age\"], df[\"PA\"], \".\")\nplt.xlabel(\"age\")\nplt.ylabel(\"PA\")\nplt.show()\n\n\n\n\n\n\n\n\n\noutlier = df[df[\"PA\"] == 7]\ndf.drop(index=outlier.index, inplace=True)\n\n\n2.2 Eseguire una PCA delle variabili da L a S. Quante componenti conviene tenere? Descrivere qualitativamente il significato delle prime componenti.\n\nfrom sklearn.decomposition import PCA\n\n\nprint(df.shape)\n\ndf_train = df[df[\"age\"].isna() != True]\ndf_test = df[df[\"age\"].isna()]\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n(2277, 10)\n(2077, 10)\n(200, 10)\n\n\n\ndf_train = df_train - df_train.mean()\ndf_test = df_test - df_test.mean()\n\n\npca = PCA()\npca.fit(df_train)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \nNone\n\n\n\ncopy \nTrue\n\n\n\nwhiten \nFalse\n\n\n\nsvd_solver \n'auto'\n\n\n\ntol \n0.0\n\n\n\niterated_power \n'auto'\n\n\n\nn_oversamples \n10\n\n\n\npower_iteration_normalizer \n'auto'\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\n\nx = np.linspace(0,1, df_train.shape[1])\ny = pca.explained_variance_ratio_\n\nplt.plot(x,y)\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nthr = 0.9\nsumm = 0\nk = 0\n\nfor v_ratio in pca.explained_variance_ratio_:\n    summ += v_ratio\n    k += 1\n\n    if summ &gt;= thr: break\n\nprint(summ)\nprint(k)\n\n0.9050591901248333\n2\n\n\n\nloadings = pca.components_.T\nloadings_df = pd.DataFrame(loadings, index=df_train.columns)\nloadings_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nage\n0.149467\n0.982362\n0.030367\n0.071332\n-0.079321\n0.000377\n-0.012669\n0.012654\n-0.000773\n-8.189253e-20\n\n\ngender\n0.000087\n0.000353\n-0.007604\n0.002312\n0.010518\n0.959367\n-0.206288\n-0.191365\n0.016317\n-2.973482e-19\n\n\nPA\n0.000531\n-0.000115\n-0.002723\n0.000352\n-0.004322\n0.281361\n0.691982\n0.664798\n0.003776\n4.255316e-17\n\n\nBMI\n0.030699\n0.022993\n0.113314\n0.516372\n0.847839\n-0.007767\n0.012016\n-0.003536\n-0.000418\n-2.738812e-18\n\n\nGLU\n0.272446\n-0.057854\n0.940743\n-0.192546\n-0.016719\n0.008135\n0.002002\n-0.001913\n0.000806\n9.080420e-18\n\n\ndiabetic\n0.000029\n0.000531\n-0.000385\n0.001406\n-0.000392\n-0.016760\n-0.004472\n0.006064\n0.999830\n1.237927e-16\n\n\nGLT\n0.949014\n-0.133745\n-0.284926\n-0.010398\n0.013671\n-0.002579\n-0.001096\n0.000378\n-0.000097\n-2.785639e-18\n\n\ninsulin\n0.043079\n-0.113648\n0.141360\n0.831313\n-0.523560\n0.003886\n-0.005553\n0.000824\n-0.001226\n-1.056133e-18\n\n\ngroup_Adult\n-0.001952\n-0.011924\n0.002121\n0.000072\n0.009093\n-0.003307\n-0.489002\n0.510495\n-0.005328\n7.071068e-01\n\n\ngroup_Senior\n0.001952\n0.011924\n-0.002121\n-0.000072\n-0.009093\n0.003307\n0.489002\n-0.510495\n0.005328\n7.071068e-01\n\n\n\n\n\n\n\n\nloadings_df = loadings_df &gt;= loadings_df.mean()\nloadings_df\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nage\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ngender\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\nPA\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\nBMI\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\nGLU\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ndiabetic\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\nGLT\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ninsulin\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\ngroup_Adult\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\ngroup_Senior\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue",
    "crumbs": [
      "Exams",
      "Esame 1 - Esercizio 2"
    ]
  }
]