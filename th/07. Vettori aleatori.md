# Vettori aleatori

Nel contesto del machine learning e della data science, è comune partire da un dataset organizzato in forma di tabella, dove le colonne rappresentano variabili (o caratteristiche) e le righe rappresentano le osservazioni. Quando si studiano più variabili contemporaneamente, si parla di situazione multivariata, mentre se si analizza una sola variabile si parla di situazione univariata.

Per descrivere formalmente la natura casuale delle variabili, si utilizza il concetto di **vettori aleatori**, in particolare ci si concentra quasi sempre sui primi due momenti: la media e la covarianza.

Un vettore aleatorio di dimensione $m$ è $X = (X_1, X_2, \ldots, X_m)$, le cui componenti sono variabili casuali (potenzialmente dipendenti tra loro). Ad esempio, per $m=6$ si possono considerare dati ordinati $X_{(1)} \le X_{(2)} \le \ldots \le X_{(6)}$, che non sono indipendenti.

La **media** o valore atteso di $X$ è un vettore deterministico $\mu = E[X]$ in $\mathbb{R}^m$, con componenti $\mu_i = E[X_i]$.

La **matrice di covarianza** di $X$ è $\Sigma = C(X)$, una matrice quadrata di ordine $m$, dove $\Sigma_{ij} = \mathrm{Cov}(X_i, X_j)$. Sulla diagonale di $\Sigma$ compaiono le varianze di ciascuna componente ($\Sigma_{ii} = \mathrm{Var}(X_i)$). 

![](images/Pasted image 20250307175616.png){width=500px}

La **covarianza** tra due variabili aleatorie $X$ e $Y$ si definisce come $\mathrm{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$. In particolare, $\mathrm{Cov}(X, X) = \mathrm{Var}(X)$ e, in caso di indipendenza, $\mathrm{Cov}(X,Y) = 0$.

 
**Recall.**

- $Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$

- $Cov(X, X) = Var(X)$

- $Cov(X, c) = Cov(c, Y) = 0$ (dove $c$ è una costante)

- $X, Y$ indipendenti $\implies Cov(X, Y) = 0$ (l'implicazione inversa non è sempre vera)

- $Cov(aX, bY) = ab Cov(X, Y)$ 



Se le componenti di $X$ sono indipendenti, $\Sigma$ risulta diagonale.

Per quanto riguarda la proprietà della covarianza, essa è una funzione bilineare, il che implica che si distribuisce linearmente rispetto alle somme e agli scalari. Formalmente, si ha:

$\operatorname{Cov} \left( \sum_{i=1}^{m} a_i X_i, \sum_{j=1}^{n} b_j Y_j \right) = \sum_{i=1}^{m} \sum_{j=1}^{n} a_i b_j \operatorname{Cov}(X_i, Y_j)$

 

**Esempio. Legge congiunta nel caso misto**

Consideriamo il caso di una distribuzione congiunta in cui una variabile $X$ è discreta e una variabile $Y$ è continua. Supponiamo che $X$ rappresenti il sesso di un individuo e $Y$ la sua statura. In particolare, $X$ può assumere i valori in $\{0, 1\}$, dove $P(X = 1) = 1/2$, ovvero la probabilità di essere maschio o femmina è uguale.

La variabile $Y$, che rappresenta la statura, segue una distribuzione normale (gaussiana) condizionatamente a $X$. Tuttavia, i parametri della distribuzione di $Y$ dipendono dal valore assunto da $X$. In altre parole, sia per gli uomini sia per le donne, la statura segue una distribuzione gaussiana, ma con medie e varianze differenti.


![](images/Pasted image 20250304140904.png)


L'immagine illustra un esempio di distribuzione congiunta nel caso misto, in cui la variabile $X$ è discreta e la variabile $Y$ è continua. In questo caso, $X$ rappresenta il sesso ( $0$ o $1$ ), mentre $Y$ rappresenta la statura.

Si osserva che la distribuzione condizionata di $Y$ dipende dal valore assunto da $X$:

- Se $X = 0$ (ad esempio, uomini), la statura $Y$ segue una distribuzione normale con media $\mu_0 = 175$ e varianza $\sigma^2 = 7^2$.
- Se $X = 1$ (ad esempio, donne), la statura $Y$ segue una distribuzione normale con media $\mu_1 = 168$ e la stessa varianza $\sigma^2 = 7^2$.

La densità condizionata di $Y$ dato $X$ è quindi:

$f_{Y|X} (y \mid x) = f_{\mathcal{N}(\mu_x, \sigma^2)}(y)$

dove $\mu_x$ varia in base al valore di $X$. Il grafico a sinistra mostra come la distribuzione di $Y$ cambi a seconda di $X$, mentre il grafico a destra rappresenta le due distribuzioni normali sovrapposte, evidenziando la differenza nelle medie ma la stessa dispersione intorno ai valori centrali.


### Vettori aleatori e Machine Learning

Nell’apprendimento automatico possiamo distinguere due grandi ambiti:

1. **Supervised Learning**. Si cerca di predire variabili di output (target) a partire da variabili di input (features). Gli esempi più comuni sono la regressione (prevedere valori numerici come tempo, soldi, energia) e la classificazione (prevedere la categoria di un’immagine o di un testo). Un esempio semplice è prevedere la statura $(Y)$ di una persona dal sesso $(X)$. Dopo aver stimato la relazione $\mathrm{P}(Y|X)$, si può dire che se $X = 0$ (maschio) allora $Y \sim N(175, 7^2)$, mentre se $X = 1$ (femmina) allora $Y \sim N(168, 7^2)$. In pratica, la previsione di $Y$ dipende da $X$ e dalla distribuzione condizionata $Y|X$.

2. **Unsupervised Learning**. Qui non esiste una variabile di output da predire. Si cerca piuttosto di comprendere la distribuzione multivariata dell’intero dataset, scoprendone strutture interne come cluster o pattern nascosti. Un classico esempio è la clusterizzazione di cellule, dove le righe della matrice di dati rappresentano le cellule e le colonne rappresentano i geni (fino a decine di migliaia). L’obiettivo è identificare come le cellule si raggruppano in modo naturale e individuare eventuali sottopopolazioni o tipologie cellulari.

### Trasformazioni lineari di vettori aleatori

Considerando una funzione lineare da $\mathbb{R}^n$ a $\mathbb{R}^m$ data da $y = \alpha + Bx$, con $\alpha \in \mathbb{R}^m$ e $B \in M_{m,n}$, se $X$ è un vettore aleatorio di dimensione $n$, allora $Y = \alpha + BX$ è un vettore aleatorio di dimensione $m$.

![](images/Pasted image 20250307171750.png)


La **media** di $Y$ è $\mu_Y = E[Y] = \alpha + B\,E[X] = \alpha + B\mu_X$. La **matrice di covarianza** di $Y$ è $\Sigma_Y = C(Y) = B\,C(X)\,B^T = B\,\Sigma_X\,B^T$.

#### Trasformazione che centra il vettore

Se si considera $Y = X - \mu_X$, dove $\mu_X = E[X]$, si ottiene un nuovo vettore aleatorio con media nulla, poiché $E[Y] = 0$, ma con la stessa matrice di covarianza di $X$. A livello di campione, questa operazione corrisponde a sottrarre la media campionaria da ogni osservazione.

#### Trasformazione che standardizza le variabili

Per standardizzare ciascuna componente $X_i$, la si centra e si divide per la sua deviazione standard, ottenendo

$Y_i = \dfrac{X_i - E[X_i]}{\mathrm{std}(X_i)}$

In forma matriciale, si può scrivere $Y = B(X - E[X])$, dove $B$ è una matrice diagonale con

$B_{ii} = 1/\mathrm{std}(X_i)$.

Così facendo, la media di ciascuna componente diventa zero e la nuova matrice di covarianza diventa la **matrice di correlazione** di $X$. Gli elementi della matrice di correlazione $\rho(X)$ sono

$\rho_{ij} = \dfrac{\mathrm{Cov}(X_i, X_j)}{\sqrt{\mathrm{Var}(X_i)\,\mathrm{Var}(X_j)}}.$

A livello di campione, si sostituiscono media e deviazione standard con i rispettivi stimatori empirici.

### Coefficiente di correlazione lineare

Data una coppia di variabili aleatorie $X$ e $Y$, il coefficiente di correlazione lineare $\rho(X,Y)$ è

$\rho(X,Y) = \dfrac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)\,\mathrm{Var}(Y)}} \in [-1, 1]$

Valori positivi alti ($\rho \approx 0.8$) indicano una forte correlazione positiva; valori negativi ($\rho < 0$) indicano correlazione negativa. Se $\rho = -1$ o $\rho = 1$, esiste una relazione lineare esatta tra $X$ e $Y$.

### Verso la PCA: Proprietà avanzate della matrice di covarianza

Sia $X$ un vettore aleatorio di dimensione mm con matrice di covarianza $\Sigma = C(X)$.

1. **Varianza di una somma**. La varianza di $X_1 + \dots + X_m$ è $\mathrm{Var}(X_1 + \dots + X_m) = \mathbf{1}^T\,\Sigma\,\mathbf{1}$ dove $1$ è il vettore di tutti $1$ in $\mathbb{R}^m$. Se $X_i$ sono indipendenti, i termini di covarianza sono nulli e la varianza si riduce alla somma delle varianze individuali. In generale, per un vettore $e$, la varianza di $e^T X$ è $e^T \Sigma\, e$.

2. **Varianza di una combinazione lineare**. Data una combinazione $a^T X$, con 
Le trasformazioni lineari rivestono un ruolo fondamentale nell’analisi dei vettori aleatori, soprattutto in ambito di machine learning e statistica multivariata. 


Nel caso univariato, una funzione lineare da $\mathbb{R}$ a $\mathbb{R}$ assume la forma di una retta

$y = \alpha + \beta x$ , 

dove $\alpha$ e $\beta$ sono parametri scalari e la trasformazione modifica sia la distribuzione teorica di $x$ sia quella empirica nel campione osservato.

![](images/Pasted image 20250307172015.png)

Nel caso multivariato, se $\vec{x}$ è un vettore aleatorio di dimensione $m$, una trasformazione lineare verso uno spazio di dimensione $k$ può essere scritta come 

$\,\vec{y} = \alpha + B \vec{x}\,$, 

dove $\alpha \in \mathbb{R}^k$ rappresenta il termine di bias (o spostamento) e $B \in M_{k,m}$ è la matrice che definisce la trasformazione lineare.
$a \in \mathbb{R}^m$, si ha

$\mathrm{Var}(a^T X) = a^T\,\Sigma\,a$

La matrice di covarianza $\Sigma$ è sempre simmetrica e definita non negativa, quindi possiede $m$ autovalori reali non negativi.

