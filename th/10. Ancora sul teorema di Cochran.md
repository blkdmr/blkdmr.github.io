# Ancora sul teorema di Cochran

Si consideri una sequenza di $n$ osservazioni indicizzate da $t = 1,2,\dots,n$. Per ciascun istante $t$ sono dati una variabile esplicativa $x(t)$, assunta deterministica, e una variabile risposta osservata $Y(t)$, che viene modellata come variabile aleatoria.

Si assume che le osservazioni seguano un modello gaussiano con varianza costante:
$$
Y(t) \sim \mathcal{N}(\mu(t), \sigma^2),
$$
dove $\sigma^2$ è una varianza incognita ma comune a tutte le osservazioni, mentre la media $\mu(t)$ dipende in modo deterministico dal covariato $x(t)$, cioè $\mu(t) = \mu(x(t))$.

Raccogliendo le medie in un vettore colonna $\mu \in \mathbb{R}^n$, il modello può essere riscritto in forma vettoriale come
$$
\mu =
\begin{pmatrix}
\mu(1) \\
\mu(2) \\
\vdots \\
\mu(n)
\end{pmatrix}
= C\beta.
$$
Qui $C \in \mathbb{R}^{n \times k}$ è la **design matrix** (o matrice di regressione), completamente nota, le cui colonne rappresentano funzioni fissate delle variabili esplicative $x(t)$, mentre $\beta \in \mathbb{R}^k$ è il vettore dei parametri incogniti del modello.

Dal punto di vista geometrico, questa formulazione implica che il vettore delle medie $\mu$ appartiene al sottospazio vettoriale di $\mathbb{R}^n$ generato dalle colonne di $C$, che ha dimensione $k$. È proprio questa struttura lineare che consente di applicare il **teorema di Cochran**: la stima di $\mu$ può essere vista come una proiezione ortogonale del vettore osservato $Y$ su tale sottospazio, mentre la parte residua misura la variabilità non spiegata dal modello.

Il teorema di Cochran garantisce che, sotto le ipotesi di normalità e omoschedasticità, la componente spiegata dal modello e quella residua sono indipendenti e che la somma dei quadrati dei residui, opportunamente normalizzata, segue una distribuzione chi-quadro con un numero di gradi di libertà pari a $n-k$. Questo risultato è alla base dell’inferenza statistica nei modelli di regressione lineare.



### Geometria della stima: proiezione ortogonale

Il vettore delle medie $\mu$ rappresenta la parte deterministica del modello e, per costruzione, è vincolato a appartenere allo spazio generato dalle colonne della matrice di regressione $C$. In termini geometrici, ciò significa che
$$
\mu \in \mathrm{Im}(C) \subset \mathbb{R}^n,
$$
dove $\mathrm{Im}(C)$ è un sottospazio vettoriale di dimensione al più $k$.

L’idea fondamentale della stima nei modelli lineari è di approssimare il vettore dei dati osservati $Y$ con un vettore che appartenga a questo sottospazio. La miglior approssimazione, nel senso dei **minimi quadrati**, si ottiene risolvendo il problema
$$
w(\beta) = \|Y - C\beta\|^2,
$$
che misura la distanza quadratica tra i dati osservati e quelli predetti dal modello lineare.

La soluzione di questo problema è unica (assumendo $C^\top C$ invertibile) ed è data dalla ben nota formula
$$
\hat{\beta} = (C^\top C)^{-1} C^\top Y.
$$
Geometricamente, questo equivale a proiettare ortogonalmente il vettore $Y$ sul sottospazio $\mathrm{Im}(C)$. Il vettore
$$
\hat{\mu} = C\hat{\beta}
$$
rappresenta quindi la stima della componente deterministica del modello, cioè la parte di $Y$ spiegata dalla regressione.

La differenza tra i dati osservati e la loro proiezione sul sottospazio del modello è il **residuo**
$$
Y - C\hat{\beta},
$$
la cui norma al quadrato
$$
SSR = \|Y - C\hat{\beta}\|^2
$$
misura la variabilità non spiegata dal modello.

Sotto le ipotesi di normalità e varianza costante, il teorema di Cochran garantisce che questa quantità, una volta normalizzata per la varianza $\sigma^2$, segue una distribuzione chi-quadro:
$$
\frac{SSR}{\sigma^2} \sim \chi^2(n - k),
$$
dove $n-k$ è il numero di **gradi di libertà residui**, corrispondente alla dimensione dello spazio ortogonale a $\mathrm{Im}(C)$.




### Derivazione analitica della stima

Per completezza, si può verificare la forma di $\hat{\beta}$ calcolando esplicitamente il gradiente della funzione da minimizzare:

$$
w(\beta) = \sum_{i=1}^n \left( Y_i - \sum_{j=1}^k C_{ij} \beta_j \right)^2
$$

Calcoliamo la derivata rispetto a ogni parametro $\beta_k$:

$$
\frac{\partial w}{\partial \beta_k} = -2 \sum_{i=1}^n \left( Y_i - \sum_{j=1}^k C_{ij} \beta_j \right) C_{ik}
$$

Ponendo le derivate uguali a zero, si ottiene il sistema normale:

$$
C^\top C \, \hat{\beta} = C^\top Y
$$

che fornisce:

$$
\hat{\beta} = (C^\top C)^{-1} C^\top Y
$$

come già anticipato.


## Distribuzione t di Student

Quando si stima la varianza di un campione per standardizzare una statistica, si ottiene una distribuzione di tipo t.

Siano:

$$
Z \sim \mathcal{N}(0,1), \quad W \sim \chi^2(k), \quad Z \perp W
$$

Allora:

$$
\frac{Z}{\sqrt{W/k}} \sim t(k)
$$



### Applicazione: intervallo di confidenza per la media

Nel caso del campione Gaussiano con media $\mu$, abbiamo:

$$
\bar{Y} \sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)
$$

Standardizzando:

$$
\frac{\bar{Y} - \mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1)
$$

Se però $\sigma$ è incognita e viene sostituita con lo stimatore empirico $S_X$, si ottiene:

$$
\frac{\bar{Y} - \mu}{S_X / \sqrt{n}} \sim t(n - 1)
$$

Questa è la base per costruire un intervallo di confidenza sulla media quando la varianza è ignota.



## Estensione: regressione lineare semplice

Supponiamo ora che le osservazioni $Y_i$ siano spiegate da una variabile esplicativa $x_i$:

$$
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \sigma^2)
$$

con $x_i$ noti e fissati. Questo è il modello di regressione lineare semplice.

In forma matriciale:

$$
Y = C\beta + \varepsilon
$$

con:

$$
C = \begin{pmatrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{pmatrix}, \quad
\beta = \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}
$$

Lo stimatore ai minimi quadrati è ancora:

$$
\hat{\beta} = (C^\top C)^{-1} C^\top Y
$$

La stima della varianza degli errori è:

$$
\hat{\sigma}^2 = \frac{1}{n - 2} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
$$

dove $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$.



## Test di significatività su $\beta_1$

Per valutare se la variabile $x$ ha un effetto significativo su $Y$, si considera il test:

$$
H_0: \beta_1 = 0 \quad \text{vs} \quad H_1: \beta_1 \neq 0
$$

Sotto l’ipotesi nulla, la statistica del test è:

$$
T = \frac{\hat{\beta}_1}{S_e \cdot \sqrt{\operatorname{Var}(\hat{\beta}_1)}} \sim t(n - 2)
$$

dove la varianza dello stimatore $\hat{\beta}_1$ è:

$$
\operatorname{Var}(\hat{\beta}_1) = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}
$$

In pratica, $\sigma^2$ viene sostituita da $S_e^2$, calcolata come sopra.


## Regressione Lineare Multipla



### Modello

Il modello lineare multiplo generalizza la regressione semplice consentendo di includere più variabili esplicative. Si scrive nella forma:

$$
Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p + \varepsilon
$$

dove $Y$ è la variabile risposta, $x_1, \ldots, x_p$ sono le variabili esplicative (note), $\beta_0, \ldots, \beta_p$ sono i parametri incogniti e $\varepsilon$ è il termine di errore, che si assume distribuito come:

$$
\varepsilon \sim \mathcal{N}(0, \sigma^2)
$$

cioè con media nulla e varianza costante $\sigma^2$ (omoscedasticità), e indipendente dai regressori.

Per ogni osservazione $i = 1, \ldots, n$, il modello osservazionale diventa:

$$
Y_i \sim \mathcal{N}(\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}, \sigma^2)
$$

Di norma, si pone $x_{i0} = 1$ per modellare l’intercetta come una variabile costante.



### Notazione matriciale

La forma compatta del modello è particolarmente utile per il calcolo e la teoria. Definiamo:

- $Y \in \mathbb{R}^n$: vettore delle osservazioni $Y_1, \ldots, Y_n$
- $X \in \mathbb{R}^{n \times (p+1)}$: matrice delle variabili esplicative, con la prima colonna formata da 1 (per l'intercetta)
- $\beta \in \mathbb{R}^{p+1}$: vettore dei parametri
- $\varepsilon \in \mathbb{R}^n$: vettore degli errori

Il modello diventa:

$$
Y = X\beta + \varepsilon
$$

e, sotto le ipotesi del modello, $Y$ ha distribuzione:

$$
Y \sim \mathcal{N}(X\beta, \sigma^2 I)
$$



### Stima dei parametri

La stima di $\beta$ avviene tramite il metodo dei minimi quadrati, che minimizza la somma dei quadrati dei residui:

$$
\hat{\beta} = \arg\min_\beta \|Y - X\beta\|^2
$$

La soluzione è esplicita e data da:

$$
\hat{\beta} = (X^\top X)^{-1} X^\top Y
$$

Sotto le ipotesi del modello lineare, si può dimostrare che:

$$
\hat{\beta} \sim \mathcal{N}\left(\beta, \sigma^2 (X^\top X)^{-1}\right)
$$

Per stimare la varianza $\sigma^2$, si utilizza la somma dei quadrati residui:

$$
SSR = \|Y - X\hat{\beta}\|^2
$$

Lo stimatore della varianza è allora:

$$
S_e^2 = \frac{SSR}{n - p - 1}
$$

e si ha che:

$$
\frac{SSR}{\sigma^2} \sim \chi^2(n - p - 1)
$$

Inoltre, $S_e^2$ è indipendente da $\hat{\beta}$: è una proprietà fondamentale che discende dal teorema di Cochran.



## Test di significatività per i coefficienti

Per ogni coefficiente $\beta_j$ (con $j = 1, \ldots, p$), si può verificare se il corrispondente predittore ha un contributo significativo alla spiegazione della variabile risposta. Si considerano le ipotesi:

$$
H_0: \beta_j = 0 \quad \text{(nessun effetto)} \\
H_1: \beta_j \neq 0 \quad \text{(effetto significativo)}
$$

Poiché $\hat{\beta}_j$ è una variabile normale sotto $H_0$:

$$
\hat{\beta}_j \sim \mathcal{N}\left(0, \sigma^2 [(X^\top X)^{-1}]_{jj} \right)
$$

Standardizzando, si ottiene la statistica del test:

$$
T_j = \frac{\hat{\beta}_j}{\sqrt{\sigma^2 [(X^\top X)^{-1}]_{jj}}} \sim \mathcal{N}(0,1)
$$

Poiché $\sigma^2$ è incognita, si sostituisce con $S_e^2$, ottenendo una statistica t:

$$
T_j := \frac{\hat{\beta}_j}{S_e \cdot \sqrt{[(X^\top X)^{-1}]_{jj}}} \sim t(n - p - 1)
$$

Questa statistica è nota anche come **coefficiente standardizzato** o **t-statistica**.



## Decisione e interpretazione del p-value

Per decidere se rifiutare $H_0$, si può:

- confrontare il valore assoluto di $T_j$ con un valore critico $q$ tale che:

$$
\text{Regione di accettazione} = [-q, +q], \quad q = F^{-1}_{t(n - p - 1)}\left(1 - \frac{\alpha}{2}\right)
$$

- oppure calcolare il **p-value** associato alla statistica osservata:

$$
p^* = 2 \cdot \left[1 - F_{t(n - p - 1)}(|T_j|)\right]
$$

Dove $F_t$ è la funzione di distribuzione cumulativa della $t$ di Student.  
La soglia $\alpha$ rappresenta il livello di significatività prefissato (es. 0.05).



### Interpretazione del risultato

In base al valore del p-value, possiamo trarre conclusioni qualitative:

- $p^* \geq 30\%$: il predittore $x_j$ non è utile → si può escludere dal modello.
- $p^* \leq 0.4\%$: forte evidenza contro $H_0$ → $x_j$ è altamente significativo.
- $0.4\% < p^* < 30\%$: situazione intermedia, da valutare con cautela.



## Problema dei test multipli

Quando si eseguono più test simultaneamente, il rischio di ottenere falsi positivi aumenta. Anche se tutti gli $H_0$ fossero veri, c'è comunque una probabilità crescente di ottenere almeno un risultato "significativo" per puro caso.

Formalmente:

$$
P(\text{almeno un errore di I specie} \mid H_0 \text{ tutte vere}) \leq \sum_{i=1}^n \alpha_i
$$

Se si mantiene un livello $\bar{\alpha}$ costante per ogni test, si ottiene:

$$
P(\text{errore complessivo}) \leq n \bar{\alpha}
$$

Questo fenomeno è noto come **inflazione dell’errore di I specie**.



## Correzione di Bonferroni

Per contenere l’errore complessivo entro un valore fissato $\bar{\alpha}$ (tipicamente 5%), si può correggere il livello di ciascun test dividendo per il numero totale di confronti $n$:

$$
\alpha_i = \frac{\bar{\alpha}}{n}, \quad \text{per ogni } i
$$

Questa è la **correzione di Bonferroni**, una tecnica semplice ma conservativa.



### Conseguenze della correzione

- Il livello di errore di I specie è mantenuto sotto controllo
- Tuttavia, si riduce la potenza dei test, ovvero aumenta la probabilità di non rilevare effetti reali (errore di II specie)



## Buone pratiche sperimentali

Per evitare conclusioni errate:

- è consigliabile **pianificare in anticipo** il numero e il tipo di test statistici,
- definire le ipotesi da verificare **prima** di esaminare i dati,
- evitare selezioni post-hoc (data dredging, cherry-picking) che portano a interpretazioni spurie.

