# Trasformazione PCA e analisi delle componenti principali

La PCA (Principal Component Analysis) è una trasformazione lineare che consente di selezionare le componenti più importanti dei dati, semplificando la loro struttura mediante una rotazione degli assi di riferimento. Consideriamo una matrice di covarianza $\Sigma = C(X)$:

- $\Sigma$ è quadrata, simmetrica e definita non negativa, e quindi ammette una decomposizione spettrale.

- Possiede $m$ autovettori $v_j \in \mathbb{R}^m$, con corrispondenti autovalori $\lambda_j$, tali che:

$$\Sigma v_j = \lambda_j v_j, \quad j=1,\dots,m$$

Questi autovettori sono ortogonali e possono essere normalizzati a norma 1:

$$v_j \cdot v_k = \begin{cases} 1 & \text{se } j=k\\ 0 & \text{altrimenti} \end{cases}$$

Costruiamo la matrice degli autovettori:

$$V = [v_1, v_2, \dots, v_m], \quad V^T V = I \quad \text{e} \quad V^T = V^{-1}$$

Quindi $V$ è una matrice ortogonale che rappresenta una rotazione. La matrice di covarianza può essere scritta come:

$$\Sigma = \sum_{i=1}^m \lambda_i v_i v_i^T$$

Utilizzando la matrice $V$, definiamo la trasformazione lineare:

$$Y = V^T X$$

Questa operazione ruota e scompone le componenti originali di $X$, diagonalizzando la matrice di covarianza:

$$C(Y) = V^T \Sigma V = \Lambda = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_m)$$

Esistono due approcci principali alla PCA:

- **Primo approccio:** Traslazione dei dati nell'origine, rotazione tramite PCA, poi standardizzazione.

- **Secondo approccio:** Traslazione nell'origine, standardizzazione preliminare, rotazione PCA basata sulla matrice di correlazione $g(X)$, quindi standardizzazione finale.

Il secondo metodo è preferibile quando le variabili hanno scale molto diverse, perché permette un'analisi più equilibrata.

## Factor Analysis

L'analisi fattoriale sintetizza un insieme di variabili in un numero ridotto di "fattori" significativi e indipendenti. 

Ad esempio, in un dataset relativo alle misure corporee, potremmo identificare fattori come:

- $f_1$: dimensione globale del corpo

- $f_2$: larghezza

- $f_3$: età o sviluppo muscolare

- $f_4$: differenze specifiche

Quando le variabili sono molte, alcune componenti PCA avranno varianza molto bassa e potranno essere ignorate senza perdere molta informazione, riducendo la dimensionalità da $m$ a $k$.

I coefficienti associati a ciascun fattore (factor loadings) indicano l'importanza e il segno delle variabili originali in ciascun fattore.

La PCA mantiene la somma delle varianze costante:

$$\mathrm{Var}(x_1) + \dots + \mathrm{Var}(x_m) = \mathrm{Var}(y_1) + \dots + \mathrm{Var}(y_m)$$

Gli autovalori $\lambda_j$ sono solitamente ordinati in modo decrescente, e uno scree plot aiuta a decidere quante componenti trattenere.

## Distribuzione Gaussiana Multidimensionale

Un vettore $X=(X_1, \dots, X_p)$ con distribuzione gaussiana multidimensionale $N(\mu, \Sigma)$ è definito dai parametri:

- $\mu$: vettore medio ($\mathbb{R}^p$)
- $\Sigma$: matrice di covarianza ($\mathbb{R}^{p \times p}$) 

La funzione di densità (pdf) per $X$ è:

$$f_X(x) = \frac{1}{(2\pi)^{p/2}\sqrt{\det \Sigma}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)\right)$$

Se le componenti di $X$ sono indipendenti, $\Sigma$ diventa diagonale, e le curve di livello sono ellissi allineate con gli assi. Altrimenti, avremo rotazioni e allungamenti delle ellissi.

## Legge di Dirichlet

La legge di Dirichlet, spesso nota come distribuzione di Dirichlet, è una distribuzione di probabilità su insiemi di proporzioni. Immagina di avere un insieme di categorie e di voler modellare come si distribuisce una probabilità tra di esse. La distribuzione di Dirichlet ti dà un insieme di proporzioni (che sommano a 1), ed è spesso usata come una distribuzione a priori in modelli bayesiani. In pratica, ti permette di esprimere in modo flessibile la tua incertezza sulle probabilità di categorie, ed è molto usata per problemi come il clustering o le distribuzioni di temi nei modelli di topic.

**Nota.** Immagina di voler dividere una torta tra amici. La distribuzione di Dirichlet ti aiuta a modellare, prima di vederli, come pensi che la torta verrà divisa. Magari pensi che un amico ne prenderà di più, altri di meno, ma non sai esattamente quanto. La Dirichlet ti dà una “famiglia di possibilità” su come si potrebbero distribuire le fette. Una volta che dividi la torta, avrai proporzioni concrete. In sostanza, è un modo per rappresentare le tue aspettative iniziali su come suddividere qualcosa.

La distribuzione di Dirichlet genera vettori $X=(X_1, \dots, X_m)$ tali che $X_i \geq 0$ e $\sum X_i=1$. Scriviamo $X \sim \text{Dirichlet}(\alpha)$ con parametri $\alpha_i > 0$. La sua pdf è:

$$f_X(x) = C_\alpha x_1^{\alpha_1-1}\dots x_m^{\alpha_m-1}$$

Dove la media di $X$ è:

$$\mathbb{E}(X) = \frac{\alpha}{\sum_{i=1}^m \alpha_i}$$

Per $m=2$ la Dirichlet coincide con la distribuzione Beta.

## Distribuzione Multinomiale

La multinomiale è quella distribuzione che descrive la probabilità di contare quante volte si verificano diversi risultati quando fai molte prove, un po' come un’estensione della binomiale. Se tiri un dado molte volte e vuoi sapere quanti “1”, “2”, “3” e così via escono, la multinomiale ti dà proprio la probabilità di quelle combinazioni. La cosa fondamentale è che hai più categorie di esito e vuoi sapere come si distribuiscono i conteggi.

Generalizzazione della distribuzione binomiale con $m$ categorie. Sia $X=(X_1, \dots, X_m)$ multinomiale con parametri $n$ e $p=(p_1,\dots,p_m)$, con $\sum p_i=1$. Ogni $X_i$ indica il conteggio degli esiti nella categoria $i$ su $n$ prove:

$$P(X=x)=\frac{n!}{x_1!\dots x_m!}p_1^{x_1}\dots p_m^{x_m},\quad \sum x_i=n$$

Le componenti $X_i$ sono variabili binomiali dipendenti, e la multinomiale permette di calcolare probabilità in esperimenti con più di due possibili risultati.