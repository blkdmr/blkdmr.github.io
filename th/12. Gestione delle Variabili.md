# Gestione delle Variabili

Quando si costruisce un modello statistico (non solo di regressione), è fondamentale gestire correttamente le **variabili** in base alla loro natura. Le principali tipologie sono:

- **Dichotomiche**: due soli livelli (es. sì/no, vero/falso, maschio/femmina)
- **Categoriche**: più di due livelli (es. stagione, regione, titolo di studio)
- **Numeriche**: variabili quantitative (continue o discrete)



## Variabili Dicotomiche

### Come variabili esplicative (di ingresso)

Le variabili dicotomiche possono essere codificate con qualsiasi coppia di numeri, ma per semplicità (e interpretabilità) si usa quasi sempre **0 e 1**.

Se la variabile $x_i = 1$ rappresenta "femmina", allora il coefficiente $\beta_1$ misura quanto, **in media**, le donne differiscono dagli uomini (gruppo di riferimento) nella variabile risposta $Y$.

Questo tipo di variabile è molto comune anche nei **disegni sperimentali**, perché consente confronti semplici e immediati.

### Come variabili di risposta

Le dicotomiche **non sono adatte** come risposta in un modello di regressione lineare classica. In questi casi si usano modelli specifici:

- **Regressione logistica**: se la risposta è binaria
- **Analisi discriminante**: in alternativa alla regressione
- **Modelli di classificazione**: alberi, reti neurali, random forest, ecc.



## Variabili Categoriche

Le variabili categoriche possono essere suddivise in due sottotipi:

### Nominali

Non hanno un ordine naturale. Alcuni esempi:
- Regione di provenienza, tipo di carburante, stagione dell’anno, tipo di veicolo...

### Ordinali

Hanno un **ordine intrinseco**, anche se non sempre le "distanze" tra le categorie sono significative.

Esempi: livello di istruzione, gravità di un sintomo, classifica sportiva (I, II, III...).

A volte si codificano gli ordinali con numeri interi (1, 2, 3...), ma bisogna fare attenzione: ciò introduce implicitamente una distanza tra i livelli che potrebbe non essere reale.

Esempio: codifica del **titolo di studio** con diversi criteri.

| Titolo               | Valore 1 | Valore 2 | Valore 3 | Note              |
|-|-|-|-|-|
| Nessuno              | 1        | 0        | 0        | 5 anni di scuola  |
| Elementari           | 2        | 10       | 0        |                   |
| Medie                | 3        | 20       | 8        |                   |
| Maturità             | 4        | 30       | ≠13      |                   |
| Laurea               | 5        | 40       | 16       |                   |
| Laurea magistrale    | 6        | 50       | 18       |                   |
| Dottorato            | 7        | 60       | 21       |                   |



## Variabili categoriche come risposta

Quando la variabile risposta è categorica (es. classificazione), **non** si usa la regressione lineare. Si passa a:

- Regressione logistica (binaria o multinomiale)
- Alberi decisionali, foreste casuali, reti neurali, ecc.



## Variabili categoriche come ingresso

In regressione, una variabile categorica con $k$ categorie va trasformata in **$k-1$ variabili dicotomiche** (dummy), altrimenti si creerebbe collinearità.

### Codifica one-hot (con categoria di riferimento)

Si sceglie una categoria come **default** (es. inverno) e si costruiscono $k-1$ colonne binarie.

Esempio con 4 stagioni:

| x₀ | primavera | estate | autunno | inverno (default) |
|-|--|--||-|
| 1  | 1         | 0      | 0       | 0                 |
| 1  | 0         | 1      | 0       | 0                 |
| 1  | 0         | 0      | 1       | 0                 |
| 1  | 0         | 0      | 0       | 1 (implicita)     |

L’equazione del modello diventa:

$$
Y = \beta_0 + \beta_{\text{pri}} x_{\text{pri}} + \beta_{\text{est}} x_{\text{est}} + \beta_{\text{aut}} x_{\text{aut}} + \varepsilon
$$

La categoria "inverno" è inclusa **nel termine noto $\beta_0$**. Gli altri coefficienti esprimono le **differenze rispetto al default**.

Esempio:

| Coefficiente | Valore |
|--|--|
| $\beta_0$ (inverno) | 16.4 |
| $\beta_1$ (primavera) | 5.4 |
| $\beta_2$ (estate) | 6.7 |
| $\beta_3$ (autunno) | 1.8 |

Allora:

| Stagione   | Y atteso |
||-|
| Inverno    | 16.4     |
| Primavera  | 21.8     |
| Estate     | 23.1     |
| Autunno    | 18.2     |

Si possono testare ipotesi come:

- $H_0: \beta_3 = 0$ → autunno = inverno
- $H_0: \beta_1 = \beta_2$ → primavera = estate



## Cosa succede nello stepwise

Quando si usa la **stepwise backward**, se una variabile dummy viene eliminata significa che **non è distinguibile dal default** (es. $\beta_3 = 0$ per autunno).

La nuova codifica raggruppa autunno e inverno:

| Stagione   | primavera | estate |
||--|--|
| Primavera  | 1         | 0      |
| Estate     | 0         | 1      |
| Autunno    | 0         | 0      |
| Inverno    | 0         | 0      |

Ora autunno è trattato **come inverno**.

Se si vuole confrontare direttamente **primavera ed estate**, si può:

1. Fare un **test manuale**: $H_0: \beta_1 = \beta_2$
2. Cambiare il **default** e rifare la codifica (es. default = estate)

**Attenzione.** Ogni categoria aggiunta aumenta $p$ → occhio al **rapporto $n/p$**. È possibile accorpare categorie rare in macro-categorie, o anche eliminare alcune righe.



## Variabili Numeriche

### Tipo "differenza"

Non richiedono trasformazioni particolari. Esempi: statura, peso, QI, consumi.

### Tipo "rapporto"

Quando le variabili sono **positive** e distribuite su ordini di grandezza diversi (es. reddito, popolazione), conviene usare il **logaritmo**.

Esempio:

$$
Y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon
$$

Se $x_1$ è di tipo rapporto, si può usare $\log x_1$ per linearizzare la relazione o normalizzare la distribuzione.

### Effetti delle trasformazioni

- Le **trasformazioni lineari** (es. cambio unità) **non alterano** le conclusioni statistiche (test, $R^2$), ma cambiano i coefficienti.
- Le **trasformazioni non lineari** (es. logaritmo, radice quadrata) possono migliorare la **normalità** dei residui o la **linearità** della relazione.

Esempio di regressione logaritmica su $Y$:

$$
\log Y = \beta_0 + \beta_1 x_1 + \cdots \quad \Rightarrow \quad Y = e^{\beta_0} \cdot e^{\beta_1 x_1} \cdot \cdots
$$

Se $\beta_1 = \log(1.4)$, significa che l’effetto di $x_1$ è un **+40%** su $Y$.