# Selezione di Variabili e Regolarizzazione

## Metodi Stepwise

I **metodi stepwise** sono procedure automatiche per la selezione delle variabili in un modello di regressione. L’obiettivo è costruire un modello che sia sufficientemente **parsimonioso**, evitando variabili inutili, ma che mantenga una buona **capacità di adattamento** ai dati.

Nel **backward stepwise** si parte dal modello più complesso possibile, che include tutte le $p$ variabili esplicative. A ogni iterazione viene rimossa la variabile meno significativa, tipicamente quella con il contributo statistico più debole, finché tutte le variabili rimanenti soddisfano un criterio di significatività prefissato.

Nel **forward stepwise**, invece, si parte dal modello nullo, privo di regressori. Le variabili vengono aggiunte una alla volta, scegliendo a ogni passo quella che produce il maggiore miglioramento del modello secondo un criterio statistico. Il procedimento si arresta quando nessuna variabile candidata apporta un miglioramento ritenuto significativo.

Il metodo **misto** (o stepwise propriamente detto) combina i due approcci. L’inclusione delle variabili avviene in modo analogo al forward, ma dopo ogni nuova aggiunta si verifica se alcune delle variabili già presenti siano diventate non significative. In tal caso, queste possono essere rimosse, rendendo il procedimento più flessibile.

Nella pratica, i software statistici non si basano sempre direttamente sui p-value, ma utilizzano spesso soglie sul test **F**, note come *F to enter* e *F to remove*. Tali soglie sono equivalenti all’introduzione di livelli di significatività $\alpha_{\text{in}}$ per l’ingresso e $\alpha_{\text{out}}$ per l’uscita delle variabili. Poiché le decisioni dipendono dai valori scelti per queste



### Metodo Backward

Il **metodo backward** per la selezione delle variabili si fonda sui **p-value** associati ai test d’ipotesi sui singoli coefficienti di regressione. Per ciascun regressore si considera il test:

$$
H_0:\ \beta_j = 0 \quad \text{contro} \quad H_1:\ \beta_j \neq 0,
$$

che verifica se la variabile $X_j$ fornisce un contributo statisticamente significativo al modello, una volta tenute fisse le altre.

Dal punto di vista operativo, si parte dal **modello completo**, contenente tutte le $p$ variabili esplicative. A ogni iterazione si esaminano i p-value stimati $\alpha_j^*$ e si decide se mantenere o rimuovere una variabile in base a soglie prefissate. Una prassi comune prevede che, se $\alpha_j^* < 0.001$, la variabile venga chiaramente mantenuta nel modello; in contesti più conservativi si può adottare una correzione di Bonferroni, con una soglia approssimativa $\alpha \approx \tfrac{5\%}{p}$, per tenere conto della molteplicità dei test. Se il p-value rientra in una fascia intermedia, ad esempio $0.1\% \le \alpha_j^* < 30\%$, la decisione non è netta e spesso la variabile viene comunque mantenuta. Valori elevati, tipicamente $\alpha_j^* \ge 30\%$, suggeriscono invece che il contributo della variabile sia trascurabile e che essa possa essere eliminata.

L’algoritmo procede rimuovendo, a ogni passo, la variabile con il **p-value più alto**, ricalcolando poi il modello ridotto e i nuovi p-value. Il procedimento si arresta quando tutte le variabili rimanenti risultano significative secondo la soglia scelta.

Un aspetto critico è la **multicollinearità**. In presenza di forti correlazioni tra i regressori, gli stimatori dei coefficienti diventano instabili e i p-value possono risultare artificialmente elevati. Di conseguenza, il metodo backward può eliminare variabili che sarebbero rilevanti dal punto di vista sostantivo, rendendo la selezione fortemente dipendente dalla struttura di correlazione dei dati.

### Metodo Forward

Il **metodo forward** costruisce il modello in modo incrementale, aggiungendo una variabile alla volta. Si parte dal **modello nullo**, che contiene solo l’intercetta, e si valuta quale variabile, tra quelle non ancora incluse, produce il maggior miglioramento del modello.

Il miglioramento può essere misurato tramite **indicatori globali di adattamento**, come la riduzione dell’errore standard della regressione $S_e$ oppure l’aumento del coefficiente di determinazione $R^2$. Parallelamente, si richiede che l’ingresso della variabile sia supportato da una **significatività statistica**, valutata tramite il p-value del coefficiente o tramite un test **F** che confronta il modello corrente con quello arricchito dalla nuova variabile.

La prima variabile inserita è quindi quella che, da sola, spiega meglio la risposta. Successivamente, a ogni iterazione si considera l’aggiunta di una nuova variabile al modello già costruito e si seleziona quella che fornisce il miglior contributo addizionale. Il procedimento si arresta quando nessuna delle variabili rimanenti è in grado di produrre un miglioramento **statisticamente significativo** secondo la soglia scelta.

È importante notare che il metodo forward non riconsidera mai l’esclusione delle variabili già inserite. Per questo motivo, in presenza di **interazioni** o **collinearità** tra regressori, il percorso di selezione può differire sensibilmente da quello del metodo backward. Di conseguenza, forward e backward possono condurre a **modelli finali diversi**, anche partendo dallo stesso insieme iniziale di variabili.

## Metodi Globali

I **metodi globali** affrontano il problema della selezione delle variabili valutando **tutti i possibili sottoinsiemi** di regressori. Dato un insieme di $p$ variabili esplicative, ciò equivale a considerare $2^p$ modelli distinti, dal modello nullo a quello completo. Questo approccio è concettualmente più completo rispetto ai metodi stepwise, ma diventa rapidamente **computazionalmente oneroso** all’aumentare di $p$.

La scelta del modello finale avviene confrontando i diversi sottoinsiemi tramite **criteri globali di bontà del modello**. Un primo criterio è il minimo **errore standard della regressione** $S_e$, che privilegia modelli con residui mediamente più piccoli. Un’alternativa è il massimo **$R_a^2$**, il coefficiente di determinazione corretto, che penalizza l’aggiunta di variabili superflue e riduce l’ottimismo del semplice $R^2$.

Sono molto utilizzati anche criteri informativi come **AIC** e **BIC**, che bilanciano qualità dell’adattamento e complessità del modello introducendo una penalizzazione esplicita per il numero di parametri. In generale, l’AIC tende a favorire modelli più ricchi, mentre il BIC applica una penalizzazione più severa e porta a modelli più parsimoniosi.

Un approccio sempre più rilevante è la valutazione delle prestazioni in **validazione**, ad esempio tramite il valore di $R^2$ su un validation set, l’errore di previsione o misure come l’MSE stimato con **cross-validation**. Questo sposta l’attenzione dall’adattamento ai dati osservati alla capacità predittiva del modello.

Un aspetto cruciale è che tutti questi indicatori sono affetti da **variabilità campionaria**. In assenza di una validazione esterna o di tecniche di ri-campionamento, la scelta del modello può risultare instabile, soprattutto quando le differenze tra modelli sono piccole o quando i regressori sono fortemente correlati.




## Coefficienti di Determinazione

### $R^2$

Il coefficiente di determinazione $R^2$ misura la **quota di variabilità totale della risposta spiegata dal modello**. È definito come

$$
R^2 = 1 - \frac{SSR}{SSY}, \qquad
SSR = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2, \qquad
SSY = \sum_{i=1}^n (Y_i - \bar{Y})^2.
$$

In questa notazione, $SSR$ rappresenta la **somma dei residui al quadrato** (*sum of squared residuals*), mentre $SSY$ è la somma dei quadrati totali rispetto alla media campionaria. Il valore di $R^2$ appartiene all’intervallo $[0,1]$: valori prossimi a $1$ indicano che il modello spiega una grande parte della variabilità osservata, mentre valori prossimi a $0$ indicano una capacità esplicativa ridotta.

Una proprietà importante è che $R^2$ **non penalizza l’aggiunta di nuove variabili**. Inserendo ulteriori regressori, $SSR$ non può aumentare e quindi $R^2$ può solo crescere o rimanere invariato, anche quando le nuove variabili non apportano un contributo informativo reale.

### $R_a^2$

Il coefficiente di determinazione corretto $R_a^2$ introduce una correzione che tiene conto del **numero di parametri stimati** nel modello, con l’obiettivo di ridurre il rischio di overfitting. È definito come

$$
R_a^2 = 1 - \frac{S_e^2}{S_Y^2}, \qquad
S_e^2 = \frac{SSR}{n - p - 1}, \qquad
S_Y^2 = \frac{SSY}{n - 1},
$$

dove $p$ è il numero di regressori esclusa l’intercetta. Qui $S_e^2$ è una stima della varianza dell’errore, mentre $S_Y^2$ rappresenta la varianza campionaria della risposta.

A differenza di $R^2$, il valore di $R_a^2$ **aumenta solo se la variabile aggiunta migliora effettivamente il modello**, compensando la perdita di gradi di libertà dovuta all’introduzione di un nuovo parametro. Per questo motivo, $R_a^2$ è spesso preferito a $R^2$ nei confronti tra modelli con diverso numero di regressori.




## Overfitting

L’**overfitting** si verifica quando un modello si adatta in modo eccessivo ai dati di training, catturando anche il rumore campionario, ma mostra una **scarsa capacità di generalizzazione** su nuovi dati. In questi casi il modello presenta prestazioni apparentemente molto buone sul campione utilizzato per la stima, ma errori elevati in previsione.

Un primo fattore chiave è il rapporto tra numerosità campionaria e complessità del modello. Mantenere un buon rapporto **dati/variabili**, espresso come $N/P \gg 1$, riduce il rischio che il modello disponga di troppi gradi di libertà rispetto alle informazioni effettivamente disponibili nei dati.

In presenza di dati molto rumorosi è spesso preferibile adottare modelli più **parsimoniosi**, evitando l’inclusione indiscriminata di regressori. In questo contesto, il metodo forward può talvolta risultare meno aggressivo del backward, poiché inserisce solo variabili con un contributo incrementale evidente; tuttavia, questa non è una regola generale e il comportamento dipende fortemente dalla struttura dei dati e dalle correlazioni tra regressori.

Strumenti fondamentali per il controllo dell’overfitting sono la **validazione**, ad esempio tramite validation set o cross-validation, e la **regolarizzazione**, che introduce penalizzazioni sui coefficienti per limitare la complessità del modello e stabilizzare le stime.




## Validazione

La **validazione** serve a valutare la capacità di **generalizzazione** di un modello, cioè quanto bene esso si comporti su dati non utilizzati nella stima. L’idea di fondo è separare, in modo esplicito o implicito, la fase di apprendimento da quella di valutazione.

Un primo approccio è l’uso di un **validation set**, in cui una frazione dei dati, tipicamente intorno al 10–20%, viene tenuta da parte e non utilizzata per stimare il modello. Il modello è addestrato sul training set e valutato sui dati lasciati fuori, ottenendo una stima diretta dell’errore di previsione.

Un metodo più stabile è la **cross-validation $k$-fold**. Il campione viene suddiviso in $k$ blocchi di dimensione simile; a ogni iterazione uno dei blocchi funge da validation set mentre i restanti $k-1$ sono usati per il training. L’errore di generalizzazione è stimato mediando le prestazioni ottenute sui diversi blocchi. Questo approccio riduce la dipendenza della valutazione da una singola partizione dei dati.

Il caso limite della cross-validation è la **leave-one-out cross-validation (LOOCV)**, in cui si pone $k=n$. A ogni iterazione si lascia fuori un solo punto e si stima il modello sugli altri $n-1$ dati. La LOOCV utilizza quasi tutta l’informazione disponibile per il training, ma può risultare computazionalmente costosa e con varianza elevata della stima dell’errore.

È importante distinguere la validazione predittiva da altre tecniche di ri-campionamento. Il termine **Jackknife** è concettualmente correlato, ma ha uno scopo diverso: è utilizzato principalmente per stimare bias e varianza di statistiche, non per valutare le prestazioni predittive di un modello. In ambito di validazione, il riferimento corretto è quindi la LOOCV, non il Jackknife.



## Regolarizzazione

La **regolarizzazione** introduce una penalizzazione sui coefficienti del modello con l’obiettivo di **contenere la complessità**, stabilizzare le stime e prevenire coefficienti di ampiezza eccessiva, fenomeno tipico in presenza di multicollinearità o di un numero elevato di regressori rispetto ai dati disponibili.

### Ridge Regression (penalizzazione L2)

La **Ridge Regression** aggiunge alla funzione di perdita una penalizzazione proporzionale alla somma dei quadrati dei coefficienti. In genere l’intercetta non viene penalizzata. Il problema di ottimizzazione è

$$
\min_{\mathbf B}\ \frac{1}{n}\sum_{i=1}^n\left(Y_i-\sum_{j=1}^p B_j X_{ij}\right)^2
\;+\;
\lambda\sum_{j=1}^p B_j^2.
$$

La penalizzazione L2 tende a **ridurre** l’ampiezza dei coefficienti, ma non li annulla esattamente. Il risultato è un modello più stabile, in cui le variabili fortemente correlate condividono il peso esplicativo invece di competere in modo instabile.

### Lasso Regression (penalizzazione L1)

La **Lasso Regression** utilizza una penalizzazione basata sulla somma dei **valori assoluti** dei coefficienti:

$$
\min_{\mathbf B}\ \frac{1}{n}\sum_{i=1}^n\left(Y_i-\sum_{j=1}^p B_j X_{ij}\right)^2
\;+\;
\lambda\sum_{j=1}^p |B_j|.
$$

La penalizzazione L1 ha una proprietà distintiva: induce **sparsità** nel vettore dei coefficienti. Alcuni coefficienti vengono spinti esattamente a zero, realizzando di fatto una selezione automatica delle variabili, oltre alla regolarizzazione.

### Elastic Net (penalizzazione L1 + L2)

L’**Elastic Net** combina le penalizzazioni L1 e L2, unendo i vantaggi di Ridge e Lasso:

$$
\min_{\mathbf B}\ \frac{1}{n}\sum_{i=1}^n\left(Y_i-\sum_{j=1}^p B_j X_{ij}\right)^2
\;+\;
\lambda_1\sum_{j=1}^p |B_j|
\;+\;
\lambda_2\sum_{j=1}^p B_j^2.
$$

Questa formulazione è particolarmente utile quando i regressori sono numerosi e fortemente correlati: la componente L2 stabilizza le stime, mentre la componente L1 consente la selezione delle variabili.

Dal punto di vista pratico, i regressori vengono **standardizzati** per rendere confrontabili le penalizzazioni tra coefficienti, l’intercetta non viene penalizzata e i parametri di regolarizzazione $\lambda$ (e il bilanciamento tra L1 e L2 nell’Elastic Net) vengono scelti tramite **cross-validation**, ottimizzando le prestazioni predittive del modello.



## Double Descent

Il **double descent** è un fenomeno osservato in modelli ad **altissima capacità**, in cui l’errore di generalizzazione non segue l’andamento classico a U, ma mostra due fasi distinte. Al crescere della complessità del modello, l’errore inizialmente **diminuisce**, poi **aumenta** in prossimità della soglia di **interpolazione** (dove il modello riesce a fittare esattamente i dati di training) e infine **ridiscende** quando la capacità continua ad aumentare.

La prima fase corrisponde al regime classico bias–variance: aumentando la complessità si riduce il bias ma cresce la varianza, portando a overfitting. Il picco di errore si verifica tipicamente quando il numero di parametri è comparabile o supera di poco il numero di osservazioni, e il modello interpola i dati di training.

Nel regime successivo, detto **overparameterized**, l’errore di generalizzazione può sorprendentemente diminuire. Questo comportamento è stato osservato soprattutto in presenza di **regolarizzazione implicita**, come quella indotta dagli algoritmi di ottimizzazione (ad esempio gradient descent), dall’early stopping o dalla struttura stessa dei modelli. In questo regime, pur avendo capacità sufficiente per interpolare, il modello tende a convergere verso soluzioni con buone proprietà di generalizzazione.

Il fenomeno del double descent è particolarmente rilevante nei **modelli moderni ad alta dimensionalità**, come le reti neurali profonde e i modelli di grandi dimensioni, e mostra che la relazione tra complessità del modello e generalizzazione è più articolata di quanto suggerito dalla visione classica dell’overfitting.


## Regressione Polinomiale

La **regressione polinomiale** estende la regressione lineare introducendo **termini polinomiali** delle variabili esplicative, consentendo di modellare relazioni non lineari pur rimanendo in un quadro lineare nei parametri. Nel caso univariato, il modello di grado $d$ è

$$
Y = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_d x^d.
$$

Nonostante la non linearità in $x$, il modello resta lineare rispetto ai coefficienti $\beta_j$, e può quindi essere stimato con i metodi usuali dei minimi quadrati.

Una regola importante è la **regola gerarchica**: se si include un termine di grado $d$, è buona pratica includere anche tutti i termini di grado inferiore. Questo garantisce interpretabilità e stabilità del modello, evitando specificazioni incoerenti come la presenza di $x^2$ senza il termine lineare $x$.

Nel caso multivariato, la regressione polinomiale può includere anche **termini di interazione** tra variabili, ad esempio $x_1 x_2$, che permettono di modellare effetti combinati in cui l’influenza di una variabile dipende dal livello di un’altra. Anche per le interazioni vale una forma di gerarchia: includere un termine di interazione implica includere i termini principali corrispondenti.

L’aumento del grado polinomiale o del numero di interazioni incrementa rapidamente la complessità del modello e il numero di parametri. Per questo motivo la regressione polinomiale è particolarmente soggetta a **overfitting** quando il grado è troppo elevato rispetto alla numerosità campionaria. In pratica, la scelta del grado e dei termini da includere dovrebbe essere guidata da **validazione** e, quando necessario, da tecniche di **regolarizzazione** per controllare la variabilità delle stime.




## Regressione Pesata

Quando l’ipotesi di **omoschedasticità** non è soddisfatta, cioè quando la varianza degli errori non è costante tra le osservazioni, la stima OLS non è più efficiente. In questi casi si utilizza la **regressione pesata** (*Weighted Least Squares*, WLS), che assegna pesi diversi alle osservazioni in funzione della loro variabilità.

L’idea è minimizzare una somma dei quadrati dei residui **pesata**:

$$
SSR_W = \sum_{i=1}^n W_i\left(Y_i - \sum_{j=1}^p B_j X_{ij}\right)^2,
\qquad
W_i = \frac{1}{\sigma_i^2},
$$

dove $\sigma_i^2$ è la varianza dell’errore associato all’osservazione $i$-esima. In questo modo, le osservazioni con varianza maggiore ricevono un peso minore e influenzano meno la stima dei coefficienti.

La WLS è equivalente a una trasformazione dei dati. Definendo

$$
\tilde{X}_{ij} = \frac{X_{ij}}{\sigma_i},
\qquad
\tilde{Y}_i = \frac{Y_i}{\sigma_i},
$$

il problema di minimizzazione pesata si riconduce a una **regressione OLS** applicata ai dati trasformati. Questa equivalenza chiarisce che la regressione pesata ristabilisce, sui dati trasformati, l’ipotesi di varianza costante degli errori.

In pratica, le varianze $\sigma_i^2$ non sono quasi mai note. In tali situazioni possono essere **stimate**, ad esempio specificando un modello per la varianza in funzione delle covariate o dei valori predetti, e utilizzate per definire i pesi. Il procedimento può essere iterato: si stima il modello, si aggiornano le varianze e i pesi, e si ricalcola la regressione fino a convergenza.
