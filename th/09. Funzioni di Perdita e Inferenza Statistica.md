# Funzioni di Perdita e Inferenza Statistica

## Cross-Entropy Loss

La **Cross-Entropy Loss** (entropia incrociata) è una funzione di perdita utilizzata quando la variabile target è **categorica**, in particolare nei problemi di classificazione multiclasse. Il suo uso è strettamente legato a modelli probabilistici che stimano, per ogni osservazione, una distribuzione di probabilità sulle classi possibili.

Si consideri un dataset composto da $n$ osservazioni e $m$ categorie. Ogni osservazione appartiene a una sola categoria ed è rappresentata tramite un vettore **one-hot** $b(i) \in \{0,1\}^m$, in cui vale $b_j(i)=1$ se l’osservazione $i$ appartiene alla classe $j$ e $b_j(i)=0$ altrimenti.  

Indichiamo con $p_j(i)$ la probabilità che l’osservazione $i$ appartenga alla categoria $j$. Per ogni $i$ tali probabilità soddisfano i vincoli di una distribuzione discreta:
$$
p_j(i) \ge 0, 
\qquad 
\sum_{j=1}^m p_j(i) = 1.
$$

Sfruttando la codifica one-hot, la probabilità dell’osservazione $i$ può essere scritta in forma compatta come
$$
P(Y(i)=j) = p_j(i) = \prod_{k=1}^m p_k(i)^{b_k(i)}.
$$
Infatti, il prodotto seleziona automaticamente la probabilità associata alla classe osservata.

La **log-verosimiglianza** dell’intero campione risulta quindi
$$
\ell = \sum_{i=1}^n \sum_{j=1}^m b_j(i)\,\log p_j(i).
$$

### Caso senza dipendenza da $x$

Se le probabilità non dipendono dall’input, cioè $p_j(i)=P_j$ costante per tutte le osservazioni, il problema si riduce alla stima di una distribuzione categorica. La massimizzazione della log-verosimiglianza porta alla stima
$$
\hat P_j = \frac{O_j}{n},
$$
dove $O_j$ è il numero di osservazioni appartenenti alla classe $j$. In questo caso, la stima coincide semplicemente con la frequenza empirica.

### Caso con dipendenza da $x$

Quando invece la probabilità della classe dipende dall’input $x(i)$, si introduce un modello parametrico
$$
p_j(i) = \pi_j(x(i); \alpha,\beta,\gamma),
$$
che associa a ogni osservazione una distribuzione di probabilità sulle classi. Un esempio fondamentale è la **softmax**, definita come
$$
\pi_j(x) = \frac{\exp(\eta_j(x))}{\sum_{k=1}^m \exp(\eta_k(x))},
\qquad
\eta_j(x) = w_j^\top x + c_j.
$$
In questo caso i parametri del modello vengono stimati numericamente massimizzando la log-verosimiglianza, tipicamente tramite algoritmi di ottimizzazione iterativa come la discesa del gradiente.

### Cross-Entropy come funzione di perdita

Poiché l’addestramento dei modelli avviene per minimizzazione, si introduce la **cross-entropy loss** come l’opposto normalizzato della log-verosimiglianza:
$$
\text{loss}_{\text{CE}}(\alpha,\beta,\gamma)
= -\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^m b_j(i)\,\log \pi_j(x(i);\alpha,\beta,\gamma).
$$
Questa quantità misura quanto la distribuzione predetta dal modello si discosta dalla distribuzione vera, rappresentata dal vettore one-hot.

In termini di entropia incrociata tra il target $b(i)$ e la predizione $\pi(x(i))$, la perdita può essere scritta come
$$
H(b,\pi) = -\sum_{j=1}^m b_j \log \pi_j,
\qquad
\text{loss}_{\text{CE}} = \frac{1}{n}\sum_{i=1}^n H\bigl(b(i),\pi(x(i))\bigr).
$$

Dal punto di vista interpretativo, minimizzare la cross-entropy equivale a rendere massima la probabilità assegnata dal modello alla classe corretta, penalizzando fortemente le predizioni errate ma troppo “sicure”.

Nella pratica, per garantire stabilità numerica, si introducono spesso piccoli accorgimenti come l’uso di un $\varepsilon$ per evitare $\log 0$, oppure tecniche come *label smoothing* e pesi di classe per gestire dataset sbilanciati.



## Mean Squared Error Loss (MSE)

La **Mean Squared Error (MSE)** è una funzione di perdita tipicamente utilizzata quando la variabile risposta è quantitativa e si assume un modello **gaussiano** per gli errori. In particolare, si ipotizza che ogni osservazione segua una distribuzione normale
$$
Y(i) \sim \mathcal{N}(\mu(i), \sigma^2(i)),
$$
dove $\mu(i)$ è la media condizionata (eventualmente dipendente dagli input) e $\sigma^2(i)$ la varianza.

Sotto questa ipotesi, la log-verosimiglianza del campione è
$$
\ell
= \sum_{i=1}^n \left[
-\log \sigma(i)
- \log \sqrt{2\pi}
- \frac{(Y(i)-\mu(i))^2}{2\sigma^2(i)}
\right].
$$

### Parametri costanti

Nel caso più semplice si assume che $\mu(i)=\mu$ e $\sigma(i)=\sigma$ per ogni $i$. La massimizzazione della log-verosimiglianza porta alle stime classiche della media e della varianza globali: la media campionaria per $\mu$ e la varianza campionaria (con denominatore $n$) per $\sigma^2$ nel senso della massima verosimiglianza.

### Caso omoschedastico (varianza costante)

Un caso molto rilevante in regressione è quello in cui la media dipende dagli input $x(i)$, mentre la varianza è costante, cioè $\sigma^2(i)=\sigma^2$. Indicando con $\mu(x(i);\alpha,\beta,\gamma)$ il modello parametrico per la media, la parte della log-verosimiglianza che dipende dai parametri della media è proporzionale alla somma dei quadrati degli errori. Di conseguenza, massimizzare la log-verosimiglianza equivale a minimizzare la **MSE**:
$$
\text{loss}_{\text{MSE}}(\alpha,\beta,\gamma)
= \frac{1}{n}\sum_{i=1}^n \bigl(Y(i)-\mu(x(i);\alpha,\beta,\gamma)\bigr)^2.
$$

Una volta stimata la media, la stima di massima verosimiglianza della deviazione standard è
$$
\hat{\sigma}
= \sqrt{\text{loss}_{\text{MSE}}(\alpha,\beta,\gamma)},
$$
mentre la stima di massima verosimiglianza della varianza è
$$
\hat{\sigma}^2_{\text{MLE}}
= \frac{1}{n}\sum_{i=1}^n \bigl(Y(i)-\hat{\mu}(i)\bigr)^2.
$$
Dal punto di vista statistico, questo stimatore è distorto; lo stimatore **non distorto** della varianza utilizza invece il denominatore $n-p$, dove $p$ è il numero di parametri stimati nel modello per la media.

### Caso eteroschedastico (dipendenza completa da $x$)

Nel caso più generale anche la varianza dipende dagli input, cioè $\sigma^2(i)=\sigma^2(x(i))$. In questa situazione la funzione obiettivo coerente con il modello gaussiano è il negativo della log-verosimiglianza:
$$
-\ell(\theta)
= \frac{1}{2}\sum_{i=1}^n \left[
\log\bigl(2\pi \sigma^2(i)\bigr)
+ \frac{(Y(i)-\mu(i))^2}{\sigma^2(i)}
\right].
$$
Questo criterio corrisponde a una forma di **weighted least squares**, in cui ogni residuo è pesato con l’inverso della sua varianza. La stima dei parametri della media e della varianza non ammette in genere una soluzione chiusa e richiede metodi iterativi di ottimizzazione.




## Regressione Lineare Semplice

Nel modello di **regressione lineare semplice**, si ipotizza che le variabili indipendenti $x_i$ siano deterministiche e che le variabili dipendenti $Y_i$ siano casuali. Si assume:

$$
Y_i \sim \mathcal{N}(\mu_i, \sigma^2), \quad \mu_i = \beta_0 + \beta_1 x_i
$$

La funzione di perdita da minimizzare è:

$$
\text{loss}_{\text{MSE}}(\beta_0, \beta_1) = \frac{1}{n} \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 x_i)^2
$$

Le stime di massima verosimiglianza per i coefficienti sono:

$$
\hat{\beta}_1 = \frac{\sum(x_i Y_i) - n\bar{x}\bar{Y}}{\sum(x_i^2) - n\bar{x}^2}, \quad \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{x}
$$

La varianza degli errori stimata in modo **non distorto** è:

$$
S_e^2 = \frac{1}{n - 2} \sum (Y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
$$



## Teorema di Cochran — Introduzione intuitiva

Il **teorema di Cochran** serve a capire come si può **scomporre la variabilità dei dati** in modo rigoroso quando si lavora con modelli lineari e ipotesi gaussiane. In particolare, fornisce la giustificazione teorica del fatto che, in molti modelli statistici, la variabilità totale può essere divisa in una parte **spiegata dal modello** e una parte **non spiegata**, detta residua.

Il teorema si applica quando le osservazioni seguono una distribuzione normale con **varianza costante** e quando le stime dei parametri possono essere viste come **proiezioni ortogonali** su opportuni sottospazi. In questa situazione, il risultato fondamentale è che la componente stimata dal modello e quella residua sono **indipendenti** tra loro e che la somma dei quadrati dei residui segue una distribuzione **chi-quadro** con un numero di gradi di libertà ben definito.

Questo è il motivo per cui il teorema di Cochran è alla base della regressione lineare, dell’**ANOVA** e dei **test F**: permette di confrontare in modo corretto la variabilità spiegata dal modello con quella dovuta al rumore, utilizzando distribuzioni note e risultati di inferenza statistica affidabili.

## Teorema di Cochran

Si consideri un vettore aleatorio $X = (X_1,\dots,X_n) \in \mathbb{R}^n$ tale che le sue componenti siano indipendenti e identicamente distribuite secondo una normale con varianza comune, cioè
$X_i \sim \mathcal{N}(\mu_i, \sigma^2)$, e si assuma che il vettore dei valori medi $\mu = (\mu_1,\dots,\mu_n)$ appartenga a un sottospazio vettoriale $V \subset \mathbb{R}^n$ di dimensione $k$.

In questo contesto, il teorema di Cochran descrive la struttura geometrica e probabilistica della stima di $\mu$ e della variabilità residua. In particolare, lo stimatore di massima verosimiglianza di $\mu$ si ottiene come proiezione ortogonale del vettore osservato $X$ sul sottospazio $V$. Indicando con $\pi_V(X)$ tale proiezione, vale
$$
\hat{\mu} = \pi_V(X).
$$
Questo risultato riflette il fatto che, nel modello gaussiano con varianza nota e omoschedastica, la stima ottimale del vettore dei parametri medi è quella che minimizza la distanza euclidea tra $X$ e il sottospazio dei valori ammissibili per $\mu$.

La parte di $X$ che non viene spiegata dal sottospazio $V$ è data dal residuo $X - \pi_V(X)$. La sua norma al quadrato
$$
W = \|X - \pi_V(X)\|^2
$$
rappresenta la somma dei quadrati residui. Il teorema di Cochran afferma che questa quantità è indipendente dalla componente stimata $\pi_V(X)$ e che, una volta normalizzata per la varianza $\sigma^2$, segue una distribuzione chi-quadro con $n-k$ gradi di libertà:
$$
\frac{W}{\sigma^2} \sim \chi^2(n - k).
$$

L’indipendenza tra la parte spiegata dal modello (la proiezione su $V$) e la parte residua, insieme alla distribuzione chi-quadro della somma dei quadrati residui, costituisce il fondamento teorico della scomposizione della varianza. Questo risultato è centrale nei test $F$, nell’analisi della varianza (ANOVA) e più in generale nell’inferenza statistica per i modelli di regressione lineare.
