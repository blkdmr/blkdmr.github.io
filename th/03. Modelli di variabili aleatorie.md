# Modelli di variabili aleatorie 

Alcuni tipi di variabili aleatorie compaiono molto frequentemente in natura

## Variabili aleatorie di bernoulli

Una variabile aleatoria $X$ si dice di **Bernoulli** se può assumere solo i valori $0$ e $1$, con probabilità rispettivamente $1 - p$ e $p$, dove $p \in [0, 1]$. La sua funzione di massa di probabilità è:

$$
P(X = x) = \begin{cases}
p & \text{se } x = 1, \\
1 - p & \text{se } x = 0.
\end{cases}
$$

Il suo **valore atteso** è $E[X] = p$, mentre la sua **varianza** è $\operatorname{Var}(X) = p(1 - p)$.

Un **processo di Bernoulli** è una successione di variabili aleatorie indipendenti $X_i$ con uguale distribuzione di Bernoulli $\mathcal{B}(p)$.

La **distribuzione binomiale** descrive la probabilità del numero di successi in $n$ prove di Bernoulli indipendenti, ovvero della variabile aleatoria:

$$
S_n = X_1 + X_2 + \dots + X_n.
$$

## Variabili aleatorie binomiali

Quando si effettuano $n$ ripetizioni **indipendenti** di un esperimento binario, ciascuna con probabilità di successo $p$ e di fallimento $1 - p$, il numero totale di successi $S_n$ è una variabile aleatoria **binomiale** $\mathcal{B}(n, p)$.

Il coefficiente binomiale è definito come:

$$
\binom{n}{i} = \dfrac{n!}{i! (n - i)!}.
$$

La **funzione di massa di probabilità** di una variabile aleatoria binomiale è:

$$
P(S_n = i) = \binom{n}{i} p^i (1 - p)^{n - i}, \quad \text{per } i = 0, 1, \dots, n.
$$

Il suo **valore atteso** è:

$$
E[S_n] = n p,
$$

mentre la sua **varianza** è:

$$
\operatorname{Var}(S_n) = n p (1 - p).
$$

La figura rappresenta il grafico della funzione di massa di una variabile binomiale con parametri $n = 10$ e $p = 0{,}5$, che risulta simmetrica rispetto al valore medio.

![[Pasted image 20241103164907 1.png|350]]

## Variabili aleatorie di poisson

Una variabile aleatoria $X$ che assume valori interi non negativi $i = 0, 1, 2, \dots$ si dice **di Poisson** di parametro $\lambda > 0$ se la sua funzione di massa di probabilità è:

$$
P(X = i) = \dfrac{e^{-\lambda} \lambda^i}{i!}.
$$

Il parametro $\lambda$ rappresenta sia il **valore atteso** $E[X] = \lambda$ sia la **varianza** $\operatorname{Var}(X) = \lambda$.

**Nota.** Le variabili di Poisson vengono spesso utilizzate come approssimazione delle variabili binomiali $\mathcal{B}(n, p)$ quando $n$ è molto grande e $p$ è molto piccolo, mantenendo $\lambda = n p$ costante.

La figura illustra il grafico della funzione di massa di una variabile di Poisson con parametro $\lambda = 4$.

![[Pasted image 20241103165152 1.png|300]]

**Nota.** La distribuzione di Poisson viene spesso utilizzata per modellare il numero di eventi in un intervallo di tempo, dato un tasso medio di occorrenza $\lambda$

## Processi di poisson

Un **processo di Poisson** con tasso $\lambda > 0$ descrive il verificarsi di eventi indipendenti nel tempo con un ritmo medio costante. Se $N(t)$ indica il numero di eventi avvenuti nell’intervallo di tempo $[0,t]$, allora $N(t)$ segue una **distribuzione di Poisson** con parametro  $\lambda t$, vale a dire:

$P\left(N(t) = k\right) = \dfrac{(\lambda t)^k}{k!}\,e^{-\lambda t}, \quad k = 0,1,2,\ldots$

Il suo **valore atteso** è $E[N(t)] = \lambda t$, mentre la **varianza** è $\operatorname{Var}(N(t)) = \lambda t$. I tempi di attesa fra un evento e il successivo, invece, seguono una **distribuzione esponenziale** con lo stesso parametro $\lambda$. Uno dei tratti distintivi di questo modello è il suo carattere “senza memoria”, per cui la probabilità di un evento futuro non dipende da quanto tempo è già trascorso.

## Variabili aleatorie uniformi

Una **variabile aleatoria continua** $X$ si dice **uniformemente distribuita** sull’intervallo $[a,b]$(con $a < b$) se la sua **funzione di densità di probabilità (pdf)** è costante su $[a,b]$ e nulla altrove. Formalmente,


$f_X(x) = \begin{cases} \dfrac{1}{b - a}, & x \in [a, b],\\ 0, & \text{altrimenti}. \end{cases}$

La distribuzione uniforme è così chiamata perché **ogni valore all’interno dell’intervallo** $[a, b]$ è **ugualmente probabile**.

![[Pasted image 20250304113422 1.png|350]]

Il **valore atteso** della variabile è dato da $E[X] = \dfrac{a + b}{2}$, mentre la **varianza** è $\operatorname{Var}(X) = \dfrac{(b - a)^2}{12}$.

Oltre alla versione continua, esiste anche una versione discreta della distribuzione uniforme. Se $X$ può assumere $n$ valori distinti $\{1, 2, \dots, n\}$ con la stessa probabilità, allora la funzione di massa di probabilità è

$P(X = k) = \frac{1}{n}, \quad k = 1, 2, \dots, n.$

In questo caso, il valore atteso è $E[X] = \dfrac{n+1}{2}$ e la varianza è $\operatorname{Var}(X) = \dfrac{n^2 - 1}{12}$.

Questa distribuzione è spesso utilizzata quando si vuole assegnare la stessa probabilità a tutti i possibili valori di una variabile, senza favorirne nessuno.

## Distribuzione geometrica

Una variabile aleatoria $T$ si dice di **distribuzione geometrica**, con parametro $p \in (0,1]$, se rappresenta il **numero di prove necessarie fino al primo successo** in un processo di Bernoulli, cioè una sequenza di prove indipendenti con probabilità di successo costante $p$.

**Funzione di massa di probabilità (pmf)**

Se $T$ rappresenta il numero della prima prova che ha successo, allora:

$$
P(T = k) = (1 - p)^{k - 1} p, \quad \text{per } k = 1, 2, 3, \dots
$$

In alternativa, se si considera $Y = T - 1$, che rappresenta il **numero di fallimenti prima del primo successo**, allora:

$$
P(Y = k) = (1 - p)^k p, \quad \text{per } k = 0, 1, 2, \dots
$$

**Valore atteso e varianza**

Per la variabile $T$ (che inizia da 1):

$$
E[T] = \frac{1}{p}, \quad \operatorname{Var}(T) = \frac{1 - p}{p^2}
$$

Per la variabile $Y = T - 1$ (che inizia da 0):

$$
E[Y] = \frac{1 - p}{p}, \quad \operatorname{Var}(Y) = \frac{1 - p}{p^2}
$$

La distribuzione geometrica è l’unica distribuzione discreta che gode della **proprietà di assenza di memoria**:

$$
P(T > m + n \mid T > m) = P(T > n), \quad \text{per ogni } m,n \in \mathbb{N}
$$

Inoltre, la distribuzione geometrica: 

- È collegata al processo di **Bernoulli**, in cui si eseguono prove ripetute fino al primo successo.
- È un caso particolare della **binomiale negativa** con un solo successo.
- È l'analogo discreto della **distribuzione esponenziale**, che ha la stessa proprietà di memoria nulla.

## Distribuzione ipergeometrica

La variabile aleatoria $X$ segue una **distribuzione ipergeometrica** se rappresenta il numero di successi ottenuti estraendo **senza reinserimento** da una popolazione finita composta da successi e insuccessi.

Supponiamo di avere:
- una popolazione di dimensione $N$,
- che contiene $K$ elementi di tipo "successo" (e quindi $N - K$ di tipo "insuccesso"),
- da cui si estraggono $n$ elementi **senza reinserimento**,

allora $X$ è il numero di successi nelle $n$ estrazioni.

**Funzione di massa di probabilità (pmf)**

$$
P(X = k) = \frac{\binom{K}{k} \binom{N - K}{n - k}}{\binom{N}{n}}
$$

dove

$$
\max(0,\; n - (N - K)) \le k \le \min(K,\; n)
$$

**Valore atteso e varianza**

$$
E[X] = n \cdot \frac{K}{N}
$$

$$
\operatorname{Var}(X) = n \cdot \frac{K}{N} \cdot \left(1 - \frac{K}{N}\right) \cdot \frac{N - n}{N - 1}
$$

A differenza della distribuzione binomiale: 

- Nella **binomiale** le prove sono indipendenti (estrazione con reinserimento).
- Nella **ipergeometrica** le estrazioni sono **senza reinserimento**, quindi **dipendenti**.
- Se $N$ è molto grande rispetto a $n$, l'ipergometrica si approssima alla binomiale con $p = \frac{K}{N}$.

## Distribuzione binomiale negativa

Una variabile aleatoria $X$ si dice di **binomiale negativa** se rappresenta il numero di fallimenti prima di ottenere $r$ successi in una sequenza di prove di Bernoulli indipendenti, ciascuna con probabilità di successo $p \in (0, 1]$. Si indica con:

$$
X \sim \operatorname{NB}(r, p)
$$

Il supporto di $X$ è l’insieme $\{0, 1, 2, \dots\}$.


La **funzione di massa di probabilità** è:

$$
P(X = k) = \binom{k + r - 1}{r - 1} p^r (1 - p)^k, \quad k = 0, 1, 2, \dots
$$

dove:
- $k$ è il numero di fallimenti,
- $r$ è il numero di successi desiderati.


Il **valore atteso** è:

$$
E[X] = \frac{r(1 - p)}{p}
$$

La **varianza** è:

$$
\operatorname{Var}(X) = \frac{r(1 - p)}{p^2}
$$


La distribuzione geometrica è un caso particolare della binomiale negativa con $r = 1$. 

**Interpretazione alternativa**

In alcuni contesti si considera una variabile $Y = X + r$, che rappresenta il **numero totale di prove** necessarie per ottenere $r$ successi. In tal caso, la funzione di massa è:

$$
P(Y = n) = \binom{n - 1}{r - 1} p^r (1 - p)^{n - r}, \quad n = r, r + 1, r + 2, \dots
$$

Il valore atteso e la varianza di $Y$ sono:

$$
E[Y] = \frac{r}{p}, \qquad \operatorname{Var}(Y) = \frac{r(1 - p)}{p^2}
$$


## Distribuzione Gamma-Poisson

La distribuzione **Gamma-Poisson** è una **mistura** tra una distribuzione di Poisson e una distribuzione Gamma. Si ottiene considerando che il parametro $\lambda$ della Poisson **non è fisso**, ma segue una distribuzione Gamma.

Formalmente, si definisce una variabile aleatoria $X$ come:

$$
X \mid \lambda \sim \operatorname{Poisson}(\lambda), \quad \lambda \sim \operatorname{Gamma}(r, \theta)
$$

dove:
- $r > 0$ è il parametro di forma della Gamma,
- $\theta > 0$ è il parametro di scala della Gamma (oppure $\beta = 1/\theta$ se si usa la parametrizzazione alternativa con il parametro di tasso).

**Distribuzione marginale di $X$**

Integrando su $\lambda$, la distribuzione marginale di $X$ è la **binomiale negativa**:

$$
X \sim \operatorname{NB}(r, p), \quad \text{con } p = \frac{\theta}{1 + \theta}
$$

oppure, invertendo la formula:

$$
\theta = \frac{p}{1 - p}
$$


La **funzione di massa di probabilità** della distribuzione marginale è:

$$
P(X = k) = \binom{k + r - 1}{k} \left( \frac{\theta}{1 + \theta} \right)^r \left( \frac{1}{1 + \theta} \right)^k, \quad k = 0, 1, 2, \dots
$$

oppure, in termini di $p$:

$$
P(X = k) = \binom{k + r - 1}{k} (1 - p)^r p^k
$$

Il **valore atteso** di $X$ è:

$$
E[X] = r \cdot \theta
$$

La **varianza** è:

$$
\operatorname{Var}(X) = r \cdot \theta (1 + \theta)
$$

oppure, usando $p = \theta / (1 + \theta)$:

$$
E[X] = \frac{r(1 - p)}{p}, \qquad \operatorname{Var}(X) = \frac{r(1 - p)}{p^2}
$$


La Gamma-Poisson modella **conteggi** dove il tasso $\lambda$ non è fisso ma **casuale**, diverso da soggetto a soggetto:

$$
\lambda \sim \operatorname{Gamma}(r, \theta), \quad X \mid \lambda \sim \operatorname{Poisson}(\lambda)
$$

Questa **eterogeneità** tra unità spiega perché:

- la **media** è $E[X] = r\theta$,
- ma la **varianza** è più grande: $\operatorname{Var}(X) = r\theta(1 + \theta)$.

La Poisson ha $\operatorname{Var}(X) = E[X]$ (dispersione fissa), mentre qui si ha **sovradispersione** dovuta alla variabilità di $\lambda$.

## Variabili aleatorie gaussiane

Una variabile aleatoria $X \sim \mathcal{N}(\mu, \sigma^2)$ si dice **gaussiana** (o **normale**) di parametri $\mu$ e $\sigma^2$ se ha la seguente funzione di densità di probabilità:

$$
f(x) = \dfrac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\dfrac{(x - \mu)^2}{2\sigma^2} \right).
$$

La funzione di densità è una curva a campana, detta **curva di Gauss**, simmetrica rispetto a $x = \mu$, con massimo in $x = \mu$ di altezza $(\sigma \sqrt{2\pi})^{-1} \approx 0{,}399/\sigma$.

Il **valore atteso** è $E[X] = \mu$, mentre la **varianza** è $\operatorname{Var}(X) = \sigma^2$.

**Nota.** Il momento secondo è $E[X^2] = \sigma^2 + \mu^2$.

**Proposizione.** Se $X \sim \mathcal{N}(\mu, \sigma^2)$ e $Y = \alpha X + \beta$ con $\alpha \neq 0$, allora $Y \sim \mathcal{N}(\alpha \mu + \beta, \alpha^2 \sigma^2)$.

La variabile standardizzata:

$$
Z = \dfrac{X - \mu}{\sigma}
$$

segue una distribuzione **normale standard** $\mathcal{N}(0, 1)$.

 Il grafico della funzione di densità di una normale standard mostra la classica forma a campana centrata in zero.

![[Pasted image 20241103170100 1.png|300]]

La **funzione di ripartizione** della normale standard è indicata con $\Phi$ ed è definita come:

$$
\Phi(x) = P(Z \leq x) = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-y^2/2} \, dy.
$$

Poiché $Z = \dfrac{X - \mu}{\sigma}$, possiamo esprimere le probabilità relative a $X$ in termini di $\Phi$:

$$
P(X < b) = \Phi\left( \dfrac{b - \mu}{\sigma} \right).
$$

Per $a < b$:

$$
P(a < X < b) = \Phi\left( \dfrac{b - \mu}{\sigma} \right) - \Phi\left( \dfrac{a - \mu}{\sigma} \right).
$$

L'integrale che definisce $\Phi(x)$ non ha una soluzione analitica esatta; si utilizzano tabelle o approssimazioni numeriche.

![[Pasted image 20241107114743 1.png|500]]

Poiché la normale standard è simmetrica rispetto a zero:

$$
\Phi(-x) = 1 - \Phi(x).
$$

Per ogni $\alpha \in (0, 1)$, definiamo $z_\alpha$ come:

$$
P(Z > z_\alpha) = \alpha \quad \Rightarrow \quad z_\alpha = \Phi^{-1}(1 - \alpha).
$$

Il **quantile** $k$-esimo della normale standard è il valore $m$ tale che:

$$
\Phi(m) = \dfrac{k}{100}.
$$

Ponendo $k = 100(1 - \alpha)$, otteniamo $z_\alpha$, indicando che la normale standard è inferiore a $z_\alpha$ nel $k\%$ dei casi.

## Variabili aleatorie beta

Le variabili aleatorie **beta** sono distribuite secondo la **distribuzione beta**, una distribuzione continua definita su un intervallo $[0,1]$ e caratterizzata da due parametri positivi $\alpha$ e $\beta$. La funzione di densità di probabilità (pdf) della distribuzione beta è data da:

$f_X(x) = \dfrac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha, \beta)}, \quad 0 \leq x \leq 1,$

dove il denominatore è la **funzione beta**, definita come:

$B(\alpha, \beta) = \int_0^1 t^{\alpha - 1} (1 - t)^{\beta - 1} dt$

Questa funzione normalizza la densità affinché l’area totale sotto la curva sia uguale a $1$.

Il valore atteso della distribuzione Beta è dato da:

$E[X] = \dfrac{\alpha}{\alpha + \beta}$

La varianza è:

$\operatorname{Var}(X) = \dfrac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$

La distribuzione Beta è molto flessibile perché, in base ai valori di $\alpha$ e $\beta$, può assumere forme molto diverse. 

![[Pasted image 20250304115356 1.png|350]]

Per esempio, se $\alpha = \beta = 1$, la distribuzione diventa uniforme su $[0,1]$. Se $\alpha > 1$ e $\beta > 1$, la distribuzione è concentrata al centro. Se invece uno dei due parametri è minore di $1$, la distribuzione diventa asimmetrica, con una maggiore probabilità vicino a $0$ o $1$.

## Variabili aleatorie esponenziali

Una variabile aleatoria continua $X$ si dice **esponenziale** con parametro $\lambda > 0$ se la sua funzione di densità di probabilità è:

$$
f(x) = \begin{cases}
\lambda e^{-\lambda x} & \text{se } x \geq 0, \\
0 & \text{se } x < 0.
\end{cases}
$$

La **funzione di ripartizione** è:

$$
F(x) = P(X \leq x) = \begin{cases}
1 - e^{-\lambda x} & \text{se } x \geq 0, \\
0 & \text{se } x < 0.
\end{cases}
$$

La distribuzione esponenziale modella tipicamente il **tempo di attesa** prima che si verifichi un evento casuale. Il suo **valore atteso** è:

$$
E[X] = \dfrac{1}{\lambda},
$$

il **momento secondo** è:

$$
E[X^2] = \dfrac{2}{\lambda^2},
$$

e la **varianza** è:

$$
\operatorname{Var}(X) = \dfrac{1}{\lambda^2}.
$$

La proprietà fondamentale è l'**assenza di memoria**:

$$
P(X > s + t \mid X > t) = P(X > s), \quad \forall s, t \geq 0.
$$



**Esempio.** Se $X$ rappresenta il tempo di vita di un oggetto, sapendo che ha già funzionato per un tempo $t$, la probabilità che continui a funzionare per un ulteriore tempo $s$ è la stessa che avrebbe avuto all'inizio.



Per la distribuzione esponenziale, vale:

$$
P(X > s + t) = P(X > s) P(X > t).
$$

Questo riflette la proprietà di assenza di memoria, poiché la probabilità che l'evento non si sia verificato entro $s + t$ è il prodotto delle probabilità di non verificarsi in $s$ e $t$.

## Distribuzioni che derivano da quella normale 

### Variabili aleatorie lognormali

Una variabile aleatoria $Y$ si dice di **distribuzione lognormale** se la variabile $\ln(Y)$ segue una distribuzione normale con parametri $\mu$ e $\sigma^2$. In altre parole, se $X \sim \mathcal{N}(\mu, \sigma^2)$, allora $Y = e^X$ si dice lognormale. La **funzione di densità** di probabilità di $Y$, per $y > 0$, è data da

$f_Y(y) = \dfrac{1}{y\,\sigma \sqrt{2\pi}} \exp\!\left(-\dfrac{(\ln(y) - \mu)^2}{2\sigma^2}\right)$


![[Pasted image 20250302200445 1.png]]

Il suo **valore atteso** si calcola come $E[Y] = e^{\mu + \frac{\sigma^2}{2}}$, mentre la sua **varianza** risulta $\operatorname{Var}(Y) = \left(e^{\sigma^2} - 1\right)\,e^{2\mu + \sigma^2}$

### Variabili aleatorie gamma, chi-quadro e erlang

La **distribuzione Gamma** caratterizza una variabile aleatoria definita per valori positivi ed è spesso parametrizzata con un parametro di forma $\alpha$ e un parametro di tasso $\beta$. In questo caso, la **funzione di densità** di probabilità, per $x > 0$, è 

$$f_X(x) = \dfrac{\beta^\alpha}{\Gamma(\alpha)}\, x^{\alpha - 1} e^{-\beta x}$$ 
Il suo **valore atteso** è $E[X] = \dfrac{\alpha}{\beta}$, mentre la sua **varianza** è $\operatorname{Var}(X) = \dfrac{\alpha}{\beta^2}$. Questa distribuzione è utile per descrivere tempi di attesa fino al verificarsi di un certo numero di eventi in processi di Poisson e, in generale, per modellare fenomeni positivi con variabilità asimmetrica. 

**Nota.** Talvolta, la distribuzione Gamma è espressa in funzione dei parametri $k$ e $\theta$, dove $\displaystyle \alpha =k$ e $\displaystyle \beta =1/\theta$.

![[Pasted image 20250302203805 1.png]]

La **distribuzione $\chi^2$ (chi-quadro)** è un caso particolare della gamma in cui $\alpha = \dfrac{k}{2}$ e $\beta = \dfrac{1}{2}$, dove $k$ rappresenta i gradi di libertà. In tale situazione, la funzione di densità risulta 

$\displaystyle f_X(x) = \frac{1}{2^{k/2}\,\Gamma(\dfrac{k}{2})} \, x^{({k}/{2}) - 1} e^{-{x}/{2}}$

per $x>0$. Il suo **valore atteso** è $E[X] = k$, la **varianza** è $\operatorname{Var}(X) = 2k$. Questa distribuzione è fondamentale in statistica per definire test di ipotesi e intervalli di confidenza relativi, ad esempio, a varianze e regressioni.

![[Pasted image 20250302203827 1.png]]

La **distribuzione Erlang** è anch’essa un caso speciale di gamma in cui il parametro di forma $\alpha$ è un intero positivo $k$. Con un parametro di tasso $\lambda$, la funzione di densità si esprime come 

$\displaystyle f_X(x) = \dfrac{\lambda^k}{(k-1)!}\, x^{k-1}\, e^{-\lambda x}$

per $x>0$. Il suo **valore atteso** è $E[X] = \frac{k}{\lambda}$, mentre la **varianza** è $\operatorname{Var}(X) = \dfrac{k}{\lambda^2}$. Si ottiene come somma di $k$ variabili esponenziali indipendenti, tutte con lo stesso parametro $\lambda$, ed è comunemente impiegata per analizzare tempi di attesa in processi di natura “conta-eventi” (processi di Poisson).

![[Pasted image 20250302204541 1.png]]


### Variabili aleatorie t di student

La distribuzione **t di Student** è una distribuzione di probabilità continua utilizzata principalmente nelle inferenze statistiche, in particolare nei test di ipotesi per campioni di dimensioni ridotte. È caratterizzata da un parametro detto **gradi di libertà** ($\nu$), che influisce sulla forma della distribuzione.

La funzione di densità di probabilità (pdf) della distribuzione t di Student è

$f_X(x) = \dfrac{\Gamma\left(\dfrac{\nu+1}{2}\right)}{\sqrt{\nu \pi}\ \Gamma\left(\dfrac{\nu}{2}\right)} \left(1 + \dfrac{x^2}{\nu} \right)^{-\dfrac{\nu+1}{2}}, \quad -\infty < x < \infty.$

Qui, $\Gamma(\cdot)$ rappresenta la funzione Gamma.

![[Pasted image 20250307155750 1.png]]

La distribuzione t di Student ha media:

$E[X] = 0, \quad \text{per } \nu > 1.$

La varianza è:

$\operatorname{Var}(X) = \dfrac{\nu}{\nu - 2}, \quad \text{per } \nu > 2.$

Per $\nu \leq 2$, la varianza non è definita, mentre per $\nu \leq 1$, la media non è definita.

La distribuzione t di Student assomiglia alla distribuzione normale standard, ma ha **code più pesanti**, il che significa che assegna una maggiore probabilità a valori estremi. Questo comportamento è particolarmente utile quando si lavora con campioni di piccole dimensioni, dove la distribuzione dei dati potrebbe non essere perfettamente normale.

Un'applicazione fondamentale della distribuzione t di Student è nel **test t di Student**, che viene utilizzato per confrontare la media di un campione con un valore noto o per confrontare le medie di due campioni. In generale, quando la dimensione del campione è grande ($\nu \to \infty$), la distribuzione t di Student si avvicina alla distribuzione normale standard.

Questa distribuzione è molto utilizzata in statistica inferenziale per stimare intervalli di confidenza e per test di significatività quando la varianza della popolazione non è nota.

### Variabili aleatorie f di fisher

La distribuzione **F di Fisher** è una distribuzione di probabilità continua utilizzata principalmente nei test di ipotesi statistici per confrontare le varianze di due popolazioni. È particolarmente importante nell’**analisi della varianza (ANOVA)** e nei **test F**, che servono a determinare se due campioni provengono da popolazioni con la stessa varianza.

La distribuzione F di Fisher dipende da due parametri, detti **gradi di libertà**: $d_1$ per il numeratore e $d_2$ per il denominatore. La funzione di densità di probabilità (pdf) è:

$f_X(x) = \dfrac{\left(\dfrac{d_1}{d_2}\right)^{\dfrac{d_1}{2}} x^{\dfrac{d_1}{2} - 1}}{B\left(\dfrac{d_1}{2}, \dfrac{d_2}{2}\right)} \left(1 + \dfrac{d_1}{d_2} x\right)^{-\dfrac{d_1 + d_2}{2}}, \quad x > 0.$

dove $B(a, b)$ è la funzione Beta.

![[Pasted image 20250307155915 1.png]]

La distribuzione F è sempre definita per valori positivi $x > 0$, perché rappresenta il rapporto tra due varianze stimate da campioni casuali. Il **valore atteso** della distribuzione è dato da:

$E[X] = \dfrac{d_2}{d_2 - 2}, \quad \text{per } d_2 > 2$

La **varianza** è

$\operatorname{Var}(X) = \dfrac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}, \quad \text{per } d_2 > 4$

Se $d_2 \leq 2$, la media non è definita, mentre se $d_2 \leq 4$, la varianza non è definita.

La distribuzione F di Fisher è ottenuta come il rapporto tra due variabili aleatorie **chi-quadrato indipendenti**, normalizzate rispetto ai rispettivi gradi di libertà:

$F = {\dfrac{S_1^2}{d_1}}/{\dfrac{S_2^2}{d_2}}.$

dove $S_1^2$ e $S_2^2$ sono varianze campionarie.

La distribuzione F è **asimmetrica e sempre positiva**, con una coda più lunga a destra. Man mano che i gradi di libertà aumentano, la distribuzione si avvicina a una distribuzione normale.

