# Regressione Logistica

La regressione logistica si usa quando la **risposta è dicotomica** (0/1). Per ogni osservazione $i=1,\dots,N$ con vettore di input $x^{(i)}=(x_1^{(i)},\dots,x_p^{(i)})$, si assume:

$$
Z_i \sim \text{Bernoulli}(p_i), \qquad p_i = f\!\big(x^{(i)}\big)
$$

dove $f$ è la **sigmoide** (logistica) applicata a un predittore lineare:

$$
f(x) = \sigma(\eta) = \sigma\!\big(\beta_0+\beta_1 x_1+\cdots+\beta_p x_p\big), \quad 
\sigma(z)=\frac{1}{1+e^{-z}}.
$$

### Stima dei parametri (MLE) e funzione obiettivo

I parametri $\beta=(\beta_0,\dots,\beta_p)$ si stimano massimizzando la **log-verosimiglianza**:

$$
\ell(\beta) 
= \sum_{i=1}^N \Big[ Z_i \log p_i + (1-Z_i)\log(1-p_i) \Big], 
\qquad p_i=\sigma\!\big(\beta_0+\cdots+\beta_p x_p^{(i)}\big).
$$

Equivalente, si **minimizza** la **cross-entropy** (negativa della log-verosimiglianza media):

$$
\text{loss}_{\text{CE}}(\beta) 
= -\frac{1}{N}\sum_{i=1}^N \Big[ Z_i \log p_i + (1-Z_i)\log(1-p_i) \Big].
$$

In pratica si usa ottimizzazione numerica (es. **IRLS/Newton-Raphson** o discesa del gradiente) con versioni **numericamente stabili** (log-sum-exp).

**Nota.** Pur non avendo una distribuzione esatta chiusa forma per i coefficienti, gli MLE sono **asintoticamente normali**: è quindi possibile fare test (Wald, **Likelihood Ratio**, Score) e costruire IC usando la matrice di **informazione di Fisher**. Attenzione a **separazione completa/quasi**: l’MLE può non esistere; si usano penalizzazioni (es. **Firth**) o regolarizzazione.

### Predizione

Dati nuovi $x$, il modello restituisce la **probabilità**:

$$
P(Z=1\mid x)=\sigma\!\big(\beta_0+\beta_1 x_1+\cdots+\beta_p x_p\big).
$$

Una soglia (es. 0.5) consente la classificazione; la scelta della soglia va calibrata (ROC/PR, costo degli errori).



## Regressione Logistica Multinomiale

Quando la risposta ha **$m>2$ categorie** ($Z_i\in\{1,\dots,m\}$), si usa la versione multiclasse. Con **codifica one-hot** $b^{(i)}=(b^{(i)}_1,\dots,b^{(i)}_m)$ (un 1 e il resto 0), si assume:

$$
B^{(i)} \sim \text{Multinomiale}\!\big(1;\ \pi(x^{(i)},W)\big),
$$

dove $W$ è la matrice dei parametri (includendo l’intercetta: si può porre una colonna di 1 in $X$ e avere $W\in\mathbb{R}^{(p+1)\times m}$). Per ciascuna classe $k$:

$$
g_k^{(i)} = w_{0k} + w_{1k} x_1^{(i)} + \cdots + w_{pk} x_p^{(i)} \quad \text{(logit)}
$$

e le **probabilità** sono date dalla **softmax**:

$$
\pi_k\!\big(x^{(i)},W\big) = 
\frac{e^{g_k^{(i)}}}{\sum_{h=1}^m e^{g_h^{(i)}}}, 
\qquad \sum_{k=1}^m \pi_k(\cdot)=1.
$$

La **log-verosimiglianza** è:

$$
\ell(W) = \sum_{i=1}^N \sum_{k=1}^m b^{(i)}_k \log \pi_k\!\big(x^{(i)},W\big),
$$

e la **loss** da minimizzare è la **cross-entropy media**:

$$
\text{loss}(W) = -\frac{1}{N}\sum_{i=1}^N \sum_{k=1}^m b^{(i)}_k \log \pi_k\!\big(x^{(i)},W\big).
$$

**Identificabilità.** Il parametro non è identificato a una traslazione comune dei logit: si impone un vincolo (p.es. togliere una classe di riferimento nel modello baseline-category logit, oppure vincoli di somma/centrazione) per avere stime univoche.

### One-vs-Rest (alternativa)

Si possono stimare **$m$ modelli binari** (classe $k$ vs resto). È semplice, ma le probabilità **non sommano a 1**; utile come baseline, meno coerente probabilisticamente della softmax.



## Riepilogo operativo

- **Binaria**: $Z\sim\text{Bernoulli}(p)$, $p=\sigma(X\beta)$; MLE via ottimizzazione; loss = cross-entropy.  
- **Multiclasse**: $B\sim\text{Multinomiale}(1;\pi)$, $\pi=\text{softmax}(XW)$; loss = cross-entropy.  
- **Inferenza**: test LR/Wald/Score con approssimazioni asintotiche; attenzione a separazione e a usare regolarizzazione se serve.  
- **Predizione**: probabilità calibrate; scelta soglia e metriche (ROC/PR) secondo il problema.
